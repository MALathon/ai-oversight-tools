{
  "$schema": "../schemas/use-cases.schema.json",
  "version": "1.0.0",
  "source": "AIHSR Risk Reference Tool v1.5 (Tamiko Eto)",
  "lastUpdated": "2025-09-02",
  "description": "Example AI use cases with risk assessments and recommended mitigations for IRB review",
  "useCases": [
    {
      "id": "predictive-triage-ai",
      "name": "Predictive triage AI",
      "phase": "phase-1",
      "modelTypes": ["predictive"],
      "riskDomains": ["discrimination-toxicity", "privacy-security"],
      "primaryRisks": ["lack-robustness-7.3", "governance-failure-6.5", "privacy-breach-2.1", "unfair-discrimination-1.1"],
      "researchFeatures": "Uses historical EHR data; no clinical impact",
      "humanSubjectsConcerns": "Privacy breach, non-clinically validated AI output/data entry into medical record, immediate or future direct harm caused by inappropriate patient care/treatment with non-clinically validated AI output",
      "likelihood": "high",
      "severity": "high",
      "riskLevel": "high",
      "riskTypes": ["systemic", "interactional"],
      "reviewerReflection": "Even with mitigation, could this error type still cause harm to participants?",
      "mitigationRequired": false,
      "approvalStatus": "modification-required",
      "benefitRiskJustification": "Weakly justified in protocol"
    },
    {
      "id": "llm-chat-consent-assistant",
      "name": "LLM Chat Consent Assistant",
      "phase": "phase-1",
      "modelTypes": ["llm"],
      "riskDomains": ["discrimination-toxicity", "privacy-security"],
      "primaryRisks": ["governance-failure-6.5", "lack-robustness-7.3", "privacy-breach-2.1", "unfair-discrimination-1.1"],
      "researchFeatures": "LLM outputs vary; difficult to validate accuracy",
      "humanSubjectsConcerns": "Inadequate informed consent (Participants may not understand AI-generated explanations)",
      "likelihood": "medium",
      "severity": "high",
      "riskLevel": "high",
      "riskTypes": ["interactional"],
      "reviewerReflection": "Does the protocol explain model behavior clearly enough for investigators and participants?",
      "mitigationRequired": true,
      "approvalStatus": "approved",
      "benefitRiskJustification": "Clear benefit justification for scalability"
    },
    {
      "id": "llm-post-discharge-instructions",
      "name": "LLM Chatbot providing post-discharge instructions",
      "phase": "phase-2",
      "modelTypes": ["llm"],
      "riskDomains": ["discrimination-toxicity", "privacy-security"],
      "primaryRisks": ["security-vulnerabilities-2.2", "lack-transparency-7.4", "false-information-3.1", "unequal-performance-1.3", "toxic-content-1.2"],
      "researchFeatures": "Chat-based interaction; post-care communication",
      "humanSubjectsConcerns": "Miscommunication; Therapeutic misconception (Participants may misunderstand chatbot suggestions as clinical orders)",
      "likelihood": "medium",
      "severity": "high",
      "riskLevel": "high",
      "riskTypes": ["interactional"],
      "reviewerReflection": "Are participants aware the chatbot is not a clinician? Is support available if confusion arises? Is language adjusted for comprehension?",
      "mitigationRequired": true,
      "approvalStatus": "approved",
      "benefitRiskJustification": "Improved access to instructions may outweigh communication and/or comprehension risks with proper guardrails"
    },
    {
      "id": "sepsis-prediction-ehr",
      "name": "Predictive model for sepsis risk from EHR data",
      "phase": "phase-2",
      "modelTypes": ["supervised-ml", "predictive"],
      "riskDomains": ["discrimination-toxicity", "privacy-security", "misinformation"],
      "primaryRisks": ["unfair-discrimination-1.1", "unequal-performance-1.3", "privacy-breach-2.1", "security-vulnerabilities-2.2", "information-pollution-3.2", "lack-transparency-7.4"],
      "researchFeatures": "EHR-based retrospective model; potential clinical impact",
      "humanSubjectsConcerns": "Biased outcomes; Safety and fairness (The model may underpredict sepsis risk in underrepresented populations.)",
      "likelihood": "high",
      "severity": "high",
      "riskLevel": "high",
      "riskTypes": ["distributional"],
      "reviewerReflection": "Has the model been tested across diverse demographics? Are mitigation strategies included?",
      "mitigationRequired": "partially",
      "approvalStatus": "caution",
      "benefitRiskJustification": "Potential for early sepsis detection justifies study with safeguards (in well-controlled settings)"
    },
    {
      "id": "tumor-boundary-imaging",
      "name": "AI-assisted imaging for tumor boundary detection",
      "phase": "phase-2",
      "modelTypes": ["supervised-ml"],
      "riskDomains": ["discrimination-toxicity", "privacy-security", "misinformation"],
      "primaryRisks": ["information-pollution-3.2", "unfair-discrimination-1.1", "unequal-performance-1.3", "privacy-breach-2.1", "governance-failure-6.5", "lack-transparency-7.4"],
      "researchFeatures": "Clinical workflow augmentation; prospective study",
      "humanSubjectsConcerns": "Downstream care decisions may be biased by automation (Overreliance on AI imaging may affect clinician decision-making workflows)",
      "likelihood": "medium",
      "severity": "medium",
      "riskLevel": "medium",
      "riskTypes": ["systemic"],
      "reviewerReflection": "Are checks in place to avoid automation bias? What is the clinician's role? How is human decision-making preserved in the workflow?",
      "mitigationRequired": true,
      "approvalStatus": "approved",
      "benefitRiskJustification": "Increased diagnostic precision if clinician agency is preserved"
    },
    {
      "id": "pediatric-llm-eligibility",
      "name": "LLM summarizing pediatric patient notes for clinical trials eligibility",
      "phase": "phase-3",
      "modelTypes": ["llm"],
      "riskDomains": ["discrimination-toxicity", "privacy-security", "misinformation", "malicious-misuse", "human-computer-interaction", "socioeconomic-harms", "ai-safety-limitations"],
      "populationVulnerability": "children",
      "primaryRisks": ["overreliance-5.1", "false-information-3.1", "privacy-breach-2.1", "loss-of-agency-5.2", "lack-robustness-7.3", "lack-transparency-7.4"],
      "researchFeatures": "Automated summarization; eligibility screening",
      "humanSubjectsConcerns": "Risk of inappropriate enrollment based on inaccurate data (The LLM may fabricate patient details, leading to inappropriate trial inclusion.)",
      "likelihood": "high",
      "severity": "medium",
      "riskLevel": "high",
      "riskTypes": ["interactional"],
      "reviewerReflection": "Are summaries reviewed by clinicians before use? Is source traceability ensured?",
      "mitigationRequired": true,
      "approvalStatus": "approved",
      "benefitRiskJustification": "Improved efficiency if human-in-the-loop maintained"
    },
    {
      "id": "diabetic-retinopathy-detection",
      "name": "Computer vision tool for diabetic retinopathy detection",
      "phase": "phase-3",
      "modelTypes": ["computer-vision"],
      "riskDomains": ["discrimination-toxicity", "privacy-security", "misinformation", "malicious-misuse", "human-computer-interaction", "socioeconomic-harms", "ai-safety-limitations"],
      "primaryRisks": ["loss-of-agency-5.2", "unfair-discrimination-1.1", "unequal-performance-1.3", "privacy-breach-2.1", "overreliance-5.1", "lack-robustness-7.3", "lack-transparency-7.4"],
      "researchFeatures": "Image classification; diagnostic support",
      "humanSubjectsConcerns": "Missed diagnosis; inequitable care (Model trained on non-representative dataset may miss early signs in some populations.)",
      "likelihood": "high",
      "severity": "high",
      "riskLevel": "high",
      "riskTypes": ["distributional"],
      "reviewerReflection": "Is the sample representative of the study population? Are exclusions justified?",
      "mitigationRequired": false,
      "approvalStatus": "modification-required",
      "benefitRiskJustification": "Potential for early detection justifies use with mitigation"
    },
    {
      "id": "behavioral-nudges-minority",
      "name": "Minority Population-Focused Behavioral nudges based on AI-identified patterns in EHR",
      "phase": "phase-2",
      "modelTypes": ["predictive", "supervised-ml", "recommendation", "generative"],
      "riskDomains": ["discrimination-toxicity", "privacy-security", "misinformation", "malicious-misuse", "human-computer-interaction", "ai-safety-limitations"],
      "populationVulnerability": "racial-ethnic-minorities",
      "primaryRisks": ["governance-failure-6.5", "unfair-discrimination-1.1", "unequal-performance-1.3", "privacy-breach-2.1", "lack-transparency-7.4", "false-information-3.1"],
      "researchFeatures": "Behavioral intervention; AI-targeted messaging",
      "humanSubjectsConcerns": "Autonomy and transparency (AI-generated nudges may influence provider behavior without awareness or consent.)",
      "likelihood": "medium",
      "severity": "medium",
      "riskLevel": "medium",
      "riskTypes": ["systemic"],
      "reviewerReflection": "Are nudges transparent to providers and participants? Are risks disclosed?",
      "mitigationRequired": "partially",
      "approvalStatus": "caution",
      "benefitRiskJustification": "Potential for improved health behaviors if transparency is adequate"
    },
    {
      "id": "mental-health-clustering-older-adults",
      "name": "Unsupervised clustering to identify mental health risk profiles in older adults",
      "phase": "phase-2",
      "modelTypes": ["unsupervised-ml", "classification"],
      "riskDomains": ["discrimination-toxicity", "privacy-security", "misinformation", "malicious-misuse", "human-computer-interaction", "ai-safety-limitations"],
      "populationVulnerability": ["cognitively-impaired", "older-adults"],
      "primaryRisks": ["lack-transparency-7.4", "unfair-discrimination-1.1", "unequal-performance-1.3", "privacy-breach-2.1", "false-information-3.1", "governance-failure-6.5", "lack-robustness-7.3"],
      "researchFeatures": "Behavioral data; privacy-sensitive clustering",
      "humanSubjectsConcerns": "Privacy; stigmatization (Unique combinations of behavioral traits may lead to participant re-identification.)",
      "likelihood": "high",
      "severity": "high",
      "riskLevel": "high",
      "riskTypes": ["distributional"],
      "reviewerReflection": "Could any cluster labels be stigmatizing? Are safeguards adequate?",
      "mitigationRequired": true,
      "approvalStatus": "approved",
      "benefitRiskJustification": "Actionable insights may justify risks with proper safeguards"
    },
    {
      "id": "radiology-report-llm-classification",
      "name": "Evaluation of LLM for radiology report classification to monitor performance, value and user engagement of FDA-approved AI algorithms",
      "phase": "phase-1",
      "modelTypes": ["predictive", "llm"],
      "riskDomains": ["discrimination-toxicity", "privacy-security", "misinformation", "human-computer-interaction"],
      "primaryRisks": ["overreliance-5.1", "false-information-3.1", "unfair-discrimination-1.1", "privacy-breach-2.1", "unequal-performance-1.3", "lack-transparency-7.4", "loss-of-agency-5.2"],
      "researchFeatures": "Retrospective chart review of radiology reports; FDA-cleared imaging algorithms already in clinical use; LLM output compared against existing AI algorithm outputs for QI",
      "humanSubjectsConcerns": "Privacy/confidentiality/data handling details; waiver criteria met? Bias due to LLM performance varying by pt demographic or report structure; No effect on pt care but monitoring results could indirectly inform clinical practice over time",
      "likelihood": "medium",
      "severity": "medium",
      "riskLevel": "low",
      "riskTypes": ["systemic", "interactional", "distributional"],
      "reviewerReflection": "How is PHI handled and stored per HIPAA requirements? Is there sufficient rationale for waiver of consent? Ensure output does not impact medical records; How will investigators prevent automation bias?",
      "mitigationRequired": true,
      "approvalStatus": "approved",
      "benefitRiskJustification": "If all mitigations are adopted: Residual risk is low; primary risk remains privacy breach, but covered by technical safeguards"
    },
    {
      "id": "nurse-patient-transcription",
      "name": "Automated transcription and structuring of nurse-patient conversation into EHR fields",
      "phase": "phase-2",
      "modelTypes": ["llm"],
      "riskDomains": ["privacy-security", "human-computer-interaction", "misinformation", "discrimination-toxicity"],
      "primaryRisks": ["privacy-breach-2.1", "security-vulnerabilities-2.2"],
      "researchFeatures": "Real-time audio capture",
      "humanSubjectsConcerns": "Privacy and confidentiality breach potential",
      "likelihood": "medium",
      "severity": "high",
      "riskLevel": "high",
      "riskTypes": ["interactional"],
      "reviewerReflection": "How is PHI handled? What measures prevent re-identification?",
      "mitigationRequired": true,
      "approvalStatus": "approved",
      "benefitRiskJustification": "Efficiency gains if privacy adequately protected"
    },
    {
      "id": "voice-ehr-autofill",
      "name": "AI system recording clinician-patient conversations to automatically fill EHR fields",
      "phase": "phase-1",
      "modelTypes": ["predictive"],
      "riskDomains": ["privacy-security", "misinformation"],
      "primaryRisks": ["privacy-breach-2.1"],
      "researchFeatures": "Real-time audio capture (fully identifiable), prospective data collection, inpatient adults at least 18 years of age, clinician-verified output",
      "humanSubjectsConcerns": "Patients may not realize conversations are being recorded; what is being done to prevent risk of accidental disclosure? How might the recordings be saved, used, and disclosed in the future?",
      "likelihood": "medium",
      "severity": "high",
      "riskLevel": "high",
      "riskTypes": ["interactional"],
      "reviewerReflection": "Will the tool 'remember' the participant? Can end users obtain PHI from the tool? Were non-English-Speaking and/or minorities excluded from training?",
      "mitigationRequired": "partially",
      "approvalStatus": "caution",
      "benefitRiskJustification": "Efficiency benefits if privacy controls are robust"
    },
    {
      "id": "medical-record-summary-chatbot",
      "name": "Chatbot that summarizes medical records in prep for a doctor appointment",
      "phase": "phase-2",
      "modelTypes": ["llm"],
      "riskDomains": ["discrimination-toxicity", "privacy-security", "misinformation", "human-computer-interaction"],
      "primaryRisks": ["unfair-discrimination-1.1", "privacy-breach-2.1", "false-information-3.1"],
      "researchFeatures": "Chat-based interaction; patient-facing tool",
      "humanSubjectsConcerns": "Biased results or misrepresentation; potential for inaccurate summaries",
      "likelihood": "medium",
      "severity": "high",
      "riskLevel": "high",
      "riskTypes": ["interactional", "distributional", "systemic"],
      "reviewerReflection": "How will bias be checked? Is output validated before patient sees it?",
      "mitigationRequired": false,
      "approvalStatus": "modification-required",
      "benefitRiskJustification": "Patient empowerment if accuracy ensured"
    }
  ],
  "approvalStatuses": {
    "approved": {
      "label": "Approval criteria met",
      "symbol": "check",
      "description": "Study can proceed with described safeguards"
    },
    "caution": {
      "label": "Caution - Recommend further mitigation",
      "symbol": "warning",
      "description": "Study may proceed with additional safeguards"
    },
    "modification-required": {
      "label": "Approval criteria not met - Modification required",
      "symbol": "stop",
      "description": "Study requires modifications before approval"
    }
  }
}
