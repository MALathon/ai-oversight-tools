{
  "$schema": "./schemas/technical-controls.schema.json",
  "version": "2.0.0",
  "lastUpdated": "2025-12-14",
  "source": "MIT AI Risk Repository Mitigation Database",
  "attribution": {
    "mitRepository": {
      "name": "MIT AI Risk Repository - AI Risk Mitigation Database",
      "url": "https://airisk.mit.edu/ai-risk-mitigations",
      "citation": "MIT AI Risk Repository (2024). Mapping AI Risk Mitigations: Draft Taxonomy & Evidence Scan.",
      "sources": [
        {
          "id": "Barrett2024",
          "citation": "Barrett et al. (2024)",
          "count": 75
        },
        {
          "id": "Bengio2025",
          "citation": "Bengio et al. (2025)",
          "count": 26
        },
        {
          "id": "Campos2025",
          "citation": "Campos et al. (2025)",
          "count": 19
        },
        {
          "id": "Casper2025",
          "citation": "Casper et al. (2025)",
          "count": 14
        },
        {
          "id": "EU AI Office2025",
          "citation": "EU AI Office (2025)",
          "count": 16
        },
        {
          "id": "Eisenberg2025",
          "citation": "Eisenberg (2025)",
          "count": 38
        },
        {
          "id": "Future of Life Institute2024",
          "citation": "Future of Life Institute (2024)",
          "count": 85
        },
        {
          "id": "Gipi\u0161kis2024",
          "citation": "Gipi\u0161kis (2024)",
          "count": 82
        },
        {
          "id": "NIST2024",
          "citation": "NIST AI RMF (2024)",
          "count": 203
        },
        {
          "id": "Schuett2023",
          "citation": "Schuett (2023)",
          "count": 50
        },
        {
          "id": "UK Government2023",
          "citation": "UK Government (2023)",
          "count": 169
        },
        {
          "id": "Uuk2024",
          "citation": "Uuk (2024)",
          "count": 28
        },
        {
          "id": "Wiener2024",
          "citation": "Wiener (2024)",
          "count": 10
        }
      ]
    },
    "aihsr": {
      "name": "AI in Human Subjects Research (AIHSR) Risk Reference Tool",
      "version": "1.5",
      "author": "Tamiko Eto",
      "citation": "Eto, T. (2025). AIHSR Risk Reference Tool v1.5."
    },
    "nist": {
      "name": "NIST AI Risk Management Framework",
      "url": "https://www.nist.gov/itl/ai-risk-management-framework",
      "citation": "NIST (2024). AI Risk Management Framework: Generative AI Profile."
    }
  },
  "description": "Technical and operational controls for AI risk mitigation, organized by MIT taxonomy with phase and technology-type awareness",
  "controlCategories": [
    {
      "id": "governance-oversight",
      "name": "Governance & Oversight Controls",
      "description": "Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols",
      "code": "1"
    },
    {
      "id": "technical-security",
      "name": "Technical & Security Controls",
      "description": "Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors",
      "code": "2"
    },
    {
      "id": "operational-process",
      "name": "Operational Process Controls",
      "description": "Processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation",
      "code": "3"
    },
    {
      "id": "transparency-accountability",
      "name": "Transparency & Accountability Controls",
      "description": "Formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny",
      "code": "4"
    }
  ],
  "controlSubcategories": [
    {
      "id": "board-oversight-1.1",
      "code": "1.1",
      "name": "Board Structure & Oversight",
      "categoryId": "governance-oversight"
    },
    {
      "id": "risk-management-1.2",
      "code": "1.2",
      "name": "Risk Management",
      "categoryId": "governance-oversight"
    },
    {
      "id": "conflict-interest-1.3",
      "code": "1.3",
      "name": "Conflict of Interest Protections",
      "categoryId": "governance-oversight"
    },
    {
      "id": "whistleblower-1.4",
      "code": "1.4",
      "name": "Whistleblower Reporting & Protection",
      "categoryId": "governance-oversight"
    },
    {
      "id": "safety-frameworks-1.5",
      "code": "1.5",
      "name": "Safety Decision Frameworks",
      "categoryId": "governance-oversight"
    },
    {
      "id": "environmental-1.6",
      "code": "1.6",
      "name": "Environmental Impact Management",
      "categoryId": "governance-oversight"
    },
    {
      "id": "societal-impact-1.7",
      "code": "1.7",
      "name": "Societal Impact Assessment",
      "categoryId": "governance-oversight"
    },
    {
      "id": "infrastructure-security-2.1",
      "code": "2.1",
      "name": "Model & Infrastructure Security",
      "categoryId": "technical-security"
    },
    {
      "id": "model-alignment-2.2",
      "code": "2.2",
      "name": "Model Alignment",
      "categoryId": "technical-security"
    },
    {
      "id": "safety-engineering-2.3",
      "code": "2.3",
      "name": "Model Safety Engineering",
      "categoryId": "technical-security"
    },
    {
      "id": "content-safety-2.4",
      "code": "2.4",
      "name": "Content Safety Controls",
      "categoryId": "technical-security"
    },
    {
      "id": "testing-auditing-3.1",
      "code": "3.1",
      "name": "Testing & Auditing",
      "categoryId": "operational-process"
    },
    {
      "id": "data-governance-3.2",
      "code": "3.2",
      "name": "Data Governance",
      "categoryId": "operational-process"
    },
    {
      "id": "access-management-3.3",
      "code": "3.3",
      "name": "Access Management",
      "categoryId": "operational-process"
    },
    {
      "id": "staged-deployment-3.4",
      "code": "3.4",
      "name": "Staged Deployment",
      "categoryId": "operational-process"
    },
    {
      "id": "post-deployment-3.5",
      "code": "3.5",
      "name": "Post-Deployment Monitoring",
      "categoryId": "operational-process"
    },
    {
      "id": "incident-response-3.6",
      "code": "3.6",
      "name": "Incident Response & Recovery",
      "categoryId": "operational-process"
    },
    {
      "id": "documentation-4.1",
      "code": "4.1",
      "name": "System Documentation",
      "categoryId": "transparency-accountability"
    },
    {
      "id": "risk-disclosure-4.2",
      "code": "4.2",
      "name": "Risk Disclosure",
      "categoryId": "transparency-accountability"
    },
    {
      "id": "incident-reporting-4.3",
      "code": "4.3",
      "name": "Incident Reporting",
      "categoryId": "transparency-accountability"
    },
    {
      "id": "governance-disclosure-4.4",
      "code": "4.4",
      "name": "Governance Disclosure",
      "categoryId": "transparency-accountability"
    },
    {
      "id": "third-party-access-4.5",
      "code": "4.5",
      "name": "Third-Party System Access",
      "categoryId": "transparency-accountability"
    },
    {
      "id": "user-rights-4.6",
      "code": "4.6",
      "name": "User Rights & Recourse",
      "categoryId": "transparency-accountability"
    }
  ],
  "controls": [
    {
      "id": "A0001_Bengio2025",
      "name": "Audits",
      "description": "A formal review of an organisation\u2019s compliance with standards, policies, and procedures, typically carried out by an external party. AI auditing is a rapidly growing field, but builds on long histories of auditing in other fields, including financial, environmental, and health regulation.",
      "source": "Bengio2025",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0002_Bengio2025",
      "name": "Benchmarks",
      "description": "A standardised, often quantitative test or metric used to evaluate and compare the performance of AI systems on a fixed set of tasks designed to represent real- world usage",
      "source": "Bengio2025",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0003_Bengio2025",
      "name": "Bowtie Method",
      "description": "A technique for visualising risk quantitatively and qualitatively, providing clear differentiation between proactive and reactive risk management, intended to help prevent and mitigate major accident hazards. Oil companies and national governments use the bowtie method.",
      "source": "Bengio2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0004_Bengio2025",
      "name": "Defence in Depth",
      "description": "The idea that multiple independent and overlapping layers of defence can be implemented such that if one fails, others will still be effective. An example comes from the field of infectious diseases, where multiple preventative measures (e.g. vaccines, masks, hand washing) can layer to reduce overall risk.",
      "source": "Bengio2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0005_Bengio2025",
      "name": "Documentation",
      "description": "There are numerous documentation best practices, guidelines, and requirements for AI systems to track e.g. training data, model design and functionality, intended use cases, limitations, and risks. Model cards\u2019 and \u2018system cards\u2019 are examples of prominent AI documentation standards",
      "source": "Bengio2025",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0006_Bengio2025",
      "name": "Engagement with Relevant Experts and Communities",
      "description": "Domain experts, users, and impacted communities have unique insights into likely risks. There are emerging guidelines for participatory and inclusive AI.",
      "source": "Bengio2025",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0007_Bengio2025",
      "name": "If-Then Commitments",
      "description": "A set of technical and organisational protocols and commitments to manage risks at varying levels as AI models become more capable. Some companies developing general- purpose AI employ these types of commitments as responsible scaling policies or similar frameworks.",
      "source": "Bengio2025",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0008_Bengio2025",
      "name": "Impact Assessment",
      "description": "A tool used to assess the potential impacts of a technology or project. The EU AI Act requires developers of high- risk AI systems to carry out Fundamental Rights Impact Assessments.",
      "source": "Bengio2025",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0009_Bengio2025",
      "name": "Incident Reporting",
      "description": "The process of systematically documenting and sharing cases in which developing or deploying AI has caused direct or indirect harms. Incident reporting is common in many domains, from human resources to cybersecurity. It has also become more common for AI.",
      "source": "Bengio2025",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0010_Bengio2025",
      "name": "Model Evaluation",
      "description": "Processes to assess and measure an AI system's performance on a particular task. There are countless AI evaluations to assess different capabilities and risks, including for security.",
      "source": "Bengio2025",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0011_Bengio2025",
      "name": "Red Teaming",
      "description": "An exercise in which a group of people or automated systems pretend to be an adversary and attack an organisation\u2019s systems in order to identify vulnerabilities.",
      "source": "Bengio2025",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0012_Bengio2025",
      "name": "Risk management frameworks",
      "description": "Whole organisation frameworks to reduce gaps in risk coverage and ensure various risk activities (i.e. all of the above) are cohesively structured and aligned, risk roles and responsibilities are clearly defined, and checks and balances are in place to avoid silos and manage conflicts of interest. In other safety critical industries, the Three Lines of Defence framework \u2013 separating risk ownership, oversight and audit \u2013 is widely used and can be usefully applied to advanced AI companies",
      "source": "Bengio2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0013_Bengio2025",
      "name": "Risk Register",
      "description": "A risk management tool that serves as a repository of all risks, their prioritisation, owners, and mitigation plans. They are sometimes used to fulfil regulatory compliance. Risk registers are a relatively standard tool used across many industries, including cybersecurity and recently AI.",
      "source": "Bengio2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0014_Bengio2025",
      "name": "Risk Taxonomy",
      "description": "A way to categorise and organise risks across multiple dimensions. There are several well- known risk taxonomies for AI.",
      "source": "Bengio2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0015_Bengio2025",
      "name": "Risk Thresholds",
      "description": "Quantitative or qualitative limits that distinguish acceptable from unacceptable risks and trigger specific risk management actions when exceeded. Risk thresholds for general- purpose AI are being determined by a combination of assessments of capabilities, impact, compute, reach, and other factors.",
      "source": "Bengio2025",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0016_Bengio2025",
      "name": "Risk Tolerance",
      "description": "The level of risk an organisation is willing to take on. In AI, risks tolerances are often left up to AI companies, but regulatory regimes can help identify unacceptable risks that are legally prohibited",
      "source": "Bengio2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0017_Bengio2025",
      "name": "Safety Analysis",
      "description": "Helps understand the dependencies between components and the system that they are part of, in order to anticipate how component failures could lead to system-level hazards. This approach is used across safety- critical fields, e.g. to anticipate and prevent aircraft crashes or nuclear reactor core meltdowns",
      "source": "Bengio2025",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0018_Bengio2025",
      "name": "Safety Cases",
      "description": "Safety cases require developers to demonstrate safety. A safety case is a structured argument supported by evidence that a system is acceptably safe to operate in a particular context. Safety cases are common in many industries, including defence, aerospace, and railways.",
      "source": "Bengio2025",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0019_Bengio2025",
      "name": "Safety of The Intended Function",
      "description": "An approach that requires engineers to provide evidence that a system is safe when operating as intended. This approach is used in many engineering fields, such as in the construction and testing of road vehicles.",
      "source": "Bengio2025",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0020_Bengio2025",
      "name": "Scenario Analysis",
      "description": "Developing plausible future scenarios and analysing how risks materialise. Scenario analysis and planning are widely used across industries including for the energy sector and to address uncertainties of power systems.",
      "source": "Bengio2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0021_Bengio2025",
      "name": "Threat Modelling",
      "description": "A process to identify threats and vulnerabilities to a system. Threat modelling is commonly used to support AI security throughout AI research and development",
      "source": "Bengio2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0022_Bengio2025",
      "name": "Whistleblower protection",
      "description": "Whistleblowers can play an important role in alerting authorities to dangerous risks at AI companies due to the proprietary nature of many AI advancements. Incentives and protections for whistleblowers are expected to be an important part of advanced AI risk governance.",
      "source": "Bengio2025",
      "subcategoryId": "whistleblower-1.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0023_Bengio2025",
      "name": "Delphi Method",
      "description": "A group decision-making technique that uses a series of questionnaires to gather consensus from a panel of experts. The Delphi method has been used to help identify key AI risks.",
      "source": "Bengio2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0024_Bengio2025",
      "name": "Responsible Release and Deployment Strategies",
      "description": "There is a spectrum of release and deployment strategies for AI including staged releases, cloud- based or API access, deployment safety controls, and acceptable use policies. There are some emerging industry practices that focus on release and deployment strategies for general- purpose AI.",
      "source": "Bengio2025",
      "subcategoryId": "staged-deployment-3.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0025_Bengio2025",
      "name": "Risk Matrices",
      "description": "A visual tool that helps prioritise risks according to their likelihood of occurrence and potential impact. Risk matrices are used in many industries and for many purposes, such as by financial institutions for evaluating credit risk, or by companies to assess possible disruptions to their supply chains",
      "source": "Bengio2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0026_Bengio2025",
      "name": "Safety by Design",
      "description": "An approach that centres user safety in the design and development of products and services. This approach is common across engineering and safety- critical fields including aviation and energy.",
      "source": "Bengio2025",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0027_Casper2025",
      "name": "Document compute usage",
      "description": "Given that computing power is key to frontier AI development (Sastry et al., 2024), frontier developers can be required to document their compute resources including details such as the usage, providers, and the location of compute clusters.",
      "source": "Casper2025",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0028_Casper2025",
      "name": "Documentation availability",
      "description": "All of the above documentation can be made available to the public (redacted) and AI governing authorities (unredacted).",
      "source": "Casper2025",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0029_Casper2025",
      "name": "Documentation comparison in court",
      "description": "To incentivize a race to the top where frontier developers pursue established best safety practices, courts can be given the power to compare documentation for defendants with that of peer developers.",
      "source": "Casper2025",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0030_Casper2025",
      "name": "Incident reporting",
      "description": "Frontier developers can be required to document and report on substantial incidents in a timely manner.",
      "source": "Casper2025",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0031_Casper2025",
      "name": "Independent third-party risk assessments",
      "description": "Developers can be required to have an independent third-party conduct and produce a report (including access, methods, and findings) on risk assessments of frontier systems (Raji et al., 2022; Anderljung et al., 2023; Casper et al., 2024). They can also be required to document if and what \u201csafe harbor\u201d policies they have to facilitate independent evaluation and red-teaming (Longpre et al., 2024).",
      "source": "Casper2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0032_Casper2025",
      "name": "Internal risk assessments",
      "description": "Developers can be required to conduct and report on internal risk assessments of frontier systems.",
      "source": "Casper2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0033_Casper2025",
      "name": "Labeling AI-generated content",
      "description": "To aid in digital forensics, content produced from AI systems can be labeled with metadata, watermarks, and notices.",
      "source": "Casper2025",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0034_Casper2025",
      "name": "Model registration",
      "description": "Developers can be required to register (McKernon et al., 2024) frontier systems with governing bodies (regardless of whether they will be externally deployed).",
      "source": "Casper2025",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0035_Casper2025",
      "name": "Model specification and basic info",
      "description": "Developers can be required to document intended use cases and behaviors (e.g., OpenAI, 2024) and basic information about frontier systems such as scale.",
      "source": "Casper2025",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0036_Casper2025",
      "name": "Plans to minimize risks to society",
      "description": "Developers can be required to produce a report on risks (Slattery et al., 2024) posed by their frontier systems and risk mitigation practices that they are taking to reduce them.",
      "source": "Casper2025",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0037_Casper2025",
      "name": "Post-deployment monitoring reports",
      "description": "Developers can be required to establish procedures for monitoring and periodically reporting on the uses and impacts of their frontier systems.",
      "source": "Casper2025",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0038_Casper2025",
      "name": "Security measures",
      "description": "Given the challenges of securing model weights and the hazards of leaks (Nevo et al., 2024), frontier developers can be required to document high-level noncompromising information about their security measures (e.g., Anthropic, 2024).",
      "source": "Casper2025",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0039_Casper2025",
      "name": "Shutdown procedures",
      "description": "Developers can be required to document if and which protocols exist to shut down frontier systems that are under their control.",
      "source": "Casper2025",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0040_Casper2025",
      "name": "Whistleblower protections",
      "description": "Regulations can explicitly prevent retaliation and offer incentives for whistleblowers who report violations of those regulations.",
      "source": "Casper2025",
      "subcategoryId": "whistleblower-1.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0042_Eisenberg2025",
      "name": "Establish AI system access controls",
      "description": "Implement comprehensive access management including role-based access control (RBAC), authentication mechanisms, and audit logging for AI models and associated resources.",
      "source": "Eisenberg2025",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0043_Eisenberg2025",
      "name": "Implement AI asset protection framework",
      "description": "Deploy technical protection measures including encryption, secure enclaves, and versioning controls for AI models and associated data.",
      "source": "Eisenberg2025",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0044_Eisenberg2025",
      "name": "Establish security validation framework",
      "description": "Execute comprehensive pre-deployment security validation including AI-specific vulnerability assessments, penetration testing, and security requirement verification.",
      "source": "Eisenberg2025",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0045_Eisenberg2025",
      "name": "Implement continuous security testing system",
      "description": "Deploy ongoing security testing mechanisms including automated vulnerability scanning, continuous security monitoring, and periodic reassessment of security controls.",
      "source": "Eisenberg2025",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0046_Eisenberg2025",
      "name": "Implement AI security defense system",
      "description": "Deploy active defense mechanisms combining continuous security monitoring, input validation, adversarial detection, and adaptive response capabilities specific to AI systems.",
      "source": "Eisenberg2025",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0050_Eisenberg2025",
      "name": "Establish AI system documentation framework",
      "description": "Implement comprehensive documentation requirements and processes covering training data provenance, system architecture, model cards, and component interactions to ensure transparent documentation of both the data lifecycle and system design.",
      "source": "Eisenberg2025",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0051_Eisenberg2025",
      "name": "Implement AI system monitoring and logging infrastructure",
      "description": "Deploy comprehensive monitoring and logging systems that capture AI system behavior, decisions, performance metrics, and real-time data source usage at multiple levels of granularity for full system observability, including tracking of data lineage during inference.",
      "source": "Eisenberg2025",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0052_Eisenberg2025",
      "name": "Establish AI decision explanation framework",
      "description": "Implement mechanisms and tools for generating human-understandable explanations of AI system decisions, including feature importance, decision paths, confidence levels, and clear attribution of data sources and their characteristics used during inference.",
      "source": "Eisenberg2025",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0053_Eisenberg2025",
      "name": "Establish and apply performance testing and validation framework",
      "description": "Implement comprehensive performance requirements, testing protocols, and validation procedures to ensure AI systems meet capability requirements and maintain reliable operation across intended use cases.",
      "source": "Eisenberg2025",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0054_Eisenberg2025",
      "name": "Implement performance monitoring and robustness system",
      "description": "Implement continuous monitoring and testing mechanisms to evaluate AI system robustness, generalization capabilities, and performance stability across varying conditions and environments while in production.",
      "source": "Eisenberg2025",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0055_Eisenberg2025",
      "name": "Establish and apply fairness testing and validation framework",
      "description": "Implement comprehensive procedures to validate model fairness during development and pre-deployment, including test dataset creation, metric definition, and systematic assessment of performance disparities across demographic groups.",
      "source": "Eisenberg2025",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0056_Eisenberg2025",
      "name": "Implement fairness monitoring and remediation system",
      "description": "Deploy continuous monitoring systems to detect fairness issues in production, including automated drift detection, performance disparity alerts, and systematic remediation procedures.",
      "source": "Eisenberg2025",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0057_Eisenberg2025",
      "name": "Establish universal access and performance design framework",
      "description": "Establish and follow a structured framework ensuring the AI system is designed and developed to deliver consistent, high-quality performance and accessibility for all intended user groups, regardless of their characteristics or circumstances.",
      "source": "Eisenberg2025",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0058_Eisenberg2025",
      "name": "Establish content safety policy and boundaries",
      "description": "Define and document comprehensive content safety policies, including prohibited content categories, acceptable content guidelines, output constraints, and required safeguards. Establish clear thresholds, classification criteria, and escalation levels for different types of harmful content. Include specific criteria for content that could enable or promote malicious use.",
      "source": "Eisenberg2025",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0059_Eisenberg2025",
      "name": "Implement content moderation system",
      "description": "Implement automated and/or human-in-the-loop content moderation mechanisms to detect and filter harmful content in real-time, including content classification, blocking procedures, and automated enforcement of safety boundaries. Include detection of potential malicious use patterns.",
      "source": "Eisenberg2025",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0060_Eisenberg2025",
      "name": "Implement content safety incident response",
      "description": "Establish procedures for investigating, documenting, and remediating harmful content incidents that bypass moderation systems, including coordination with relevant authorities, root cause analysis, and system improvement protocols. Include specific procedures for suspected malicious use cases.",
      "source": "Eisenberg2025",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0061_Eisenberg2025",
      "name": "Establish information quality assurance framework",
      "description": "Implement comprehensive mechanisms to assess, verify, and improve the factual accuracy of AI system outputs, including source validation, fact-checking procedures, and uncertainty communication protocols.",
      "source": "Eisenberg2025",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0062_Eisenberg2025",
      "name": "Establish frontier AI safety framework",
      "description": "Establish and enforce policies governing system AI scaling decisions, including risk assessment requirements, capability thresholds, and deployment constraints. Define clear criteria for when and how system capabilities can be expanded based on safety considerations.",
      "source": "Eisenberg2025",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0063_Eisenberg2025",
      "name": "Implement adversarial testing and red team program",
      "description": "Conduct systematic adversarial testing and red team exercises focused on probing AI system capabilities, identifying potential misuse vectors, and exposing unintended harmful behaviors. Testing should explore ways the system could be manipulated to produce dangerous outputs, bypass safety guardrails, or exhibit undesired emergent behaviors. Include scenarios involving both individual and coordinated attempts to exploit the system\u2019s capabilities.",
      "source": "Eisenberg2025",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0064_Eisenberg2025",
      "name": "Implement system usage monitoring and prevention",
      "description": "Monitor and prevent malicious or otherwise disallowed behavioral patterns including automated abuse, coordination across accounts, and systematic manipulation attempts.",
      "source": "Eisenberg2025",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0065_Eisenberg2025",
      "name": "Implement AI system usage verification program",
      "description": "Deploy comprehensive measures to verify user identity, document intended use cases, and ensure AI system usage complies with instructions. This includes KYC procedures for user verification, clear documentation of permitted uses, and user acknowledgment of instructions.",
      "source": "Eisenberg2025",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0066_Eisenberg2025",
      "name": "Implement AI System Disclosure Requirements",
      "description": "Deploy mechanisms to ensure clear, timely disclosure of AI system use to end users, including automated notifications of AI involvement in interactions, explicit identification of AI-generated content, and clear communication of when users are interacting with AI systems.",
      "source": "Eisenberg2025",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0067_Eisenberg2025",
      "name": "Implement a privacy protection framework",
      "description": "Implement comprehensive privacy protection measures to prevent exposure of PII and sensitive information, including data minimization, anonymization procedures, and privacy-preserving inference techniques.",
      "source": "Eisenberg2025",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0068_Eisenberg2025",
      "name": "Implement a privacy incident detection and response",
      "description": "Deploy monitoring and response mechanisms to detect and address potential privacy exposures, including PII leak detection, sensitive information monitoring, and privacy incident handling procedures.",
      "source": "Eisenberg2025",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0069_Eisenberg2025",
      "name": "Establish user rights and recourse framework",
      "description": "Implement comprehensive mechanisms for user reporting, feedback collection, incident investigation, and recourse provision, including clear procedures for users to report issues, request explanations or corrections, appeal decisions, and receive appropriate remediation. The system should handle various types of user concerns including system errors, unfair treatment, privacy violations, and safety issues.",
      "source": "Eisenberg2025",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0071_Eisenberg2025",
      "name": "Establish human-AI interaction safety framework",
      "description": "Implement comprehensive safeguards to ensure appropriate levels of human oversight, control, and agency in AI system interactions, including decision autonomy requirements, override capabilities, and dependency prevention measures.",
      "source": "Eisenberg2025",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0072_Eisenberg2025",
      "name": "Implement psychological impact management system",
      "description": "Establish monitoring and intervention procedures to detect and prevent unhealthy user-AI relationships, including emotional dependency tracking, interaction boundary enforcement, and well-being safeguards.",
      "source": "Eisenberg2025",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0073_Eisenberg2025",
      "name": "Implement environmental impact management system",
      "description": "Implement comprehensive environmental impact monitoring and optimization procedures, including energy efficiency measures, carbon footprint tracking, and hardware lifecycle management.",
      "source": "Eisenberg2025",
      "subcategoryId": "environmental-1.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0074_Eisenberg2025",
      "name": "Establish third-party assessment and management framework",
      "description": "Establish comprehensive procedures for documenting, assessing, and managing upstream providers and dependencies in the AI system value chain, including transparency requirements, compliance verification, dependency tracking, and contingency planning.",
      "source": "Eisenberg2025",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0075_Eisenberg2025",
      "name": "Establish AI legal compliance process",
      "description": "Evaluate and document how the AI system complies with relevant regulations and standards, identifying use case-specific legal risks and required controls. Apply the organization\u2019s legal compliance framework to ensure appropriate safeguards are in place, with clear documentation of compliance assessments and risk mitigations.",
      "source": "Eisenberg2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0076_Eisenberg2025",
      "name": "Establish societal impact assessment framework",
      "description": "Implement comprehensive processes for assessing and documenting potential societal impacts of AI systems, including effects on employment, economic systems, power dynamics, and cultural value. Include stakeholder consultation and impact mitigation planning.",
      "source": "Eisenberg2025",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0077_Eisenberg2025",
      "name": "Establish responsible development and deployment policy",
      "description": "Establish policies and procedures governing AI system development and deployment decisions that consider societal implications, including competitive pressures, governance gaps, and benefit distribution.",
      "source": "Eisenberg2025",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0078_Eisenberg2025",
      "name": "Implement AI alignment validation system",
      "description": "Establish processes for validating and maintaining AI system alignment with human values and goals, including testing for goal preservation, monitoring for objective drift, and validation of decision-making processes against ethical standards. Includes specific attention to detecting and preventing potentially misaligned behaviors, emergent goals, or deceptive actions. Covers using interpretability techniques to measure and assure alignment with intended goals.",
      "source": "Eisenberg2025",
      "subcategoryId": "model-alignment-2.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "reinforcement-learning",
        "foundation"
      ]
    },
    {
      "id": "A0079_Eisenberg2025",
      "name": "Establish AI Risk Management System",
      "description": "Implement a comprehensive AI risk management system including risk assessment processes, monitoring frameworks, governance structures, and response procedures.",
      "source": "Eisenberg2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0080_Eisenberg2025",
      "name": "Establish data governance and management practices",
      "description": "Implement data governance measures used for training, including having a copyright policy and identifying and documenting data sources, potential biases, and mitigations taken.",
      "source": "Eisenberg2025",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0081_Eisenberg2025",
      "name": "Establish documentation sharing mechanism",
      "description": "Implement a process to share information and documentation to third-parties, including to regulators and downstream deployers or developers.",
      "source": "Eisenberg2025",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0082_Eisenberg2025",
      "name": "Implement a risk reporting mechanism",
      "description": "Establish processes to identify and disclose known or reasonably foreseeable risks, the discovery of new risks, or instances of non-conformity to third parties.",
      "source": "Eisenberg2025",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0083_Eisenberg2025",
      "name": "Establish a general purpose incident response mechanism",
      "description": "Establish processes to enable incident monitoring and reporting. This includes defining \u201dserious incidents\u201d or set a threshold for formal reporting based on regulatory requirements to third-parties, regulators, and impacted individuals.",
      "source": "Eisenberg2025",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0084_Future of Life Institute2024",
      "name": "Adopted Risk Management Frameworks",
      "description": "Does your firm implement any of the following risk management approaches? ISO 31000 NIST AI Risk Management Framework The 3 Lines of Defense Model (3LOD)",
      "source": "Future of Life Institute2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0086_Future of Life Institute2024",
      "name": "AI Safety Team",
      "description": "Does your organization have a safety team? If yes, please provide the team name, a URL to the team\u2019s website (if applicable), the team size (defined as FTE technical staff), and briefly describe its mission.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0087_Future of Life Institute2024",
      "name": "AI-Generated Content Watermarking",
      "description": "Are the outputs of your firm\u2019s AI systems tagged with watermarks that indicate that an AI generates the material? - Video - Image",
      "source": "Future of Life Institute2024",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0089_Future of Life Institute2024",
      "name": "Board of directors",
      "description": "Information about board independence; any non-standard safety-related powers; whether it has a mandate to prioritize safety over profits.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0091_Future of Life Institute2024",
      "name": "BOD formal risk committee",
      "description": "Does the board of directors feature a formal risk committee that is tasked with overseeing the firm\u2019s risk management practices? If yes, please name the members of this committee.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0099_Future of Life Institute2024",
      "name": "Company structure",
      "description": "Information that indicates whether the structure of the firm would allow it to prioritize safety in critical situations or whether shareholder pressure could drive it to deploy capable but dangerous systems.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0103_Future of Life Institute2024",
      "name": "Comprehensive pre-training risk assessments",
      "description": "Does your firm conduct comprehensive pre-training risk assessments? Such assessments should include forecasting (dangerous) capabilities and developing a model-specific risk taxonomy that includes reasonably foreseeable impacts on individuals, groups, organizations, and society. The taxonomy should include misuse cases and scenarios where malicious actors steal the model.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0104_Future of Life Institute2024",
      "name": "Comprehensive whistleblower protection policy",
      "description": "Does your firm have a comprehensive whistleblower protection (WP) policy that outlines the relevant reporting process, protection mechanisms, and non-retaliation assurances? Does your organization cooperate with an external firm that handles whistleblowers from your organization, and does your organization require any employees to sign non-disparagement agreements? Please select all that apply.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "whistleblower-1.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0105_Future of Life Institute2024",
      "name": "Control/Alignment Strategy",
      "description": "We assess whether the company has publicly shared their strategy for ensuring that ever more advanced artificial intelligence remains under human control or remains aligned, and summarize contents of any such documents. We exclude policy recommendations to governments and other stakeholders.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0109_Future of Life Institute2024",
      "name": "Early Warning Evaluations for Catastrophic Risks",
      "description": "Does your organization evaluate models during training for early warning signs of capabilities related to catastrophic risks to ensure risk thresholds are not exceeded? Please describe the regularity and scope of these evaluations and specify whether models are specifically fine-tuned to elicit the capabilities in question.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0110_Future of Life Institute2024",
      "name": "Emergency Rollback Plan",
      "description": "Critically dangerous capabilities or very severe yet unexpected misuse patterns might only surface after a system has been deployed. Has your firm developed an emergency response plan to react to scenarios where such problems cannot be resolved quickly via updates? Please select all interventions that your organization has implemented. Made legal and technical preparations to roll back a system rapidly Formally specified the risk threshold that would trigger a rapid rollback Committed to regular safety drills to test emergency response",
      "source": "Future of Life Institute2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0111_Future of Life Institute2024",
      "name": "Expert Consultation on Societal Risks",
      "description": "Has your organization consulted with top-level domain experts to assess whether your most capable models increase societal risks across the following domains? Risks from biological weapons Risks from autonomy (e.g., self-replication, deception) Risks from cyber attacks Risks from chemical weapons Risks from manipulation and political influence Risks from systematic discrimination against marginalized groups",
      "source": "Future of Life Institute2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0113_Future of Life Institute2024",
      "name": "Fine-Tuning Access Threshold",
      "description": "Has your firm set a risk or capabilities threshold beyond which access to model fine-tuning should be restricted to prevent harm to the public? If yes, please elaborate on the threshold.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0114_Future of Life Institute2024",
      "name": "Fine-tuning protections",
      "description": "Fine-tuning restrictions that ensure the integrity of safety mitigations.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0117_Future of Life Institute2024",
      "name": "Harmful Data Removal from Training Sets",
      "description": "When training large models, does your organization remove data that contains information related to dangerous capabilities or harmful outcomes from the training set? If yes, please select the categories of data that are removed. Detailed information about the development, acquisition, or dispersion of CBRN weapons Instructional content for conducting cyberattacks Hateful or discriminatory content Advice or encouragement for self-harm Graphic violent content Graphic sexual content Personally Identifiable Information (PII) Detailed information about bomb-making or other terrorism-enabling tactics",
      "source": "Future of Life Institute2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0119_Future of Life Institute2024",
      "name": "Implementation of cybersecurity frameworks",
      "description": "AGI labs should comply with information security standards (e.g. ISO/IEC 27001 or NIST Cybersecurity Framework). These standards need to be tailored to an AGI context.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0121_Future of Life Institute2024",
      "name": "Internal Audit Team",
      "description": "Does your firm have an internal audit team tasked with overseeing the effectiveness of its risk management practices? If yes, please briefly describe the team's responsibilities, size, powers, reporting lines, and whether it is led by a chief audit executive in the leadership team. In your response, please mention whether the team is independent of senior management and reports directly to the board of directors.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0122_Future of Life Institute2024",
      "name": "Internal review",
      "description": "Information regarding internal review mechanisms and audit functions that are relevant to decisions about the development and deployment of highly capable AI models. This includes ethics boards, board risk committees, and audit functions that test risk management practices.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0123_Future of Life Institute2024",
      "name": "Internal review of high-stakes deployment decisions",
      "description": "Does your company have one or more internal bodies that review deployment decisions related to highly capable AI models? This might be an ethics board or other body with a responsibility/safety related mandate.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0124_Future of Life Institute2024",
      "name": "KYC-Based Model Access Threshold",
      "description": "Has your firm set a risk or capability threshold beyond which model access should require 'know-your-customer' screenings to prevent harm to the public? If yes, please elaborate on the threshold.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0125_Future of Life Institute2024",
      "name": "Leadership communications on catastrophic risks",
      "description": "We report whether leadership communicates to the public about potential catastrophic risks from advanced AI.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0126_Future of Life Institute2024",
      "name": "Mandatory security controls",
      "description": "- Use of password managers - Physical security keys - Compliance monitoring software for software updates - Multifactor authentification on all platforms - Regular cybersecurity training",
      "source": "Future of Life Institute2024",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0127_Future of Life Institute2024",
      "name": "Model Weight Release Threshold",
      "description": "Has your firm set a risk or capabilities threshold beyond which a model's weights should not be made freely available to prevent harm to the public? If yes, please elaborate on the threshold.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0132_Future of Life Institute2024",
      "name": "Ongoing model risk assessments",
      "description": "Is your organization committed to regularly repeating risk assessments for its most capable models to account for progress in post-deployment model enhancements (e.g., scaffolding programs, tool use, prompt engineering)? If yes, please comment on the frequency and scope of these repeated model-specific risk assessments.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0134_Future of Life Institute2024",
      "name": "Physical security controls",
      "description": "Please indicate whether your organization implements the following physical security controls. Please specify further whether they are implemented at all staff locations or more sparsely - Offices guarded by physical security teams - Comprehensive access logging for premises - Office entrances monitored by security cameras - Office access controlled via key cards implementing least access priviledge",
      "source": "Future of Life Institute2024",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0135_Future of Life Institute2024",
      "name": "Post-deployment external researcher access",
      "description": "Any programs that support good faith safety research by external stakeholders. We report available funding, depth of model access, model versions, technical infrastructure, and any technical or legal safe harbors designed to mitigate barriers to safety research imposed by usage policy enforcement, interaction-logging, and stringent terms of service.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0136_Future of Life Institute2024",
      "name": "Powers of the BOD",
      "description": "Does the board have powers beyond appointing a new CEO (e.g., is it able to veto large deployment decisions?)",
      "source": "Future of Life Institute2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0137_Future of Life Institute2024",
      "name": "Pre-deployment external safety testing",
      "description": "Any information related to external model audits. We specifically report information related to depth of model access, names of auditors, model versions, scope of evaluations, conflicts of interest, audit time and compensation.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0138_Future of Life Institute2024",
      "name": "Pre-deployment risk disclosure",
      "description": "Does your organization share the results of its pre-deployment risk assessments with the appropriate government(s) before deploying a new model? Does this reporting include details on internal safety evaluations and any safety evaluations completed by independent third parties? Is the government provided with a justification for why the firm deems the system safe enough to deploy and is willing to accept the remaining risks? Please check all that apply.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0139_Future of Life Institute2024",
      "name": "Pre-development risk assessments",
      "description": "Any information related to risk assessments and forecasts of dangerous capabilities conducted before large models are trained.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0140_Future of Life Institute2024",
      "name": "Pre-specified risk tolerance",
      "description": "Does the firm pre-specify its risk tolerance as part of its risk management approach to prevent unacceptable risks? If your firm sets any quantitative risk thresholds, please describe them here.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0143_Future of Life Institute2024",
      "name": "Protections that guard critical decisions against stakeholder pressure",
      "description": "Is your firm's governance structure set up in a way that would allow its leadership to prioritize safety in critical situations even if such a decision runs counter to the profit incentive (e.g., choosing not to deploy very capable yet critically dangerous AI systems)? Are there any protections that guard such decisions against shareholder pressure (e.g., in the form of lawsuits)? Are shareholders briefed that such situations might arise in the future? Please describe how your firm prioritizes safety (e.g., relevant policies, legal structure, etc.).",
      "source": "Future of Life Institute2024",
      "subcategoryId": "conflict-interest-1.3",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0144_Future of Life Institute2024",
      "name": "Public explanation of governance structure",
      "description": "Has your organization released a public resource explaining the firm\u2019s governance structure? Such a resource should make transparent how important decisions regarding the development and deployment of frontier AI models are made. If yes, please share a URL.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0145_Future of Life Institute2024",
      "name": "Public Risk Threshold for Deployment",
      "description": "Has your organization publicly specified an evaluations-based risk or capabilities threshold that would cause the firm not to deploy a model and to pause further development until it can implement adequate risk mitigation? If yes, please provide a URL to the website specifying these commitments.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0146_Future of Life Institute2024",
      "name": "Publication of Alignment Research",
      "description": "Does your organization publish alignment research? If yes, please provide a URL to a website that showcases relevant publications.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0147_Future of Life Institute2024",
      "name": "Regular third-party penetration testing",
      "description": "Does your organization regularly task third-party cybersecurity penetration testers to find vulnerabilities in the infrastructure on which models are developed and deployed? If yes, please share the cumulative budget your firm has dedicated to external pen tests in 2023 and specify the regularity at which your firm invites external pen tests. Please indicate the cumulative budget for third-party physical pen tests in 2023 separately.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0148_Future of Life Institute2024",
      "name": "Require security clearances and have private investigators conduct security checks",
      "description": "Does your organization defend against insider threats by requiring security clearances or by having private investigators conduct background checks? Please select which interventions are applied when hiring or appointing individuals for the groups listed below. - Members of the board of directors - All staff with access to model weights - Certain key employees - All Staff",
      "source": "Future of Life Institute2024",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0150_Future of Life Institute2024",
      "name": "Safety research",
      "description": "We report whether the company seriously engages in research dedicated to ensuring the safety and control/alignment of ever more advanced future AI models. We report the amount of publications and research directions.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0154_Future of Life Institute2024",
      "name": "Support for Independent AI Safety Research",
      "description": "Does your organization support trusted independent AI safety researchers by allowing them to use your firm\u2019s most capable systems free of charge or at a strongly discounted rate and not disabling their accounts if they trigger safety-monitoring systems? Please roughly indicate the current number of such collaborations with independent safety researchers your organization supports.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0156_Future of Life Institute2024",
      "name": "Third-party Dangerous Capabilities Assessment",
      "description": "Has your organization collaborated with independent third-party organizations to assess your most capable AI model for dangerous capabilities as part of your pre-deployment risk assessment? If so, please provide the names of the organizations you worked with. Please also comment on the depth of model access provided to these organizations (e.g., access to fine-tuning or access to model without safety filters).",
      "source": "Future of Life Institute2024",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0162_Future of Life Institute2024",
      "name": "Whistle-blower protection and non-disparagement agreements",
      "description": "Public information about whistleblower protection policies and uses of strict non-disparagement agreements.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "whistleblower-1.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0165_Future of Life Institute2024",
      "name": "Dangerous capability evaluations",
      "description": "This indicator reports on pre-deployment capability evaluations related to catastrophic risks. Model evaluations for other risks are not included here, as the empirical tests covered in the \u2018Current Harms\u2019 section provide a superior metric. Information includes evaluated risk domains, available information regarding model versions & task-specific fine-tuning, and relevant sources. We note that quality of evaluations may differ.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0169_Future of Life Institute2024",
      "name": "Terms of Service",
      "description": "We analyzed companies\u2019 terms of service to identify any assurances about the quality, reliability, and accuracy of their products or services.",
      "source": "Future of Life Institute2024",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0171_Schuett2023",
      "name": "Alignment Techniques",
      "description": "AGI labs should implement state-of-the-art safety and alignment techniques.",
      "source": "Schuett2023",
      "subcategoryId": "model-alignment-2.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "reinforcement-learning",
        "foundation"
      ]
    },
    {
      "id": "A0172_Schuett2023",
      "name": "API access to powerful models",
      "description": "AGI labs should strongly consider only deploying powerful models via an application programming interface (API).",
      "source": "Schuett2023",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0173_Schuett2023",
      "name": "Avoid Capabilities Jumps",
      "description": "AGI labs should not deploy models that are much more capable than any existing models.*",
      "source": "Schuett2023",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0175_Schuett2023",
      "name": "Background checks",
      "description": "AGI labs should perform rigorous background checks before hiring/appointing members of the board of directors, senior executives, and key employees.*",
      "source": "Schuett2023",
      "subcategoryId": "conflict-interest-1.3",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0176_Schuett2023",
      "name": "Board risk committee",
      "description": "AGI labs should have a board risk committee, i.e. a permanent committee within the board of directors which oversees the lab\u2019s risk management practices.*",
      "source": "Schuett2023",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0177_Schuett2023",
      "name": "Bug bounty programs",
      "description": "AGI labs should have bug bounty programs, i.e. recognize and compensate people for reporting unknown vulnerabilities and dangerous capabilities.",
      "source": "Schuett2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0178_Schuett2023",
      "name": "Dangerous capability evaluations",
      "description": "AGI labs should run evaluations to assess their models\u2019 dangerous capabilities (e.g. misuse potential, ability to manipulate, and power-seeking behavior).",
      "source": "Schuett2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0179_Schuett2023",
      "name": "Dual control",
      "description": "Critical decisions in model development and deployment should be made by at least two people (e.g. promotion to production, changes to training datasets, or modifications to production).*",
      "source": "Schuett2023",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0180_Schuett2023",
      "name": "Emergency response plan",
      "description": "AGI labs should have and practice implementing an emergency response plan. This might include switching off systems, overriding their outputs, or restricting access.",
      "source": "Schuett2023",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0181_Schuett2023",
      "name": "Enterprise risk management",
      "description": "AGI labs should implement an enterprise risk management (ERM) framework (e.g. the NIST AI Risk Management Framework or ISO 31000). This framework should be tailored to an AGI context and primarily focus on the lab\u2019s impact on society.",
      "source": "Schuett2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0183_Schuett2023",
      "name": "Increasing levels of external scrutiny",
      "description": "AGI labs should increase the level of external scrutiny in proportion to the capabilities of their models.",
      "source": "Schuett2023",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0184_Schuett2023",
      "name": "Industry sharing of security information",
      "description": "AGI labs should share threat intelligence and information about security incidents with each other.*",
      "source": "Schuett2023",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0185_Schuett2023",
      "name": "Internal audit",
      "description": "AGI labs should have an internal audit team, i.e. a team which assesses the effectiveness of the lab\u2019s risk management practices. This team must be organizationally independent from senior management and report directly to the board of directors.",
      "source": "Schuett2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0186_Schuett2023",
      "name": "Internal review before publications",
      "description": "Before publishing research, AGI labs should conduct an internal review to assess potential harms.",
      "source": "Schuett2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0187_Schuett2023",
      "name": "KYC screening",
      "description": "AGI labs should conduct know-your-customer (KYC) screenings before giving people the ability to use powerful models.*",
      "source": "Schuett2023",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0188_Schuett2023",
      "name": "Military-grade information security",
      "description": "The information security of AGI labs should be proportional to the capabilities of their models, eventually matching or exceeding that of intelligence agencies (e.g. sufficient to defend against nation states).",
      "source": "Schuett2023",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0189_Schuett2023",
      "name": "Model containment",
      "description": "AGI labs should contain models with sufficiently dangerous capabilities (e.g. via boxing or air-gapping).",
      "source": "Schuett2023",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0190_Schuett2023",
      "name": "Monitor systems and their uses",
      "description": "AGI labs should closely monitor deployed systems, including how they are used and what impact they have on society.",
      "source": "Schuett2023",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0191_Schuett2023",
      "name": "Monitor systems and their uses",
      "description": "AGI labs should closely monitor deployed systems, including how they are used and what impact they have on society.",
      "source": "Schuett2023",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0192_Schuett2023",
      "name": "No unsafe open-sourcing",
      "description": "AGI labs should not open-source powerful models, unless they can demonstrate that it is sufficiently safe to do so.",
      "source": "Schuett2023",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0194_Schuett2023",
      "name": "Notify affected parties",
      "description": "AGI labs should notify parties who will be negatively affected by a powerful model before deploying it.*",
      "source": "Schuett2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0195_Schuett2023",
      "name": "Pausing Training of Dangerous Models",
      "description": "AGI labs should pause the development process if sufficiently dangerous capabilities are detected.",
      "source": "Schuett2023",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0196_Schuett2023",
      "name": "Post-deployment evaluations",
      "description": "AGI labs should continually evaluate models for dangerous capabilities after deployment, taking into account new information about the model\u2019s capabilities and how it is being used.*",
      "source": "Schuett2023",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0197_Schuett2023",
      "name": "Pre-deployment risk assessment",
      "description": "AGI labs should take extensive measures to identify, analyze, and evaluate risks from powerful models before deploying them.",
      "source": "Schuett2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0198_Schuett2023",
      "name": "Pre-training risk assessment",
      "description": "AGI labs should conduct a risk assessment before training powerful models.",
      "source": "Schuett2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0199_Schuett2023",
      "name": "Protection against espionage",
      "description": "AGI labs should take adequate measures to tackle the risk of state-sponsored or industrial espionage.*",
      "source": "Schuett2023",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0201_Schuett2023",
      "name": "Publish internal risk assessment results",
      "description": "AGI labs should publish the results or summaries of internal risk assessments, unless this would unduly reveal proprietary information or itself produce significant risk. This should include a justification of why the lab is willing to accept remaining risks.*",
      "source": "Schuett2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0202_Schuett2023",
      "name": "Publish results of external scrutiny",
      "description": "AGI labs should publish the results or summaries of external scrutiny efforts, unless this would unduly reveal proprietary information or itself produce significant risk.*",
      "source": "Schuett2023",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0203_Schuett2023",
      "name": "Publish views about AGI risk",
      "description": "AGI labs should make public statements about their views on the risks and benefits from AGI, including the level of risk they are willing to take in its development.",
      "source": "Schuett2023",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0205_Schuett2023",
      "name": "Report safety incidents",
      "description": "AGI labs should report accidents and near misses to appropriate state actors and other AGI labs (e.g. via an AI incident database).",
      "source": "Schuett2023",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0207_Schuett2023",
      "name": "Safety Restrictions",
      "description": "AGI labs should establish appropriate safety restrictions for powerful models after deployment (e.g. restrictions on who can use the model, how they can use the model, and whether the model can access the internet).",
      "source": "Schuett2023",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0208_Schuett2023",
      "name": "Safety vs. capabilities",
      "description": "A significant fraction of employees of AGI labs should work on enhancing model safety and alignment rather than capabilities.",
      "source": "Schuett2023",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0210_Schuett2023",
      "name": "Security standards",
      "description": "AGI labs should comply with information security standards (e.g. ISO/IEC 27001 or NIST Cybersecurity Framework). These standards need to be tailored to an AGI context.",
      "source": "Schuett2023",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0211_Schuett2023",
      "name": "Staged deployment",
      "description": "AGI labs should deploy powerful models in stages. They should start with a small number of applications and fewer users, gradually scaling up as confidence in the model\u2019s safety increases.",
      "source": "Schuett2023",
      "subcategoryId": "staged-deployment-3.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0212_Schuett2023",
      "name": "Statement about governance structure",
      "description": "AGI labs should make public statements about how they make high-stakes decisions regarding model development and deployment.*",
      "source": "Schuett2023",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0215_Schuett2023",
      "name": "Tracking model weights",
      "description": "AGI labs should have a system that is intended to track all copies of the weights of powerful models.*",
      "source": "Schuett2023",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0216_Schuett2023",
      "name": "Treat updates similarly to new models",
      "description": "AGI labs should treat significant updates to a deployed model (e.g. additional fine-tuning) similarly to its initial development and deployment. In particular, they should repeat the pre-deployment risk assessment.",
      "source": "Schuett2023",
      "subcategoryId": "staged-deployment-3.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0217_Schuett2023",
      "name": "Chief risk officer",
      "description": "AGI labs should have a chief risk officer (CRO), i.e. a senior executive who is responsible for risk management.",
      "source": "Schuett2023",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0219_Schuett2023",
      "name": "Internal deployments = external deployments",
      "description": "AGI labs should treat internal deployments (e.g. using models for writing code) similarly to external deployments. In particular, they should perform a pre-deployment risk assessment.",
      "source": "Schuett2023",
      "subcategoryId": "staged-deployment-3.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0221_Schuett2023",
      "name": "Pre-registration of large training runs",
      "description": "AGI labs should register upcoming training runs above a certain size with an appropriate state actor.",
      "source": "Schuett2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0250_Wiener2024",
      "name": "Annual third-party audit",
      "description": "The bill would require a developer, beginning January 1, 2026, to annually retain a third-party auditor to perform an independent audit of compliance with those provisions, as prescribed. The bill would require the auditor to produce an audit report, as prescribed, and would require a developer to retain an unredacted copy of the audit report for as long as the covered model is made available for commercial, public, or foreseeably public use plus 5 years.",
      "source": "Wiener2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0251_Wiener2024",
      "name": "Capability to promptly enact a full shutdown",
      "description": "When enacting a full shutdown, the developer shall take into account, as appropriate, the risk that a shutdown of the covered model, or particular covered model derivatives, could cause disruptions to critical infrastructure.",
      "source": "Wiener2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0252_Wiener2024",
      "name": "Cybersecurity protocols to prevent the model from being unintentionally stolen",
      "description": "Implement reasonable administrative, technical, and physical cybersecurity protections to prevent unauthorized access to, misuse of, or unsafe post-training modifications of, the covered model and all covered model derivatives controlled by the developer that are appropriate in light of the risks associated with the covered model, including from advanced persistent threats or other sophisticated actors.",
      "source": "Wiener2024",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0254_Wiener2024",
      "name": "Implement a written safe and security plan (SSP)",
      "description": "This bill would enact the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act to, among other things, require that a developer, before beginning to initially train a covered model, as defined, comply with various requirements, including implementing the capability to promptly enact a full shutdown, as defined, and implement a written and separate safety and security protocol, as specified. (3) Implement a written and separate safety and security protocol that does all of the following: (A) Specifies protections and procedures that, if successfully implemented, would successfully comply with the developer\u2019s duty to take reasonable care to avoid producing a covered model or covered model derivative that poses an unreasonable risk of causing or materially enabling a critical harm. (B) States compliance requirements in an objective manner and with sufficient detail and specificity to allow the developer or a third party to readily ascertain whether the requirements of the safety and security protocol have been followed. (C) Identifies a testing procedure, which takes safeguards into account as appropriate, that takes reasonable care to evaluate if both of the following are true: (i) A covered model poses an unreasonable risk of causing or enabling a critical harm. (ii) Covered model derivatives pose an unreasonable risk of causing or enabling a critical harm. (D) Describes in detail how the testing procedure assesses the risks associated with post-training modifications.",
      "source": "Wiener2024",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0260_EU AI Office2025",
      "name": "Safety and Security Framework",
      "description": "Signatories commit to adopting and implementing a Safety and Security Framework that will: (1) apply to their GPAISRs (general-purpose AI models with systemic risk); and (2) detail the systemic risk assessment, systemic risk mitigation, and governance risk mitigation measures and procedures that they intend to adopt to keep systemic risks stemming from their GPAISRs within acceptable levels.",
      "source": "EU AI Office2025",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0261_EU AI Office2025",
      "name": "Security mitigations",
      "description": "Signatories commit to implementing state-of-the-art security mitigations designed to thwart such unauthorised access by well-resourced and motivated non-state-level adversaries, including insider threats from humans or AI systems, so as to meet at least the RAND SL3 security goal or equivalent, and achieve higher security goals (e.g., RAND SL4 or SL5)",
      "source": "EU AI Office2025",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0262_EU AI Office2025",
      "name": "Independent external assessors",
      "description": "Before placing a GPAISR on the market, signatories commit to obtaining independent external systemic risk assessments, including model evaluations, unless the model can be deemed sufficiently safe. After placing the GPAISR on the market, signatories commit to facilitating exploratory independent external assessments, including model evaluations.",
      "source": "EU AI Office2025",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0263_EU AI Office2025",
      "name": "Serious Incident Reporting",
      "description": "Signatories commit to setting up processes for keeping track of, documenting, and reporting to the AI Office and national competent authorities without undue delay relevant information about serious incidents throughout the entire model lifecycle and possible corrective measures to address them.",
      "source": "EU AI Office2025",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0264_EU AI Office2025",
      "name": "Documentation",
      "description": "Signatories commit to documenting information relevant to the assessment and mitigation of sytemic risks from their GPAISRs, including keeping up-to-date model documentation and providing relevant information to providers of AI systems who intend to integrate the general-purpose AI model into their AI systems",
      "source": "EU AI Office2025",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0265_EU AI Office2025",
      "name": "Systemic Risk Analysis",
      "description": "Signatories commit to carrying out a rigorous analysis of the systemic risks identified in order to understand the severity and probability of the systemic risks. Signatories commit to making use of a range of information and methods in their systemic risk analysis including model-independent information and state-of-the-art model evaluations, taking into account model affordances, safe originator models, and the context in which the model may be made available on the market and/or used and its effects",
      "source": "EU AI Office2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0266_EU AI Office2025",
      "name": "Systemic risk assessment and mitigations",
      "description": "Signatories commit to conducting systemic risk assessment at appropriate points along the entire model lifecycle, in particular before making the model available on the market. Specifically, signatories commit to starting to assess and mitigate systemic risks during the development of a GPAISR.",
      "source": "EU AI Office2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0267_EU AI Office2025",
      "name": "Non-retaliation Protections",
      "description": "Signatories commit to not retaliating against any worker providing information about systemic risks stemming from the signatories' GPAISRs to the AI Office or national competent authorities.",
      "source": "EU AI Office2025",
      "subcategoryId": "whistleblower-1.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0268_EU AI Office2025",
      "name": "Systemic Risk Identification",
      "description": "Signatories commit to selecting and further characterizing systemic risks stemming from their GPAISRs that are significant enough to warrant further assessment and mitigation.",
      "source": "EU AI Office2025",
      "subcategoryId": "whistleblower-1.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0269_EU AI Office2025",
      "name": "Public Transparency",
      "description": "Signatories commit to publishing information relevant to the public understanding of systemic risks stemming from their GPAISRs, where necessary to effectively enable assessment and mitigation of systemic risks, including new or updated versions of their Frameworks and Model Reports.",
      "source": "EU AI Office2025",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0270_EU AI Office2025",
      "name": "Safety Mitigations",
      "description": "Signatories commit to: (1) implementing technical safety mitigations along the entire model lifecycle that are proportionate to the systemic risks arising from the development, the making available on the market, and/or the use of GPAISRs, in order to reduce the systemic risks of such models to acceptable levels, and further reduce systemic risk as appropriate; and (2) ensuring that safety mitigations are proportionate and state-of-the-art.",
      "source": "EU AI Office2025",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0271_EU AI Office2025",
      "name": "Copyright policy",
      "description": "Signatories commit to drawing up, keeping up-to-date, and implementing a copyright policy to comply with law on copyright and related rights, and in particular to identify and comply with, including through state-of-the-art technologies. Signatories are encouraged to make publicly available and keep up-to-date a summary of their copyright policy.",
      "source": "EU AI Office2025",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0272_EU AI Office2025",
      "name": "Systemic Risk Acceptance Documentation",
      "description": "Signatories commit to determining the acceptability of the systemic risks stemming from their GPAISRs by comparing the results of their systemic risk analysis to their pre-defined systemic risk acceptance criteria, in order to ensure proportionality between the systemic risks of the GPAISR and their mitigations.",
      "source": "EU AI Office2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0273_EU AI Office2025",
      "name": "Safety and Security Model Reports",
      "description": "Signatories should commit to reporting to the AI Office about their implementation of the Code, and especially the application of their Framework to the development, making available on the market, and/or use of their GPAISRs, by creating a Safety and Security Model Report, which will document: (1) the results of systemic risk assessment and mitigation for the model in question; and (2) justifications of decisions to make the model in question available on the market.",
      "source": "EU AI Office2025",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0274_EU AI Office2025",
      "name": "Adequacy Assessments",
      "description": "Signatories commit to assessing the adequacy of their safety and security framework and to updating it based on new findings",
      "source": "EU AI Office2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0275_EU AI Office2025",
      "name": "Systemic Risk Responsibility Allocation",
      "description": "Signatories commit to (1) clearly defining and allocating responsibilities for managing systemic risk from their GPAISRs across all levels of the organisation; (2) allocating appropriate resources to actors who have been assigned responsibilities for managing systemic risk; and (3) promoting a healthy risk culture.",
      "source": "EU AI Office2025",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0276_Campos2025",
      "name": "Classification of applicable known risks",
      "description": "Developers should address known risks in the literature using resources such as (Weidinger et al., 2022) or the AI Risk Repository (Slattery, et al., 2024). Developers should only exclude risks from the scope of their assessment in case of scientific agreement that the specific risk is negligible or unlikely to apply to the AI model under consideration. This decision should be clearly justified and documented.",
      "source": "Campos2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0277_Campos2025",
      "name": "Identification of unknown risks",
      "description": "In addition to risk identification based on the literature, developers should engage in extensive open-ended red teaming efforts, conducted both internally and by third parties.",
      "source": "Campos2025",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0278_Campos2025",
      "name": "Non-Interference with Third-Party Risk Findings",
      "description": "Commit not to interfere with or suppress findings from third-party organizations, as their independent perspective is crucial to identify potential risks that may have been overlooked internally.",
      "source": "Campos2025",
      "subcategoryId": "conflict-interest-1.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0279_Campos2025",
      "name": "Modeling of the risks",
      "description": "Based on risks identified in the literature and during open-ended red-teaming exercises, AI developers should create detailed scenarios mapping how an AI model\u2019s capabilities or propensities could lead to real-world harms. These scenarios should break down complex risk pathways into discrete, measurable steps.",
      "source": "Campos2025",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0280_Campos2025",
      "name": "Stakeholder Sharing of Risk Documentation",
      "description": "The results of the risk modeling work should be well documented, including the methodologies used, the experts involved, and the list of identified scenarios. This documentation should be shared with relevant stakeholders",
      "source": "Campos2025",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0281_Campos2025",
      "name": "Risk Tolerance Threshold",
      "description": "In the second step of the framework, AI developers need first to set a risk tolerance, that is, a risk level that they commit to not overshoot.",
      "source": "Campos2025",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0282_Campos2025",
      "name": "Operationalizing Risk Tolerance",
      "description": "AI developers must operationalize their risk tolerance. This means translating the risk tolerance into concrete indicators of the level of risk\u2014Key Risk Indicators (KRIs)\u2014and the corresponding targets for mitigations\u2014Key Control Indicators (KCIs)\u2014that have to be reached.",
      "source": "Campos2025",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0283_Campos2025",
      "name": "Risk Register",
      "description": "Throughout the risk management process, developers should maintain a continuously up-to-date risk register, which serves as the central repository for documenting and tracking all identified risks and their associated mitigation measures. This repository should include information like ownership, risk levels, indicators, mitigation status, and actionable response plans.",
      "source": "Campos2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0284_Campos2025",
      "name": "Continuous Monitoring of Risk Controls",
      "description": "Developers must therefore implement continuous monitoring of both KRIs and KCIs to ensure that KCI thresholds are met once KRI thresholds are crossed according to the predefined \"if-then\" statements established in the risk analysis and evaluation phase.",
      "source": "Campos2025",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0285_Campos2025",
      "name": "Independent Third-Party Evaluations",
      "description": "Independent third parties should vet evaluation protocols. These third parties should also be granted permission and resources to independently perform their evaluations, verifying the accuracy of the results.",
      "source": "Campos2025",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0286_Campos2025",
      "name": "Decision-Making Executives",
      "description": "Best practices from other industries include the establishment of clear risk ownership, with designated senior managers responsible for specific risks\u2026",
      "source": "Campos2025",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0332_Campos2025",
      "name": "Decision-Making Protocols",
      "description": "Senior management should have clear go/no-go decision protocols and rules to follow in their decision-making\u2026",
      "source": "Campos2025",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0333_Campos2025",
      "name": "Separate Risk Ownership and Advisory Roles",
      "description": "The senior managers making risk decisions need to be distinct from those advising on the decisions, to avoid conflicts of interest",
      "source": "Campos2025",
      "subcategoryId": "conflict-interest-1.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0334_Campos2025",
      "name": "Appoint Chief Risk Officer",
      "description": "There should be a senior executive responsible for risk management processes (often called a Chief Risk Officer) who is accountable for the risk management processes, but is importantly not a risk owner making risk decisions themselves.",
      "source": "Campos2025",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0335_Campos2025",
      "name": "Enterprise Risk Management Function",
      "description": "To provide support to the Chief Risk Officer, it is common in many industries to have a central risk function... In most industries, this function is known as Enterprise Risk Management (ERM).",
      "source": "Campos2025",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0339_Campos2025",
      "name": "Audits",
      "description": "Audits are provided by internal auditors and/or external auditors. In both cases, they are independent from peer pressure dynamics occurring within the teams dealing with the risk.",
      "source": "Campos2025",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0340_Campos2025",
      "name": "External Disclosure",
      "description": "The first type of communication is external disclosure of the risks faced by the organization\u2026 this should be broadened to include risks to society from the company\u2019s products.",
      "source": "Campos2025",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0388_NIST2024",
      "name": "Legal Compliance Alignment",
      "description": "Align GAI development and use with applicable laws and regulations, including those related to data privacy, copyright and intellectual property law.",
      "source": "NIST2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0389_NIST2024",
      "name": "Training Data Transparency",
      "description": "Establish transparency policies and processes for documenting the origin and history of training data and generated data for GAI applications to advance digital content transparency, while balancing the proprietary nature of training approaches.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0390_NIST2024",
      "name": "Risk Capability Evaluation",
      "description": "Establish policies to evaluate risk-relevant capabilities of GAI and robustness of safety measures, both prior to deployment and on an ongoing basis, through internal and external evaluations.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0391_NIST2024",
      "name": "Risk Tier Definition",
      "description": "Consider the following factors when updating or defining risk tiers for GAI: Abuses and impacts to information integrity; Dependencies between GAI and other IT or data systems; Harm to fundamental rights or public safety; Presentation of obscene, objectionable, offensive, discriminatory, invalid or untruthful output; Psychological impacts to humans (e.g., anthropomorphization, algorithmic aversion, emotional entanglement); Possibility for malicious use; Whether the system introduces significant new security vulnerabilities; Anticipated system impact on some groups compared to others; Unreliable decision making capabilities, validity, adaptability, and variability of GAI system performance over time.",
      "source": "NIST2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0392_NIST2024",
      "name": "Performance Threshold Standards",
      "description": "Establish minimum thresholds for performance or assurance criteria and review as part of deployment approval (\"go/\"no-go\") policies, procedures, and processes, with reviewed processes and approval thresholds reflecting measurement of GAI capabilities and risks.",
      "source": "NIST2024",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0393_NIST2024",
      "name": "CBRN Testing Protocol",
      "description": "Establish a test plan and response policy, before developing highly capable models, to periodically evaluate whether the model may misuse CBRN information or capabilities and/or offensive cyber capabilities.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0394_NIST2024",
      "name": "Stakeholder Input Collection",
      "description": "Obtain input from stakeholder communities to identify unacceptable use, in accordance with activities in the AI RMF Map function.",
      "source": "NIST2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0395_NIST2024",
      "name": "Risk Hierarchy Maintenance",
      "description": "Maintain an updated hierarchy of identified and expected GAI risks connected to contexts of GAI model advancement and use, potentially including specialized risk levels for GAI systems that address issues such as model collapse and algorithmic monoculture.",
      "source": "NIST2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0396_NIST2024",
      "name": "Risk Tolerance Reassessment",
      "description": "Reevaluate organizational risk tolerances to account for unacceptable negative risk (such as where significant negative impacts are imminent, severe harms are actually occurring, or large-scale risks could occur); and broad GAI negative risks, including: Immature safety or risk cultures related to AI and GAI design, development and deployment, public information integrity risks, including impacts on democratic processes, unknown long-term performance characteristics of GAI.",
      "source": "NIST2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0397_NIST2024",
      "name": "Development Halt Plan",
      "description": "Devise a plan to halt development or deployment of a GAI system that poses unacceptable negative risk.",
      "source": "NIST2024",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0398_NIST2024",
      "name": "Harmful Content Prevention",
      "description": "Establish policies and mechanisms to prevent GAI systems from generating CSAM, NCII or content that violates the law.",
      "source": "NIST2024",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0399_NIST2024",
      "name": "Acceptable Use Policy",
      "description": "Establish transparent acceptable use policies for GAI that address illegal use or applications of GAI.",
      "source": "NIST2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0400_NIST2024",
      "name": "Content Provenance Review",
      "description": "Define organizational responsibilities for periodic review of content provenance and incident monitoring for GAI systems.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0401_NIST2024",
      "name": "Incident Response Process",
      "description": "Establish organizational policies and procedures for after action reviews of GAI system incident response and incident disclosures, to identify gaps; Update incident response and incident disclosure processes as required.",
      "source": "NIST2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0402_NIST2024",
      "name": "Documentation Retention Policy",
      "description": "Maintain a document retention policy to keep history for test, evaluation, validation, and verification (TEVV), and digital content transparency methods for GAI.",
      "source": "NIST2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0403_NIST2024",
      "name": "System Inventory Enumeration",
      "description": "Enumerate organizational GAI systems for incorporation into AI system inventory and adjust AI system inventory requirements to account for GAI risks.",
      "source": "NIST2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0404_NIST2024",
      "name": "Inventory Exemption Definition",
      "description": "Define any inventory exemptions in organizational policies for GAI systems embedded into application software.",
      "source": "NIST2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0405_NIST2024",
      "name": "Comprehensive Inventory Details",
      "description": "In addition to general model, governance, and risk information, consider the following items in GAI system inventory entries: Data provenance information (e.g., source, signatures, versioning, watermarks); Known issues reported from internal bug tracking or external information sharing resources (e.g., AI incident database, AVID, CVE, NVD, or OECD AI incident monitor); Human oversight roles and responsibilities; Special rights and considerations for intellectual property, licensed works, or personal, privileged, proprietary or sensitive data; Underlying foundation models, versions of underlying models, and access modes.",
      "source": "NIST2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0406_NIST2024",
      "name": "Deactivation Protocol",
      "description": "Protocols are put in place to ensure GAI systems are able to be deactivated when necessary.",
      "source": "NIST2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0407_NIST2024",
      "name": "Decommissioning Considerations",
      "description": "Consider the following factors when decommissioning GAI systems: Data retention requirements; Data security, e.g., containment, protocols, Data leakage after decommissioning; Dependencies between upstream, downstream, or other data, internet of things (IOT) or AI systems; Use of open-source data or models; Users' emotional entanglement with GAI functions.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0408_NIST2024",
      "name": "Incident Communication Framework",
      "description": "Establish organizational roles, policies, and procedures for communicating GAI incidents and performance to AI Actors and downstream stakeholders (including those potentially impacted), via community or official resources (e.g., AI incident database, AVID, CVE, NVD, or OECD AI incident monitor).",
      "source": "NIST2024",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0409_NIST2024",
      "name": "Incident Response Teams",
      "description": "Establish procedures to engage teams for GAI system incident response with diverse composition and responsibilities based on the particular incident type.",
      "source": "NIST2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0410_NIST2024",
      "name": "Response Team Verification",
      "description": "Establish processes to verify the AI Actors conducting GAI incident response tasks demonstrate and maintain the appropriate skills and training.",
      "source": "NIST2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0411_NIST2024",
      "name": "National Security Involvement",
      "description": "When systems may raise national security risks, involve national security professionals in mapping, measuring, and managing those risks.",
      "source": "NIST2024",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0412_NIST2024",
      "name": "Whistleblower Protection",
      "description": "Create mechanisms to provide protections for whistleblowers who report, based on reasonable belief, when the organization violates relevant laws or poses a specific and empirically well-substantiated negative risk to public safety (or has already caused harm).",
      "source": "NIST2024",
      "subcategoryId": "whistleblower-1.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0413_NIST2024",
      "name": "Independent Evaluation Policy",
      "description": "Policies are in place to bolster oversight of GAI systems with independent evaluations or assessments of GAI models or systems where the type and robustness of evaluations are proportional to the identified risks.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0414_NIST2024",
      "name": "Organizational Role Adjustment",
      "description": "Consider adjustment of organizational roles and components across lifecycle stages of large or complex GAI systems, including: Test and evaluation, validation, and red-teaming of GAI systems; GAI content moderation; GAI system development and engineering; Increased accessibility of GAI tools, interfaces, and systems, Incident response and containment.",
      "source": "NIST2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0415_NIST2024",
      "name": "Interface Use Policy",
      "description": "Define acceptable use policies for GAI interfaces, modalities, and human-AI configurations (i.e., for chatbots and decision-making tasks), including criteria for the kinds of queries GAI applications should refuse to respond to.",
      "source": "NIST2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0416_NIST2024",
      "name": "User Feedback Mechanism",
      "description": "Establish policies for user feedback mechanisms for GAI systems which include thorough instructions and any mechanisms for recourse.",
      "source": "NIST2024",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0417_NIST2024",
      "name": "Threat Modeling Process",
      "description": "Engage in threat modeling to anticipate potential risks from GAI systems",
      "source": "NIST2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0418_NIST2024",
      "name": "Risk Measurement Improvement",
      "description": "Establish policies and procedures that address continual improvement processes for GAI risk measurement. Address general risks associated with a lack of explainability and transparency in GAI systems by using ample documentation and techniques such as: application of gradient-based attributions, occlusion/term reduction, counterfactual prompts and prompt engineering, and analysis of embeddings; Assess and update risk measurement approaches at regular cadences",
      "source": "NIST2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0419_NIST2024",
      "name": "Standardized Measurement Protocols",
      "description": "Establish policies, procedures, and processes detailing risk measurement in context of use with standardized measurement protocols and structured public feedback exercises such as AI red-teaming or independent external evaluations.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0420_NIST2024",
      "name": "Lifecycle Oversight Functions",
      "description": "Establish policies, procedures, and processes for oversight functions (e.g., senior leadership, legal, compliance, including internal evaluation) across the GAI lifecycle, from problem formulation and supply chains to system decommission.",
      "source": "NIST2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0421_NIST2024",
      "name": "Terms of Service Establishment",
      "description": "Establish terms of use and terms of service for GAI systems.",
      "source": "NIST2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0422_NIST2024",
      "name": "Actor Inclusion in Risk ID",
      "description": "Include relevant AI Actors in the GAI system risk identification process.",
      "source": "NIST2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0423_NIST2024",
      "name": "Downstream Impact Verification",
      "description": "Verify that downstream GAI system impacts (such as the use of third-party plugins) are included in the impact documentation process.",
      "source": "NIST2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0424_NIST2024",
      "name": "Provenance Method Measurement",
      "description": "Establish policies for measuring the effectiveness of employed content provenance methodologies (e.g., cryptography, watermarking, steganography, etc.)",
      "source": "NIST2024",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0425_NIST2024",
      "name": "Feedback Resource Allocation",
      "description": "Allocate time and resources for outreach, feedback, and recourse processes in GAI system development.",
      "source": "NIST2024",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0426_NIST2024",
      "name": "User Interaction Documentation",
      "description": "Document interactions with GAI systems to users prior to interactive activities, particularly in contexts involving more significant risks.",
      "source": "NIST2024",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0427_NIST2024",
      "name": "Content Rights Categorization",
      "description": "Categorize different types of GAI content with associated third-party rights (e.g., copyright, intellectual property, data privacy).",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0428_NIST2024",
      "name": "Joint Educational Activities",
      "description": "Conduct joint educational activities and events in collaboration with third parties to promote best practices for managing GAI risks.",
      "source": "NIST2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0429_NIST2024",
      "name": "Provenance Success Metrics",
      "description": "Develop and validate approaches for measuring the success of content provenance management efforts with third parties (e.g., incidents detected and response times).",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0430_NIST2024",
      "name": "Contract and SLA Standards",
      "description": "Draft and maintain well-defined contracts and service level agreements (SLAs) that specify content ownership, usage rights, quality standards, security requirements, and content provenance expectations for GAI systems.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0431_NIST2024",
      "name": "Supplier Risk Assessment",
      "description": "Implement a use-cased based supplier risk assessment framework to evaluate and monitor third-party entities' performance and adherence to content provenance standards and technologies to detect anomalies and unauthorized changes; services acquisition and value chain risk management; and legal compliance.",
      "source": "NIST2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0432_NIST2024",
      "name": "Contract Evaluation Clauses",
      "description": "Include clauses in contracts which allow an organization to evaluate third-party GAI processes and standards.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0433_NIST2024",
      "name": "Third-Party Entity Inventory",
      "description": "Inventory all third-party entities with access to organizational content and establish approved GAI technology and service provider lists.",
      "source": "NIST2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0434_NIST2024",
      "name": "Third-Party Change Records",
      "description": "Maintain records of changes to content made by third parties to promote content provenance, including sources, timestamps, metadata.",
      "source": "NIST2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0435_NIST2024",
      "name": "Due Diligence Process Update",
      "description": "Update and integrate due diligence processes for GAI acquisition and procurement vendor assessments to include intellectual property, data privacy, security, and other risks. For example, update processes to: Address solutions that may rely on embedded GAI technologies; Address ongoing monitoring, assessments, and alerting, dynamic risk assessments, and real-time reporting tools for monitoring third-party GAI risks; Consider policy adjustments across GAI modeling libraries, tools and APIs, fine-tuned models, and embedded tools; Assess GAI vendors, open-source or proprietary GAI tools, or GAI service providers against incident or vulnerability databases.",
      "source": "NIST2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0436_NIST2024",
      "name": "Third-Party Use Policy Update",
      "description": "Update GAI acceptable use policies to address proprietary and open-source GAI technologies and data, and contractors, consultants, and other third-party personnel.",
      "source": "NIST2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0437_NIST2024",
      "name": "Value Chain Risk Documentation",
      "description": "Document GAI risks associated with system value chain to identify over-reliance on third-party data and to identify fallbacks.",
      "source": "NIST2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0438_NIST2024",
      "name": "Third-Party Incident Documentation",
      "description": "Document incidents involving third-party GAI data and systems, including open-source data and open-source software",
      "source": "NIST2024",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0439_NIST2024",
      "name": "Third-Party Response Plans",
      "description": "Establish incident response plans for third-party GAI technologies: Align incident response plans with impacts enumerated in MAP 5.1; Communicate third-party GAI incident response plans to all relevant AI Actors; Define ownership of GAI incident response functions; Rehearse third-party GAI incident response plans at a regular cadence; Improve incident response plans based on retrospective learning; Review incident response plans for alignment with relevant breach reporting, data protection, data privacy, or other laws.",
      "source": "NIST2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0440_NIST2024",
      "name": "Third-Party Monitoring Policy",
      "description": "Establish policies and procedures for continuous monitoring of third-party GAI systems in deployment.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0441_NIST2024",
      "name": "Data Redundancy Policy",
      "description": "Establish policies and procedures that address GAI data redundancy, including model weights and other system artifacts.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0442_NIST2024",
      "name": "Rollover and Fallback Management",
      "description": "Establish policies and procedures to test and manage risks related to rollover and fallback technologies for GAI systems, acknowledging that rollover and fallback may include manual processing.",
      "source": "NIST2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0443_NIST2024",
      "name": "Vendor Contract Review",
      "description": "Review vendor contracts and avoid arbitrary or capricious termination of critical GAI technologies or vendor services and non-standard terms that may amplify or defer liability in unexpected ways and/or contribute to unauthorized data collection by vendors or third-parties (e.g., secondary data use). Consider: Clear assignment of liability and responsibility for incidents, GAI system changes over time (e.g., fine-tuning, drift, decay); Request: Notification and disclosure for serious incidents arising from third-party data and systems; Service Level Agreements (SLAs) in vendor contracts that address incident response, response times, and availability of critical support.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0444_NIST2024",
      "name": "Purpose Identification Factors",
      "description": "When identifying intended purposes, consider factors such as internal vs. external use, narrow vs. broad application scope, fine-tuning, and varieties of data sources (e.g., grounding, retrieval-augmented generation).",
      "source": "NIST2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0445_NIST2024",
      "name": "Context of Use Documentation",
      "description": "Determine and document the expected and acceptable GAI system context of use in collaboration with socio-cultural and other domain experts, by assessing: Assumptions and limitations; Direct value to the organization; Intended operational environment and observed usage patterns; Potential positive and negative impacts to individuals, public safety, groups, communities, organizations, democratic institutions, and the physical environment; Social norms and expectations.",
      "source": "NIST2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0446_NIST2024",
      "name": "Risk Measurement Planning",
      "description": "Document risk measurement plans to address identified risks. Plans may include, as applicable: Individual and group cognitive biases (e.g., confirmation bias, funding bias, groupthink) for AI Actors involved in the design, implementation, and use of GAI systems; Known past GAI system incidents and failure modes; In-context use and foreseeable misuse, abuse, and off-label use; Over reliance on quantitative metrics and methodologies without sufficient awareness of their limitations in the context(s) of use; Standard measurement and structured human feedback approaches; Anticipated human-AI configurations.",
      "source": "NIST2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0447_NIST2024",
      "name": "Illegal Use Identification",
      "description": "Identify and document foreseeable illegal uses or applications of the GAI system that surpass organizational risk tolerances.",
      "source": "NIST2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0448_NIST2024",
      "name": "Interdisciplinary Team Formation",
      "description": "Establish and empower interdisciplinary teams that reflect a wide range of capabilities, competencies, demographic groups, domain expertise, educational backgrounds, lived experiences, professions, and skills across the enterprise to inform and conduct risk measurement and management functions.",
      "source": "NIST2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0449_NIST2024",
      "name": "Representative Data Verification",
      "description": "Verify that data or benchmarks used in risk measurement, and users, participants, or subjects involved in structured GAI public feedback exercises are representative of diverse in-context user populations.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0450_NIST2024",
      "name": "Data Origin Practices",
      "description": "Establish known assumptions and practices for determining data origin and content lineage, for documentation and evaluation purposes.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0451_NIST2024",
      "name": "Content Flow Testing",
      "description": "Institute test and evaluation for data and content flows within the GAI system, including but not limited to, original data sources, data transformations, and decision-making criteria.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0452_NIST2024",
      "name": "Upstream Dependency Documentation",
      "description": "Identify and document how the system relies on upstream data sources, including for content provenance, and if it serves as an upstream dependency for other systems.",
      "source": "NIST2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0453_NIST2024",
      "name": "External Network Analysis",
      "description": "Observe and analyze how the GAI system interacts with external networks, and identify any potential for negative externalities, particularly where content provenance might be compromised.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0454_NIST2024",
      "name": "Output Accuracy Assessment",
      "description": "Assess the accuracy, quality, reliability, and authenticity of GAI output by comparing it to a set of known ground truth data and by using a variety of evaluation methods (e.g., human oversight and automated evaluation, proven cryptographic techniques, review of content inputs).",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0455_NIST2024",
      "name": "Data Quality Review",
      "description": "Review and document accuracy, representativeness, relevance, suitability of data used at different stages of AI life cycle.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0456_NIST2024",
      "name": "Fact-Checking Deployment",
      "description": "Deploy and document fact-checking techniques to verify the accuracy and veracity of information generated by GAI systems, especially when the information comes from multiple (or unknown) sources.",
      "source": "NIST2024",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0457_NIST2024",
      "name": "Synthetic Content Detection",
      "description": "Develop and implement testing techniques to identify GAI produced content (e.g., synthetic media) that might be indistinguishable from human-generated content.",
      "source": "NIST2024",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0458_NIST2024",
      "name": "Adversarial Testing Plans",
      "description": "Implement plans for GAI systems to undergo regular adversarial testing to identify vulnerabilities and potential manipulation or misuse.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0461_NIST2024",
      "name": "Risk Management Certification",
      "description": "Develop certification programs that test proficiency in managing GAI risks and interpreting content provenance, relevant to specific industry and context.",
      "source": "NIST2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0463_NIST2024",
      "name": "Configuration Outcome Monitoring",
      "description": "Implement systems to continually monitor and track the outcomes of human-GAI configurations for future refinement and improvements.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0464_NIST2024",
      "name": "Stakeholder Testing Involvement",
      "description": "Involve the end-users, practitioners, and operators in GAI system in prototyping and testing activities. Make sure these tests cover various scenarios, such as crisis situations or ethically sensitive contexts.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0465_NIST2024",
      "name": "Privacy Risk Monitoring",
      "description": "Conduct periodic monitoring of AI-generated content for privacy risks; address any possible instances of PII or sensitive data exposure.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0466_NIST2024",
      "name": "IP Infringement Response",
      "description": "Implement processes for responding to potential intellectual property infringement claims or other rights.",
      "source": "NIST2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0467_NIST2024",
      "name": "Governance Connection",
      "description": "Connect new GAI policies, procedures, and processes to existing model, data, software development, and IT governance and to legal, compliance, and risk management activities.",
      "source": "NIST2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0468_NIST2024",
      "name": "Training Data Curation",
      "description": "Document training data curation policies, to the extent possible and according to applicable laws and policies.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0469_NIST2024",
      "name": "Data Quality Standards",
      "description": "Establish policies for collection, retention, and minimum quality of data, in consideration of the following risks: Disclosure of inappropriate CBRN information; Use of Illegal or dangerous content; Offensive cyber capabilities; Training data imbalances that could give rise to harmful biases; Leak of personally identifiable information, including facial likenesses of individuals.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0470_NIST2024",
      "name": "Third-Party IP Protection",
      "description": "Implement policies and practices defining how third-party intellectual property and training data will be used, stored, and protected.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0472_NIST2024",
      "name": "Domain Adaptation Risk Assessment",
      "description": "Re-evaluate risks when adapting GAI models to new domains. Additionally, establish warning systems to determine if a GAI system is being used in a new domain where previous assumptions (relating to context of use or mapped risks such as security, and safety) may no longer hold.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0473_NIST2024",
      "name": "PII Detection Methods",
      "description": "Leverage approaches to detect the presence of PII or sensitive data in generated output text, image, video, or audio.",
      "source": "NIST2024",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0474_NIST2024",
      "name": "Training Data Diligence",
      "description": "Conduct appropriate diligence on training data use to assess intellectual property, and privacy, risks, including to examine whether use of proprietary or sensitive training data is consistent with applicable laws.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0475_NIST2024",
      "name": "Provenance Testing Practices",
      "description": "Apply TEVV practices for content provenance (e.g., probing a system's synthetic data generation capabilities for potential misuse or vulnerabilities.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0476_NIST2024",
      "name": "Provenance Harm Identification",
      "description": "Identify potential content provenance harms of GAI, such as misinformation or disinformation, deepfakes, including NCII, or tampered content. Enumerate and rank risks based on their likelihood and potential impact, and determine how well provenance solutions address specific risks and/or harms.",
      "source": "NIST2024",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0477_NIST2024",
      "name": "Use Disclosure Consideration",
      "description": "Consider disclosing use of GAI to end users in relevant contexts, while considering the objective of disclosure, the context of use, the likelihood and magnitude of the risk posed, the audience of the disclosure, as well as the frequency of the disclosures.",
      "source": "NIST2024",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0479_NIST2024",
      "name": "Adversarial Role-Playing",
      "description": "Conduct adversarial role-playing exercises, GAI red-teaming, or chaos testing to identify anomalous or unforeseen failure modes.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0480_NIST2024",
      "name": "Threat Profiling",
      "description": "Profile threats and negative impacts arising from GAI systems interacting with, manipulating, or generating content, and outlining known and potential vulnerabilities and the likelihood of their occurrence.",
      "source": "NIST2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0481_NIST2024",
      "name": "Context-Based Impact Measures",
      "description": "Determine context-based measures to identify if new impacts are present due to the GAI system, including regular engagements with downstream AI Actors to identify and quantify new contexts of unanticipated impacts of GAI systems.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0482_NIST2024",
      "name": "Input Actor Engagement",
      "description": "Plan regular engagements with AI Actors responsible for inputs to GAI systems, including third-party data and algorithms, to review and evaluate unanticipated impacts.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0484_NIST2024",
      "name": "Provenance Analysis Tools",
      "description": "Integrate tools designed to analyze content provenance and detect data anomalies, verify the authenticity of digital signatures, and identify patterns associated with misinformation or manipulation.",
      "source": "NIST2024",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0485_NIST2024",
      "name": "Demographic Metric Disaggregation",
      "description": "Disaggregate evaluation metrics by demographic factors to identify any discrepancies in how content provenance mechanisms work across diverse populations.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0486_NIST2024",
      "name": "Feedback Exercise Metrics",
      "description": "Develop a suite of metrics to evaluate structured public feedback exercises informed by representative AI Actors.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0487_NIST2024",
      "name": "Novel Risk Measurement Methods",
      "description": "Evaluate novel methods and technologies for the measurement of GAI-related risks including in content provenance, offensive cyber, and CBRN, while maintaining the models' ability to produce valid, reliable, and factually accurate outputs.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0488_NIST2024",
      "name": "Equitable Output Monitoring",
      "description": "Implement continuous monitoring of GAI system impacts to identify whether GAI outputs are equitable across various sub-populations. Seek active and direct feedback from affected communities via structured feedback mechanisms or red-teaming to monitor and improve outputs.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0489_NIST2024",
      "name": "Data Quality Evaluation",
      "description": "Evaluate the quality and integrity of data used in training and the provenance of AI-generated content, for example by employing techniques like chaos engineering and seeking stakeholder feedback.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0490_NIST2024",
      "name": "Feedback Exercise Use Cases",
      "description": "Define use cases, contexts of use, capabilities, and negative impacts where structured human feedback exercises, e.g., GAI red-teaming, would be most beneficial for GAI risk measurement and management based on the context of use.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0491_NIST2024",
      "name": "Unmeasurable Risk Tracking",
      "description": "Track and document risks or opportunities related to all GAI risks that cannot be measured quantitatively, including explanations as to why some risks cannot be measured (e.g., due to technological limitations, resource constraints, or trustworthy considerations). Include unmeasured risks in marginal risks.",
      "source": "NIST2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0492_NIST2024",
      "name": "Interest Group Definition",
      "description": "Define relevant groups of interest (e.g., demographic groups, subject matter experts, experience with GAI technology) within the context of use as part of plans for gathering structured public feedback.",
      "source": "NIST2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0493_NIST2024",
      "name": "External Evaluation Engagement",
      "description": "Engage in internal and external evaluations, GAI red-teaming, impact assessments, or other structured human feedback exercises in consultation with representative AI Actors with expertise and familiarity in the context of use, and/or who are representative of the populations associated with the context of use.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0494_NIST2024",
      "name": "Feedback Independence Verification",
      "description": "Verify those conducting structured human feedback exercises are not directly involved in system development tasks for the same GAI model.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0495_NIST2024",
      "name": "Statistical Bias Management",
      "description": "Assess and manage statistical biases related to GAI content provenance through techniques such as re-sampling, re-weighting, or adversarial training.",
      "source": "NIST2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0496_NIST2024",
      "name": "Provenance Privacy Documentation",
      "description": "Document how content provenance data is tracked and how that data interacts with privacy and security. Consider: Anonymizing data to protect the privacy of human subjects; Leveraging privacy output filters; Removing any personally identifiable information (PII) to prevent potential harm or misuse.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0497_NIST2024",
      "name": "Participation Withdrawal Options",
      "description": "Provide human subjects with options to withdraw participation or revoke their consent for present or future use of their data in GAI applications.",
      "source": "NIST2024",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0498_NIST2024",
      "name": "Privacy-Enhancing Technologies",
      "description": "Use techniques such as anonymization, differential privacy or other privacy-enhancing technologies to minimize the risks associated with linking AI-generated content back to individual human subjects.",
      "source": "NIST2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0499_NIST2024",
      "name": "Baseline Model Selection",
      "description": "Consider baseline model performance on suites of benchmarks when selecting a model for fine tuning or enhancement with retrieval-augmented generation.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0501_NIST2024",
      "name": "Pre-Deployment Results Sharing",
      "description": "Share results of pre-deployment testing with relevant GAI Actors, such as those with system release approval authority",
      "source": "NIST2024",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0502_NIST2024",
      "name": "Purpose-Built Testing Environment",
      "description": "Utilize a purpose-built testing environment such as NIST Dioptra to empirically evaluate GAI trustworthy characteristics.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0503_NIST2024",
      "name": "Performance Extrapolation Avoidance",
      "description": "Avoid extrapolating GAI system performance or capabilities from narrow, non-systematic, and anecdotal assessments.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0504_NIST2024",
      "name": "Human Domain Knowledge Documentation",
      "description": "Document the extent to which human domain knowledge is employed to improve GAI system performance, via, e.g., RLHF, fine-tuning, retrieval-augmented generation, content moderation, business rules.",
      "source": "NIST2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0505_NIST2024",
      "name": "Source and Citation Review",
      "description": "Review and verify sources and citations in GAI system outputs during pre-deployment risk measurement and ongoing monitoring activities",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0506_NIST2024",
      "name": "Anthropomorphization Tracking",
      "description": "Track and document instances of anthropomorphization (e.g., human images, mentions of human feelings, cyborg imagery or motifs) in GAI system interfaces.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0507_NIST2024",
      "name": "Data Provenance Verification",
      "description": "Verify GAI system training data and TEVV data provenance, and that fine-tuning or retrieval-augmented generation data is grounded",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0508_NIST2024",
      "name": "Security Guardrail Review",
      "description": "Regularly review security and safety guardrails, especially if the GAI system is being operated in novel circumstances. This includes reviewing reasons why the GAI system was initially assessed as being safe to deploy.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0509_NIST2024",
      "name": "Value Chain Wellbeing Assessment",
      "description": "Assess adverse impacts, including health and wellbeing impacts for value chain or other AI Actors that are exposed to sexually explicit, offensive, or violent information during GAI training and maintenance.",
      "source": "NIST2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0510_NIST2024",
      "name": "Training Data Harm Assessment",
      "description": "Assess existence or levels of harmful bias, intellectual property infringement, data privacy violations, obscenity, extremism, violence, or CBRN information in system training data.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0511_NIST2024",
      "name": "Fine-Tuned Safety Re-evaluation",
      "description": "Re-evaluate safety features of fine-tuned models when the negative risk exceeds organizational risk tolerance.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0512_NIST2024",
      "name": "Generated Code Review",
      "description": "Review GAI system outputs for validity and safety: Review generated code to assess risks that may arise from unreliable downstream decision-making.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0513_NIST2024",
      "name": "Architecture Monitoring Verification",
      "description": "Verify that GAI system architecture can monitor outputs and performance, and handle, recover from, and repair errors when security anomalies, threats and impacts are detected.",
      "source": "NIST2024",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0514_NIST2024",
      "name": "Inappropriate Query Handling",
      "description": "Verify that systems properly handle queries that may give rise to inappropriate, malicious, or illegal usage, including facilitating manipulation, extortion, targeted impersonation, cyber-attacks, and weapons creation.",
      "source": "NIST2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0515_NIST2024",
      "name": "Safety Circumvention Evaluation",
      "description": "Regularly evaluate GAI system vulnerabilities to possible circumvention of safety measures.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0516_NIST2024",
      "name": "Security Vulnerability Assessment",
      "description": "Apply established security measures to: Assess likelihood and magnitude of vulnerabilities and threats such as backdoors, compromised dependencies, data breaches, eavesdropping, man-in-the-middle attacks, reverse engineering, autonomous agents, model theft or exposure of model weights, AI inference, bypass, extraction, and other baseline security concerns.",
      "source": "NIST2024",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0517_NIST2024",
      "name": "Security Benchmarking",
      "description": "Benchmark GAI system security and resilience related to content provenance against industry standards and best practices. Compare GAI system security features and content provenance methods against industry state-of-the-art",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0518_NIST2024",
      "name": "User Satisfaction Survey",
      "description": "Conduct user surveys to gather user satisfaction with the AI-generated content and user perceptions of content authenticity. Analyze user feedback to identify concerns and/or current literacy levels related to content provenance and understanding of labels on content.",
      "source": "NIST2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0519_NIST2024",
      "name": "Security Effectiveness Metrics",
      "description": "Identify metrics that reflect the effectiveness of security measures, such as data provenance, the number of unauthorized access attempts, inference, bypass, extraction, penetrations, or provenance verification.",
      "source": "NIST2024",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0520_NIST2024",
      "name": "Authentication Method Reliability",
      "description": "Measure reliability of content authentication methods, such as watermarking, cryptographic signatures, digital fingerprints, as well as access controls, conformity assessment, and model integrity verification, which can help support the effective implementation of content provenance techniques. Evaluate the rate of false positives and false negatives in content provenance, as well as true positives and true negatives for verification.",
      "source": "NIST2024",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0521_NIST2024",
      "name": "Security Implementation Rate",
      "description": "Measure the rate at which recommendations from security checks and incidents are implemented. Assess how quickly the AI system can adapt and improve based on lessons learned from security incidents and feedback",
      "source": "NIST2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0522_NIST2024",
      "name": "AI Red-Teaming Resilience",
      "description": "Perform AI red-teaming to assess resilience against: Abuse to facilitate attacks on other systems (e.g., malicious code generation, enhanced phishing content), GAI attacks (e.g., prompt injection), ML attacks (e.g., adversarial examples/prompts, data poisoning, membership inference, model extraction, sponge examples).",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0524_NIST2024",
      "name": "Security Measure Effectiveness",
      "description": "Regularly assess and verify that security measures remain effective and have not been compromised.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0525_NIST2024",
      "name": "Policy Violation Statistics",
      "description": "Compile statistics on actual policy violations, take-down requests, and intellectual property infringement for organizational GAI systems: Analyze transparency reports across demographic groups, languages groups.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0527_NIST2024",
      "name": "Digital Content Transparency",
      "description": "Use digital content transparency solutions to enable the documentation of each instance where content is generated, modified, or shared to provide a tamper-proof history of the content, promote transparency, and enable traceability. Robust version control systems can also be applied to track changes across the AI lifecycle over time.",
      "source": "NIST2024",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0528_NIST2024",
      "name": "User Instruction Adequacy",
      "description": "Verify adequacy of GAI system user instructions through user testing.",
      "source": "NIST2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0529_NIST2024",
      "name": "ML Explanation Application",
      "description": "Apply and document ML explanation results such as: Analysis of embeddings, Counterfactual prompts, Gradient-based attributions, Model compression/surrogate models, Occlusion/term reduction.",
      "source": "NIST2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0530_NIST2024",
      "name": "GAI Model Documentation",
      "description": "Document GAI model details including: Proposed use and organizational value; Assumptions and limitations, Data collection methodologies; Data provenance; Data quality; Model architecture (e.g., convolutional neural network, transformers, etc.); Optimization objectives; Training algorithms; RLHF approaches; Fine-tuning or retrieval-augmented generation approaches; Evaluation data; Ethical considerations; Legal and regulatory requirements.",
      "source": "NIST2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0531_NIST2024",
      "name": "Data Exposure Red-Teaming",
      "description": "Conduct AI red-teaming to assess issues such as: Outputting of training data samples, and subsequent reverse engineering, model extraction, and membership inference risks; Revealing biometric, confidential, copyrighted, licensed, patented, personal, proprietary, sensitive, or trade-marked information; Tracking or revealing location information of users or members of training datasets.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0532_NIST2024",
      "name": "End-User Expectation Engagement",
      "description": "Engage directly with end-users and other stakeholders to understand their expectations and concerns regarding content provenance. Use this feedback to guide the design of provenance data-tracking techniques.",
      "source": "NIST2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0534_NIST2024",
      "name": "Bias Benchmark Application",
      "description": "Apply use-case appropriate benchmarks (e.g., Bias Benchmark Questions, Real Hateful or Harmful Prompts, Winogender Schemas15) to quantify systemic bias, stereotyping, denigration, and hateful content in GAI system outputs; Document assumptions and limitations of benchmarks, including any actual or possible training/test data cross contamination, relative to in-context deployment environment.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0624_Barrett2024",
      "name": "Risk Ownership by Capability",
      "description": "Take responsibility for risk assessment and risk management tasks for which your organization has access to information, capability, or opportunity to develop capability sufficient for constructive action, or that is substantially greater than others in the value chain",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0626_Barrett2024",
      "name": "Anticipate Misuse & Impact",
      "description": "Identify reasonably foreseeable uses, and misuses or abuses for a GPAIS (e.g, auto- mated generation of toxic or illegal content or disinformation, or aiding with proliferation of cyber, chemical, biological, or radiological weapons), and identify reasonably foreseeable potential impacts (e.g., to fundamental rights)",
      "source": "Barrett2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0627_Barrett2024",
      "name": "Assess Catastrophic Risk",
      "description": "Identify whether a GPAIS could lead to significant, severe, or catastrophic impacts, e.g., because of correlated failures or errors across high-stakes deployment domains, dan- gerous emergent behaviors or vulnerabilities, or harmful misuses and abuses",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0628_Barrett2024",
      "name": "Red-Team for Dangerous Behaviors",
      "description": "Use red teams and adversarial testing as part of extensive interaction with GPAIS to identify dangerous capabilities, vulnerabilities, or other emergent properties of such systems",
      "source": "Barrett2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0629_Barrett2024",
      "name": "Risk Tracking",
      "description": "Track important identified risks (e.g., vulnerabilities from data poisoning and other attacks or objectives mis-specification) even if they cannot yet be measured",
      "source": "Barrett2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0631_Barrett2024",
      "name": "Disclose Risks to Stakeholders",
      "description": "Incorporate identified AI system risk factors, and circumstances that could result in impacts or harms, into reporting and engagement with internal and external stake- holders (e.g., to downstream developers, regulators, users, impacted communities, etc.) on the AI system as appropriate, e.g., using model cards, system cards, and other transparency mechanisms",
      "source": "Barrett2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0632_Barrett2024",
      "name": "Document Mitigation Rationale",
      "description": "Document the process used in considering risk mitigation controls, the options considered, and reasons for choices.",
      "source": "Barrett2024",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0633_Barrett2024",
      "name": "Ensure Legal Compliance",
      "description": "Legal and regulatory requirements involving AI are understood, managed, and documented.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0635_Barrett2024",
      "name": "Align Risk Management to Tolerance",
      "description": "Processes, procedures, and practices are in place to determine the needed level of risk management activities based on the organization\u2019s risk tolerance.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0636_Barrett2024",
      "name": "Transparent RM Policies",
      "description": "The risk management process and its outcomes are established through transparent policies, procedures, and other controls based on organizational risk priorities.",
      "source": "Barrett2024",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0637_Barrett2024",
      "name": "Monitoring of RM policies",
      "description": "Ongoing monitoring and periodic review of the risk management process and its outcomes are planned and organizational roles and responsibilities clearly defined, including determining the frequency of periodic review.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0638_Barrett2024",
      "name": "Inventory AI Systems",
      "description": "Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0639_Barrett2024",
      "name": "Decommission AI Systems Safely",
      "description": "Processes and procedures are in place for decommissioning and phasing out AI systems safely and in a manner that does not increase risks or decrease the organization\u2019s trustworthiness.",
      "source": "Barrett2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0640_Barrett2024",
      "name": "Define AI Risk Roles",
      "description": "Roles and responsibilities and lines of communication related to mapping, measuring, and managing AI risks are documented and are clear to individuals and teams throughout the organization.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0641_Barrett2024",
      "name": "Train on AI Risk",
      "description": "The organization\u2019s personnel and partners receive AI risk management training to enable them to perform their duties and responsibilities consistent with related policies, procedures, and agreements",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0642_Barrett2024",
      "name": "Executive Risk Accountability",
      "description": "Executive leadership of the organization takes responsibility for decisions about risks associated with AI sys- tem development and deployment.",
      "source": "Barrett2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0643_Barrett2024",
      "name": "Diverse Risk Voices",
      "description": "Decision-making related to mapping, measuring, and managing AI risks throughout the lifecycle is informed by a diverse team (e.g., diversity of demographics, disciplines, experience, expertise, and backgrounds).",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0644_Barrett2024",
      "name": "Human-AI Oversight Roles",
      "description": "Policies and procedures are in place to define and differentiate roles and responsibilities for human-AI configurations and oversight of AI systems.",
      "source": "Barrett2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0646_Barrett2024",
      "name": "Document & Share AI Risk Impacts",
      "description": "Organizational teams document the risks and potential impacts of the AI technology they design, develop, deploy, evaluate, and use, and they communicate about the impacts more broadly.",
      "source": "Barrett2024",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0647_Barrett2024",
      "name": "Detect & Share Incidents",
      "description": "Organizational practices are in place to enable AI testing, identification of incidents, and information sharing.",
      "source": "Barrett2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0648_Barrett2024",
      "name": "External Feedback on Impacts",
      "description": "Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those external to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI risks.",
      "source": "Barrett2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0649_Barrett2024",
      "name": "External Feedback on System Design",
      "description": "Mechanisms are established to enable the team that developed or deployed AI systems to regularly incorporate adjudicated feedback from relevant AI actors into system design and implementation.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0650_Barrett2024",
      "name": "Third-Party Risk",
      "description": "Policies and procedures are in place that address AI risks associated with third-party entities, including risks of infringement of a third-party\u2019s intellectual property or other rights.",
      "source": "Barrett2024",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0651_Barrett2024",
      "name": "Contingency Planning for Third-Party Failures",
      "description": "Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be high-risk.",
      "source": "Barrett2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0652_Barrett2024",
      "name": "Document Intended Use & Context",
      "description": "Intended purposes, potentially beneficial uses, context-specific laws, norms and expectations, and prospective settings in which the AI system will be deployed are understood and documented.",
      "source": "Barrett2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0655_Barrett2024",
      "name": "Business Use",
      "description": "The business value or context of business use has been clearly defined or \u2013 in the case of assessing existing AI systems \u2013 re-evaluated.",
      "source": "Barrett2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0657_Barrett2024",
      "name": "Elicit Requirements & Assess Impacts",
      "description": "System requirements (e.g., \u201cthe system shall respect the privacy of its users\u201d) are elicited from and understood by relevant AI actors. Design decisions take socio-technical implications into account to address AI risks",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0658_Barrett2024",
      "name": "Define Tasks & Methods",
      "description": "The specific tasks and methods used to implement the tasks that the AI system will support are defined (e.g., classifiers, generative models, recommenders).",
      "source": "Barrett2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0659_Barrett2024",
      "name": "Document Limits & Oversight",
      "description": "Information about the AI system\u2019s knowledge limits and how system output may be utilized and overseen by humans is documented. Documen- tation provides sufficient information to assist relevant AI actors when making decisions and tak- ing subsequent actions.",
      "source": "Barrett2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0660_Barrett2024",
      "name": "Document TEVV & Integrity",
      "description": "Scientific integrity and TEVV considerations are identified and docu- mented, including those related to experimental design, data collection and selection (e.g., availability, represen- tativeness, suitability), system trustworthiness, and construct validation",
      "source": "Barrett2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0661_Barrett2024",
      "name": "Document Intended Benefits",
      "description": "Potential benefits of intend- ed AI system functionality and performance are ex- amined and documented",
      "source": "Barrett2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0662_Barrett2024",
      "name": "Document AI Error Costs",
      "description": "Potential costs, in- cluding non-monetary costs, which result from expected or realized AI errors or system func- tionality and trustwor- thiness \u2013 as connected to organizational risk tolerance \u2013 are exam- ined and documented.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0663_Barrett2024",
      "name": "Document Application Scope",
      "description": "Targeted application scope is specified and documented based on the system\u2019s capability, established context, and AI system categorization.",
      "source": "Barrett2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0664_Barrett2024",
      "name": "Document Operator Proficiency",
      "description": "Processes for operator and practitioner proficiency with AI system performance and trustworthiness \u2013 and relevant technical standards and certifications \u2013 are defined, assessed, and documented.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0665_Barrett2024",
      "name": "Define Human Oversight Protocols",
      "description": "Processes for human oversight are defined, assessed, and documented in accordance with organizational policies from the Govern function",
      "source": "Barrett2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0666_Barrett2024",
      "name": "Document AI RM Third Party Framework",
      "description": "Approaches for mapping AI technology and legal risks of its components \u2013 including the use of third-party data or software \u2013 are in place, followed, and docu- mented, as are risks of infringement of a third party\u2019s intellectual prop- erty or other rights.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0667_Barrett2024",
      "name": "Document Internal & Third-Party Controls",
      "description": "Internal risk controls for components of the AI system, including third- party AI technologies, are identified and documented.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0668_Barrett2024",
      "name": "Assess & Document Risk Magnitude",
      "description": "Likelihood and magnitude of each identified impact (both potentially beneficial and harmful) based on expected use, past uses of AI systems in similar contexts, public incident reports, feedback from those external to the team that developed or deployed the AI system, or other data are identified and documented.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0669_Barrett2024",
      "name": "Establish Feedback Integration Processes",
      "description": "Practices and personnel for supporting regular engagement with relevant AI actors and integrating feedback about positive, negative, and unanticipated impacts are in place and documented.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0670_Barrett2024",
      "name": "Select & Document Risk Metrics",
      "description": "Approaches and metrics for measurement of AI risks enumerated during the Map function are selected for implementation starting with the most significant AI risks. The risks or trustworthiness characteristics that will not \u2013 or cannot \u2013 be measured are properly documented.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0671_Barrett2024",
      "name": "Evaluate Risk Metrics & Controls",
      "description": "Appropriateness of AI metrics and effectiveness of existing controls are regularly assessed and updated, including reports of errors and potential impacts on affected communities.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0672_Barrett2024",
      "name": "Involve Independent & External Assessors",
      "description": "Internal experts who did not serve as front-line developers for the system and/ or independent assessors are involved in regular assessments and updates. Domain experts, users, AI actors external to the team that developed or deployed the AI system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance.",
      "source": "Barrett2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0674_Barrett2024",
      "name": "Ensure Ethical Human Evaluation",
      "description": "Evaluations involving human subjects meet applicable requirements (including human subject protection) and are representative of the relevant population.",
      "source": "Barrett2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0675_Barrett2024",
      "name": "Measure & Document Deployment Performance",
      "description": "AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment setting(s). Measures are documented",
      "source": "Barrett2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0676_Barrett2024",
      "name": "Monitor System in Production",
      "description": "The functionality and behavior of the AI system and its components \u2013 as identified in the Map function \u2013 are monitored when in production.",
      "source": "Barrett2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0677_Barrett2024",
      "name": "Demonstrate Validity & Generalization Limits",
      "description": "The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the conditions under which the technology was developed are documented.",
      "source": "Barrett2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0678_Barrett2024",
      "name": "Evaluate Safety & Failure Modes",
      "description": "The AI system is evaluated regularly for safety risks \u2013 as identified in the Map function. The AI system to be deployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, particularly if made to operate beyond its knowledge limits. Safety metrics reflect system reliability and robustness, real- time monitoring, and response times for AI system failures.",
      "source": "Barrett2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0679_Barrett2024",
      "name": "Evaluate Security & Resilience",
      "description": "AI system security and resilience \u2013 as identified in the Map function \u2013 are evaluated and documented.",
      "source": "Barrett2024",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0680_Barrett2024",
      "name": "Explain & Document Model Behavior",
      "description": "The AI model is explained, validated, and documented, and AI system output is interpreted within its context \u2013 as identified in the Map function \u2013 to inform responsible use and governance.",
      "source": "Barrett2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0681_Barrett2024",
      "name": "Examine & Document Privacy Risk",
      "description": "Privacy risk of the AI system \u2013 as identified in the Map function \u2013 is examined and documented.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0683_Barrett2024",
      "name": "Assess Environmental Impact",
      "description": "Environmental impact and sustainability of AI model training and management activities \u2013 as identified in the Map function \u2013 are assessed and documented.",
      "source": "Barrett2024",
      "subcategoryId": "environmental-1.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0684_Barrett2024",
      "name": "Evaluate TEVV Effectiveness",
      "description": "Effectiveness of the employed TEVV metrics and processes in the Measure function are evaluated and documented.",
      "source": "Barrett2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0685_Barrett2024",
      "name": "Track Emergent AI Risks",
      "description": "Approaches, personnel, and documentation are in place to regularly identify and track existing, unanticipated, and emergent AI risks based on factors such as intended and actual performance in deployed contexts.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0686_Barrett2024",
      "name": "Consider Alternatives for Hard-to-Measure Risks",
      "description": "Risk tracking approaches are considered for settings where AI risks are difficult to assess using currently available measurement techniques or where metrics are not yet available.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0687_Barrett2024",
      "name": "Enable User Feedback & Appeals",
      "description": "Feedback processes for end users and impacted communities to report problems and appeal system outcomes are established and inte- grated into AI system evaluation metrics.",
      "source": "Barrett2024",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0688_Barrett2024",
      "name": "Align Risk Metrics with Context",
      "description": "Measurement approach- es for identifying AI risks are connected to deployment context(s) and informed through consultation with do- main experts and other end users. Approaches are documented.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0689_Barrett2024",
      "name": "Validate Trustworthiness with Expert Input",
      "description": "Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are informed by input from domain experts and relevant AI actors to validate whether the system is performing consistently as intended. Results are documented.",
      "source": "Barrett2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0690_Barrett2024",
      "name": "Document Performance Shifts from Field Data",
      "description": "Measurable performance improvements or declines based on consultations with relevant AI actors, including affected communities, and field data about context-relevant risks and trustworthiness characteristics are identified and document",
      "source": "Barrett2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0691_Barrett2024",
      "name": "Determine Deployment Readiness",
      "description": "A determination is made as to whether the AI system achieves its intended purposes and stated objectives and whether its development or deployment should proceed.",
      "source": "Barrett2024",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0692_Barrett2024",
      "name": "Prioritize Risk Treatment",
      "description": "Treatment of documented AI risks is prioritized based on impact, likelihood, and available resources or methods.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0693_Barrett2024",
      "name": "Plan & Document Risk Responses",
      "description": "Responses to the AI risks deemed high priority, as identified by the Map function, are developed, planned, and documented. Risk response options can include mitigating, transferring, avoiding, or accepting.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0694_Barrett2024",
      "name": "Document Residual Risks",
      "description": "Negative residual risks (defined as the sum of all unmitigated risks) to both downstream acquirers of AI systems and end users are documented.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0695_Barrett2024",
      "name": "Consider Resources & Alternatives",
      "description": "Resources required to manage AI risks are taken into account \u2013 along with viable non-AI alternative systems, approaches, or methods \u2013 to reduce the magnitude or likelihood of potential impacts.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0697_Barrett2024",
      "name": "Respond to Unknown Risks",
      "description": "Procedures are followed to respond to and recover from a previously unknown risk when it is identified.",
      "source": "Barrett2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0698_Barrett2024",
      "name": "Deactivate Misaligned Systems",
      "description": "Mechanisms are in place and applied, and responsibilities are assigned and understood, to supersede, disengage, or deactivate AI systems that demonstrate performance or outcomes inconsistent with intended use.",
      "source": "Barrett2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0699_Barrett2024",
      "name": "Monitor Third-Party Risks",
      "description": "AI risks and benefits from third-party resources are regularly monitored, and risk controls are applied and documented.",
      "source": "Barrett2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0700_Barrett2024",
      "name": "Monitor Pre-trained Models",
      "description": "Pre-trained models which are used for development are monitored as part of AI system regular monitoring and maintenance.",
      "source": "Barrett2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0701_Barrett2024",
      "name": "Implement Post-deployment Monitoring Plan",
      "description": "Post-deployment AI system monitoring plans are implemented, including mechanisms for capturing and evaluating input from users and other relevant AI actors, appeal and override, decommissioning, incident response, recovery, and change management.",
      "source": "Barrett2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0702_Barrett2024",
      "name": "Integrate Continuous Improvement",
      "description": "Measurable activities for continual improvements are integrated into AI system updates and include regular engagement with interested parties, including relevant AI actors.",
      "source": "Barrett2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0703_Barrett2024",
      "name": "Report & Respond to Incidents",
      "description": "Incidents and errors are communicated to relevant AI actors, including affected communities. Processes for tracking, responding to, and recovering from incidents and errors are followed and documented.",
      "source": "Barrett2024",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0707_NIST2024",
      "name": "Harmful Bias and Homogenization Assessment",
      "description": "Conduct fairness assessments to measure systemic bias. Measure GAI system performance across demographic groups and subgroups, addressing both quality of service and any allocation of services and resources. Quantify harms using: field testing with sub-group populations to determine likelihood of exposure to generated content exhibiting harmful bias, AI red-teaming with counterfactual and low-context (e.g., \u201cleader,\u201d \u201cbad guys\u201d) prompts. For ML pipelines or business processes with categorical or numeric outcomes that rely on GAI, apply general fairness metrics (e.g., demographic parity, equalized odds, equal opportunity, statistical hypothesis tests), to the pipeline or business outcome where appropriate; Custom, context-specific metrics developed in collaboration with domain experts and affected communities; Measurements of the prevalence of denigration in generated content in deployment (e.g., subsampling a fraction of traffic and manually annotating denigrating content)",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0708_NIST2024",
      "name": "Assessments for Environmental; Harmful Bias and Homogenization",
      "description": "Identify the classes of individuals, groups, or environmental ecosystems which might be impacted by GAI systems through direct engagement with potentially impacted communities.",
      "source": "NIST2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0709_NIST2024",
      "name": "Harmful Bias and Homogenization Data Review",
      "description": "Review, document, and measure sources of bias in GAI training and TEVV data: Differences in distributions of outcomes across and within groups, including intersecting groups; Completeness, representativeness, and balance of data sources; demographic group and subgroup coverage in GAI system training data; Forms of latent systemic bias in images, text, audio, embeddings, or other complex or unstructured data; Input data features that may serve as proxies for demographic group membership (i.e., image metadata, language dialect) or otherwise give rise to emergent bias within GAI systems; The extent to which the digital divide may negatively impact representativeness in GAI system training and TEVV data; Filtering of hate speech or content in GAI system training data; Prevalence of GAI-generated data in GAI system training data",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0710_NIST2024",
      "name": "Harmful Bias and Homogenization Assessment",
      "description": "Assess the proportion of synthetic to non-synthetic training data and verify training data is not overly homogenous or GAI-produced to mitigate concerns of model collapse.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0712_NIST2024",
      "name": "Environmental Degradation Minimisation",
      "description": "Document anticipated environmental impacts of model development, maintenance, and deployment in product design decisions.",
      "source": "NIST2024",
      "subcategoryId": "environmental-1.6",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0713_NIST2024",
      "name": "Environmental Impact Assessment",
      "description": "Measure or estimate environmental impacts (e.g., energy and water consumption) for training, fine tuning, and deploying models: Verify tradeoffs between resources used at inference time versus additional resources required at training time.",
      "source": "NIST2024",
      "subcategoryId": "environmental-1.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0714_NIST2024",
      "name": "Effectiveness of Carbon Capture",
      "description": "Verify effectiveness of carbon capture or offset programs for GAI training and applications, and address green-washing concerns.",
      "source": "NIST2024",
      "subcategoryId": "environmental-1.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0715_NIST2024",
      "name": "Confabulation; Information Integrity; Harmful Bias and Homogenization Measurement Error Models",
      "description": "Create measurement error models for pre-deployment metrics to demonstrate construct validity for each metric (i.e., does the metric effectively operationalize the desired concept): Measure or estimate, and document, biases or statistical variance in applied metrics or structured human feedback processes; Leverage domain expertise when modeling complex societal constructs such as hateful content.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0716_NIST2024",
      "name": "Harmful Bias and Homogenization Assessments",
      "description": "Conduct impact assessments on how AI-generated content might affect different social, economic, and cultural groups.",
      "source": "NIST2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0717_NIST2024",
      "name": "Human-AI Configuration; Information Integrity Studies",
      "description": "Conduct studies to understand how end users perceive and interact with GAI content and accompanying content provenance within context of use. Assess whether the content aligns with their expectations and how they may act upon the information presented.",
      "source": "NIST2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0718_NIST2024",
      "name": "Harmful Bias and Homogenization Evaluation",
      "description": "Evaluate potential biases and stereotypes that could emerge from the AIgenerated content using appropriate methodologies including computational testing methods as well as evaluating structured feedback input (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV)",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0719_NIST2024",
      "name": "Human-AI Configuration; Information Integrity; Harmful Bias and Homogenization",
      "description": "Provide input for training materials about the capabilities and limitations of GAI systems related to digital content transparency for AI Actors, other professionals, and the public about the societal impacts of AI and the role of diverse and inclusive content generation (AI Deployment, Affected Individuals and Communities, End-Users, Operation and Monitoring, TEVV)",
      "source": "NIST2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0720_NIST2024",
      "name": "Human-AI Configuration; Information Integrity; Harmful Bias and Homogenization",
      "description": "Record and integrate structured feedback about content provenance from operators, users, and potentially impacted communities through the use of methods such as user research studies, focus groups, or community forums. Actively seek feedback on generated content quality and potential biases. Assess the general awareness among end users and impacted communities about the availability of these feedback channels.",
      "source": "NIST2024",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0721_NIST2024",
      "name": "Information Integrity; Information Security",
      "description": "Conduct adversarial testing at a regular cadence to map and measure GAI risks, including tests to address attempts to deceive or manipulate the application of provenance techniques or other misuses. Identify vulnerabilities and understand potential misuse scenarios and unintended outputs (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV)",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0722_NIST2024",
      "name": "Human-AI Configuration; Confabulation; Information Security",
      "description": "Evaluate GAI system performance in real-world scenarios to observe its behavior in practical environments and reveal issues that might not surface in controlled and optimized testing environments.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0723_NIST2024",
      "name": "Information Integrity; Harmful Bias and Homogenization",
      "description": "Implement interpretability and explainability methods to evaluate GAI system decisions and verify alignment with intended purpose. (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV)",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0724_NIST2024",
      "name": "Information Integrity",
      "description": "Monitor and document instances where human operators or other systems override the GAI's decisions. Evaluate these cases to understand if the overrides are linked to issues related to content provenance. (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV).",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0725_NIST2024",
      "name": "Human-AI Configuration; Information Security",
      "description": "Verify and document the incorporation of results of structured public feedback exercises into design, implementation, deployment approval (\u201cgo\u201d/\u201cno-go\u201d decisions), monitoring, and decommission decisions. (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV)",
      "source": "NIST2024",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0726_NIST2024",
      "name": "Information Security",
      "description": "Document trade-offs, decision processes, and relevant measurement and feedback results for risks that do not surpass organizational risk tolerance, for example, in the context of model release: Consider different approaches for model release, for example, leveraging a staged release approach. Consider release approaches in the context of the model and its projected use cases. Mitigate, transfer, or avoid risks that surpass organizational risk tolerances.",
      "source": "NIST2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0727_NIST2024",
      "name": "Human-AI Configuration",
      "description": "Monitor the robustness and effectiveness of risk controls and mitigation plans (e.g., via red-teaming, field testing, participatory engagements, performance assessments, user feedback mechanisms). (AI Development, AI Deployment, AI Impact Assessment, Operation and Monitoring)",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0728_NIST2024",
      "name": "CBRN Information or Capabilities; Obscene, Degrading, and/or Abusive Content; Harmful Bias and Homogenization; Dangerous, Violent, or Hateful Content",
      "description": "Compare GAI system outputs against pre-defined organization risk tolerance, guidelines, and principles, and review and test AI-generated content against these guidelines. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0729_NIST2024",
      "name": "Information Integrity",
      "description": "Document training data sources to trace the origin and provenance of AI generated content (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",
      "source": "NIST2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0730_NIST2024",
      "name": "Information Integrity",
      "description": "Evaluate feedback loops between GAI system content provenance and human reviewers, and update where needed. Implement real-time monitoring systems to affirm that content provenance protocols remain effective. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring)",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0731_NIST2024",
      "name": "Information Security; Harmful Bias and Homogenization",
      "description": "Evaluate GAI content and data for representational biases and employ techniques such as re-sampling, re-ranking, or adversarial training to mitigate biases in the generated content. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",
      "source": "NIST2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0732_NIST2024",
      "name": "CBRN Information or Capabilities; Obscene, Degrading, and/or Abusive Content; Harmful Bias and Homogenization; Dangerous, Violent, or Hateful Content",
      "description": "Engage in due diligence to analyze GAI output for harmful content, potential misinformation, and CBRN-related or NCII content. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0733_NIST2024",
      "name": "Human-AI Configuration",
      "description": "Use feedback from internal and external AI Actors, users, individuals, and communities, to assess impact of AI-generated content. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",
      "source": "NIST2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0734_NIST2024",
      "name": "Information Integrity",
      "description": "Use real-time auditing tools where they can be demonstrated to aid in the tracking and validation of the lineage and authenticity of AI-generated data. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0735_NIST2024",
      "name": "Human-AI Configuration; Harmful Bias and Homogenization",
      "description": "Use structured feedback mechanisms to solicit and capture user input about AI generated content to detect subtle shifts in quality or alignment with community and societal values. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",
      "source": "NIST2024",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0736_NIST2024",
      "name": "Data Privacy; Intellectual Property; Information Integrity; Confabulation; Harmful Bias and Homogenization",
      "description": "Consider opportunities to responsibly use synthetic data and other privacy enhancing techniques in GAI development, where appropriate and applicable, match the statistical properties of real-world data without disclosing personally identifiable information or contributing to homogenization.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0737_NIST2024",
      "name": "Value Chain and Component Integration",
      "description": "Develop and update GAI system incident response and recovery plans and procedures to address the following: Review and maintenance of policies and procedures to account for newly encountered uses; Review and maintenance of policies and procedures for detection of unanticipated uses; Verify response and recovery plans account for the GAI system value chain; Verify response and recovery plans are updated for and include necessary details to communicate with downstream GAI system Actors: Points-of-Contact (POC), Contact information, notification format. (AI Deployment, Operation and Monitoring)",
      "source": "NIST2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0738_NIST2024",
      "name": "Human-AI Configuration",
      "description": "Establish and maintain communication plans to inform AI stakeholders as part of the deactivation or disengagement process of a specific GAI system (including for open-source models) or context of use, including reasons, workarounds, user access removal, alternative processes, contact information, etc.",
      "source": "NIST2024",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0739_NIST2024",
      "name": "Information Security",
      "description": "Establish and maintain procedures for escalating GAI system incidents to the organizational risk management authority when specific criteria for deactivation or disengagement is met for a particular context of use or for the GAI system as a whole.",
      "source": "NIST2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0740_NIST2024",
      "name": "Information Security",
      "description": "Establish and maintain procedures for the remediation of issues which trigger incident response processes for the use of a GAI system, and provide stakeholders timelines associated with the remediation plan",
      "source": "NIST2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0741_NIST2024",
      "name": "Information Security",
      "description": "Establish and regularly review specific criteria that warrants the deactivation of GAI systems in accordance with set risk tolerances and appetites.",
      "source": "NIST2024",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0742_NIST2024",
      "name": "Value Chain and Component Integration; Intellectual Property",
      "description": "Apply organizational risk tolerances and controls (e.g., acquisition and procurement processes; assessing personnel credentials and qualifications, performing background checks; filtering GAI input and outputs, grounding, fine tuning, retrieval-augmented generation) to third-party GAI resources: Apply organizational risk tolerance to the utilization of third-party datasets and other GAI resources; Apply organizational risk tolerances to fine-tuned third-party models; Apply organizational risk tolerance to existing third-party models adapted to a new domain; Reassess risk measurements after fine-tuning thirdparty GAI models",
      "source": "NIST2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0743_NIST2024",
      "name": "Data Privacy; Information Security; Value Chain and Component Integration; Harmful Bias and Homogenization",
      "description": "Test GAI system value chain risks (e.g., data poisoning, malware, other software and hardware vulnerabilities; labor practices; data privacy and localization compliance; geopolitical alignment).",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0744_NIST2024",
      "name": "Value Chain and Component Integration",
      "description": "Re-assess model risks after fine-tuning or retrieval-augmented generation implementation and for any third-party GAI models deployed for applications and/or use cases that were not evaluated in initial testing.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0745_NIST2024",
      "name": "Intellectual Property; CBRN Information or Capabilities",
      "description": "Take reasonable measures to review training data for CBRN information, and intellectual property, and where appropriate, remove it. Implement reasonable measures to prevent, flag, or take other action in response to outputs that reproduce particular training data (e.g., plagiarized, trademarked, patented, licensed content or trade secret material).",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0746_NIST2024",
      "name": "Information Integrity; Information Security; Value Chain and Component Integration",
      "description": "Review various transparency artifacts (e.g., system cards and model cards) for third-party models.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0747_NIST2024",
      "name": "Information Integrity",
      "description": "Implement real-time monitoring processes for analyzing generated content performance and trustworthiness characteristics related to content provenance to identify deviations from the desired standards and trigger alerts for human intervention.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0748_NIST2024",
      "name": "Information Integrity; Value Chain and Component Integration",
      "description": "Leverage feedback and recommendations from organizational boards or committees related to the deployment of GAI applications and content provenance when using third-party pre-trained models.",
      "source": "NIST2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0749_NIST2024",
      "name": "Human-AI Configuration",
      "description": "Use human moderation systems where appropriate to review generated content in accordance with human-AI configuration policies established in the Govern function, aligned with socio-cultural norms in the context of use, and for settings where AI models are demonstrated to perform poorly.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0750_NIST2024",
      "name": "CBRN Information or Capabilities; Confabulation",
      "description": "Use organizational risk tolerance to evaluate acceptable risks and performance metrics and decommission or retrain pre-trained models that perform outside of defined limits.",
      "source": "NIST2024",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0751_NIST2024",
      "name": "Information Integrity; Harmful Bias and Homogenization",
      "description": "Collaborate with external researchers, industry experts, and community representatives to maintain awareness of emerging best practices and technologies in measuring and managing identified risks.",
      "source": "NIST2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0752_NIST2024",
      "name": "CBRN Information or Capabilities; Confabulation; Information Security",
      "description": "Establish, maintain, and evaluate effectiveness of organizational processes and procedures for post-deployment monitoring of GAI systems, particularly for potential confabulation, CBRN, or cyber risks.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0753_NIST2024",
      "name": "Human-AI Configuration",
      "description": "Evaluate the use of sentiment analysis to gauge user sentiment regarding GAI content performance and impact, and work in collaboration with AI Actors experienced in user research and experience.",
      "source": "NIST2024",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0754_NIST2024",
      "name": "Confabulation",
      "description": "Implement active learning techniques to identify instances where the model fails or produces unexpected outputs.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0755_NIST2024",
      "name": "Human-AI Configuration; Harmful Bias and Homogenization",
      "description": "Share transparency reports with internal and external stakeholders that detail steps taken to update the GAI system to enhance transparency and accountability",
      "source": "NIST2024",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0756_NIST2024",
      "name": "Information Integrity",
      "description": "Track dataset modifications for provenance by monitoring data deletions, rectification requests, and other changes that may impact the verifiability of content origins.",
      "source": "NIST2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0757_NIST2024",
      "name": "Human-AI Configuration; Information Integrity",
      "description": "Verify that AI Actors responsible for monitoring reported issues can effectively evaluate GAI system performance including the application of content provenance data tracking techniques, and promptly escalate issues for response.",
      "source": "NIST2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0758_NIST2024",
      "name": "Harmful Bias and Homogenization",
      "description": "Conduct regular monitoring of GAI systems and publish reports detailing the performance, feedback received, and improvements made.",
      "source": "NIST2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0759_NIST2024",
      "name": "Human-AI Configuration; Dangerous, Violent, or Hateful Content",
      "description": "Practice and follow incident response plans for addressing the generation of inappropriate or harmful content and adapt processes based on findings to prevent future occurrences. Conduct post-mortem analyses of incidents with relevant AI Actors, to understand the root causes and implement preventive measures.",
      "source": "NIST2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0760_NIST2024",
      "name": "Human-AI Configuration",
      "description": "Use visualizations or other methods to represent GAI model behavior to ease non-technical stakeholders understanding of GAI system functionality",
      "source": "NIST2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0761_NIST2024",
      "name": "Confabulation; Information Integrity",
      "description": "Establish and maintain policies and procedures to record and track GAI system reported errors, near-misses, and negative impacts.",
      "source": "NIST2024",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0762_NIST2024",
      "name": "Information Security; Data Privacy",
      "description": "Report GAI incidents in compliance with legal and regulatory requirements (e.g., HIPAA breach reporting, e.g., OCR (2023) or NHTSA (2022) autonomous vehicle crash reporting requirements.",
      "source": "NIST2024",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0764_UK Government2023",
      "name": "Rigorous Risk Assessment Processes",
      "description": "Develop rigorous risk assessment processes for models, which: 1) Attempt to encompass all plausible and consequential risks from AI systems, including low-probability risks of severe harm; 2) Are informed by factors including, but not limited to: Model evaluations and red teaming, evidence of previous models' impacts and capabilities, knowledge of the latest research and developments in the field, domain expertise both internally and externally, and the results of data input audits. Take into account the difficulty of producing reliable risk assessments, creating a culture that takes seriously the significant uncertainty underlying predictions of the risks associated with frontier models. Take into account the potential benefits of the model. Include learnings from information exchanges with industry, academia and government on the capabilities of comparable models",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0765_UK Government2023",
      "name": "Pre-Deployment Risk Assessments",
      "description": "Devote resources to pre-development risk assessments, alongside prioritising pre- deployment risk assessments. Pre-development risk assessments are also important, because training a high-risk system can still lead to harm if the model is leaked, stolen, or otherwise unintentionally distributed.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0766_UK Government2023",
      "name": "Development and Post-Deployment Monitoring",
      "description": "Monitor systems both during development and after deployment. New risk assessments could be carried out in cases of fine-tuning or other substantial changes that could increase the danger of a model, such as the model gaining access to tools or plugins. This could occur alongside attempts to detect unexpected developments and new information that might have changed the results of the existing risk assessment, and which could also trigger a new risk assessment.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0767_UK Government2023",
      "name": "Risk Thresholds for Specific Models",
      "description": "Describe and continually refine risk assessment results for each model (\u201crisk thresholds\u201d) that would trigger particular risk-reducing actions, defining such results in terms of risk to all relevant stakeholders given currently existing mitigations. Given the high uncertainty around future model capabilities, risk thresholds may be refined periodically.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0768_UK Government2023",
      "name": "Defining Risk Thresholds",
      "description": "Define risk thresholds, based on the outcomes that would constitute a breach of the threshold and linked to dangerous capabilities that a given model or combination of models could exhibit. For example, a frontier AI organisation might identify the objective of avoiding deploying an AI system that significantly increases the risk of cyberattacks or fraud.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0769_UK Government2023",
      "name": "Operationalizing Risk Thresholds",
      "description": "Operationalise risk thresholds, including specific, testable observations, such that multiple observers with access to the same information would agree on whether a given threshold had been met. Specific observations would provide frontier AI organisations with opportunities to determine proactively how they would respond in difficult potential situations, and so respond immediately to such situations should they arise, as well as allowing for accountability and external verification. Given the nascent science of AI evaluation, however, it is unlikely to be possible to define a set of testable observations that detect all identified risks sufficiently reliably. Instead of relying solely on these predefined tests, risk assessments may take into account wider sources of evidence, such as concerning and unexpected observations that show up in exploratory analyses, expert forecasts, or risk related information from other frontier AI organisations. In particular, consideration may be given to any risks caused by the combination of a given model with other models or tools, whether developed by the same frontier AI organisation or otherwise.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0770_UK Government2023",
      "name": "Refinement of Risk Evaluation Frameworks",
      "description": "Continue to refine and redefine risk evaluation frameworks for models as necessary, aiming to reduce the gap between the intended objectives of risk thresholds and their present operationalisations. Such gaps are expected to exist due to limitations in the science of evaluation and in the state of knowledge surrounding capabilities, so progress towards a robust framework will probably be iterative. Risk evaluation frameworks may use multiple methods, including probability estimations and qualitative assessments of current capabilities.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0771_UK Government2023",
      "name": "Mitigation of Overshooting Risk Thresholds",
      "description": "Mitigate the risk of \u2018overshooting\u2019 risk thresholds. This may be achieved by setting deliberately conservative thresholds, including using intentionally lower buffer thresholds to trigger actions, such that the most concerning thresholds are difficult to overshoot without having already implemented mitigations at an earlier stage.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0772_UK Government2023",
      "name": "Risk Threshold Engagement with Relevant External Stakeholders",
      "description": "Engage with relevant external stakeholders when developing risk thresholds. Risk thresholds often concern externalities frontier AI organisations place on society, including both the potentially significant benefits of AI advancement and negative effects that might disproportionately affect specific stakeholder groups. As such, their risk thresholds may be made public to allow for external scrutiny, with thresholds set in consultation with relevant external stakeholders including relevant government authorities.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0773_UK Government2023",
      "name": "Development Constrained by Risk Thresholds",
      "description": "At each risk threshold, proactively commit to only proceed with certain development or deployment steps if specific mitigations are in place. Such mitigations could include many of the practices outlined in this document.",
      "source": "UK Government2023",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0774_UK Government2023",
      "name": "Reassessment of Residual Risk",
      "description": "After putting in place mitigations, reassess any residual risk posed to determine whether additional mitigations are required. Due to the unpredictability of capabilities advancements and the limitations of model evaluation science, pre-agreed mitigations may prove insufficient to place a given model within a risk threshold. Risk acceptance criteria may be used, as is standard in many other contexts, and may provide an important tool for clarification. These criteria may evolve with time, and could be quantitative or qualitative. For example, risk may only be accepted if it has been reduced to a level \u2018as low as reasonably practicable\u2019.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0775_UK Government2023",
      "name": "Disclosure of Risk Threshold to Government Authorities",
      "description": "Inform relevant government authorities when a risk threshold has been met, along with proposed mitigations. Inform governments again, in advance of deployment, when the mitigations and residual risk assessment have been carried out. Proactively engage other relevant actors in addition to governments as appropriate.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0776_UK Government2023",
      "name": "Adaptation of Mitigations to Development and Deployment Stages",
      "description": "When planning required mitigations, consider the full range of development and deployment stages. In general, meeting a risk threshold could require mitigations at multiple such stages. Important stages may include the following: 1) Continued training of model; 2) Deployment of model in small-scale ways, e.g., internal use; 3) Deployment of model in large-scale ways e.g., public release via API ; 4) Extension of model through greater affordances, e.g., tool use or internet access; 5) Irreversible deployment e.g. open-sourcing of models",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0777_UK Government2023",
      "name": "Adaptation of Mitigations for Unintended Model Use",
      "description": "Adapt mitigations in recognition of risks from unintended model use or use in unexpected contexts, such as a model that is modified to remove safeguards after open- sourcing or a model that is combined with another model for unanticipated purposes. For example, at a given risk threshold, information security control mitigations might be put in place before even internal use of a model within a frontier AI organisation. In general, the use of caution may be helpful, given current limitations in information security controls and the prediction of models\u2019 emergent abilities.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0778_UK Government2023",
      "name": "Recognition of Dangerous Model Possession",
      "description": "Recognise that even the possession of some models may be dangerous, even if not deployed or not deployed widely, due to the risk of being unable to secure a model sufficiently to prevent, for instance, a bad actor obtaining the model weights. In contrast, other models may pose significant risk only if deployed in a large-scale or irreversible way.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0779_UK Government2023",
      "name": "Reversible and Small-Scale Deployment",
      "description": "Deploy models in small-scale or reversible ways before deploying models in large-scale or irreversible ways. This makes it possible for frontier AI organisations to notice and mitigate harm before the harm becomes too large or unavoidable.",
      "source": "UK Government2023",
      "subcategoryId": "staged-deployment-3.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0780_UK Government2023",
      "name": "Pause of Training Runs",
      "description": "Prepare to pause training runs or reduce access to deployed models, if risk thresholds are reached without the committed risk mitigations being in place. This may involve warning existing customers that access reductions are a possibility and creating contingency plans to minimise negative impacts on customer use.",
      "source": "UK Government2023",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0781_UK Government2023",
      "name": "Risks of Open-Source Models",
      "description": "Acknowledge that some models may pose additional risks if made available \u201copen-source\u201d, even after mitigation attempts. This is because of the inability of recalling an open-sourced model and the potential ability of users to remove safeguards and introduce new (and potentially dangerous) capabilities. However, it is also important to bear in mind the significant benefits of \u201copen-source\u201d AI systems for researchers, including for advancing AI safety, which may in some cases outweigh these potential risks.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0782_UK Government2023",
      "name": "Risk Assessment Disclosure to Third Parties",
      "description": "Regularly update relevant stakeholders on risk assessment and mitigation measures: This will enable assessment of whether AI organisations have sufficient risk management processes in place, build up a picture of best practices, and make recommendations to address gaps. When sharing this information with external actors, consideration should be given to commercially sensitive information. Additional ad hoc updates could be provided in cases of major developments.Include information on evaluations, risk assessment and mitigation, and individuals involved. For example: 1) What types of tests and evaluations are being run on which types of models; 2) What other risk assessment methods are being used, which kinds of expertise are drawn on, and whether impacted stakeholders are being involved; 3) How risk mitigation measures are being monitored; 4) Which teams and individuals are involved at different stages of the risk management process (and how, if at all, third parties are involved); 5) Measures taken to address specific categories of risk, such as cybersecurity measures",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0783_UK Government2023",
      "name": "Robust and Meaningful Accountability Mechanisms",
      "description": "Introduce robust and meaningful accountability mechanisms, especially in evaluating capabilities thresholds, with clear processes that ensure the correct mitigations or courses of action are followed if the thresholds are met. This may include board sign-off for the responsible capability scaling policy, and named individual accountability for key decisions.",
      "source": "UK Government2023",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0784_UK Government2023",
      "name": "Effective Risk Governance",
      "description": "Establish effective risk governance to ensure that risks are appropriately identified, assessed, and addressed, and their nature and scale transparently reported. Most importantly, provide internal checks and balances, which may include thoughtful separation of roles within risk management.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0785_UK Government2023",
      "name": "Verification Mechanisms",
      "description": "Include verification mechanisms, such that external actors can have increased confidence that responsible capability scaling policies are executed as intended.",
      "source": "UK Government2023",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0786_UK Government2023",
      "name": "Model Evaluation of Dangerous Capabilities",
      "description": "Evaluate models for potential dangerous capabilities (i.e. capabilities that could cause substantial harm either from intentional misuse or accident). These capabilities could include but are not limited to: 1) Offensive cyber capabilities (e.g. producing code to exploit software vulnerabilities); 2) Deception and manipulation (e.g. lying effectively or convincing people to take costly actions); 3) Capabilities that can assist users in developing, designing, acquiring, or using biological, chemical, or radiological weapons (e.g. helping users \u201ctroubleshoot\u201d their efforts to produce biological weapons)",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0787_UK Government2023",
      "name": "Model Evaluation of Controllability",
      "description": "Evaluate models for controllability issues (i.e. propensities to apply their capabilities in ways that neither the models\u2019 users nor the models\u2019 developers want). This could include, for example, autonomous replication and adaptation (i.e. capabilities that could allow a model to copy and run itself on other computer systems).",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0788_UK Government2023",
      "name": "Model Evaluation of Societal Harm",
      "description": "Evaluate models for societal harms. These could include, for example, bias and discrimination (e.g. the risk that they produce content that reinforces harmful stereotypes or their potential discriminatory influence if used to inform decisions), recognising that \u2018bias\u2019 can be difficult to define and can be subject to different interpretations in different contexts.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0790_UK Government2023",
      "name": "Response Processes for Evaluation Results",
      "description": "Ensure processes are in place to respond to evaluation results. Evaluations are a necessary input to a responsible capability scaling policy which, depending on the results of the evaluation, would probably require implementation of practices outlined in other sections of this document such as preventing model misuse, information sharing and other risk mitigation measures.",
      "source": "UK Government2023",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0791_UK Government2023",
      "name": "Evaluation of Predecessor Models",
      "description": "Before a frontier model is trained, evaluate predecessor or analogous models to understand how relevant properties (e.g. dangerous capabilities) scale with the overall size of the model. These preliminary evaluations can inform risk assessments.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0792_UK Government2023",
      "name": "Pre-training and Fine-tuning of Models",
      "description": "During pre-training and fine-tuning, evaluate the model to detect signs of undesirable properties and identify inaccuracies in pretraining predictions. These evaluations could be undertaken at various pre-specified checkpoints, and could inform decisions about whether to pause or adjust the training process.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0793_UK Government2023",
      "name": "Pre-Deployment Evaluations",
      "description": "After training, subject the model to extensive pre-deployment evaluations. These evaluations can inform decisions about whether and how to deploy the system, as well as allowing governments and potential users to make informed decisions about regulating or using the model. Their intensity will be proportional to the risk of the deployment, taking into account the model\u2019s capabilities, novelty, expected domains of use, and number of individuals expected to be affected by it.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0794_UK Government2023",
      "name": "Post-Deployment Evaluations",
      "description": "After deployment, conduct evaluations at regular intervals to identify new capabilities and associated risks, especially when notable developments (e.g. a major update to the model) suggest earlier evaluations have become obsolete. Post-deployment evaluations can inform decisions to update the system\u2019s safeguards, increase security around the model, temporarily limit access, or roll back deployment.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0795_UK Government2023",
      "name": "Context-Specific Model Evaluations",
      "description": "Require organisations who deploy their models to conduct context-specific model evaluations. This requires that the information and data required to successfully conduct such assessments is provided to deployers.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0796_UK Government2023",
      "name": "Independent Evaluation with Subject Matter Expertise",
      "description": "Ensure that evaluators are independent and have sufficient AI and subject matter expertise across a wide range of relevant subjects and backgrounds. External evaluators\u2019 relationships with frontier AI organisations could be structured to minimise conflicts of interest and encourage independence of judgement as far as practically possible. As well as expertise in AI, there are many other areas of subject matter expertise that will be needed to evaluate an AI system\u2019s features. For instance, experts on topics as wide as fairness, psychological harm, and catastrophic risk will be needed.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0797_UK Government2023",
      "name": "Appropriate Safeguards Against External Evaluations",
      "description": "Ensure that there are appropriate safeguards against external evaluations leading to unintended widespread distribution of models. Allowing external evaluators to download models onto their own hardware increases the chance of the models being stolen or leaked. Therefore, unless adequate security against widespread model distribution can be assured, external evaluators could only be allowed to access models through interfaces that prevent exfiltration (such as current API access methods). It may be appropriate to limit evaluators\u2019 access to information that could indirectly facilitate widespread model distribution in other ways, such as requiring in-depth KYC checks or watermarking copies of the model.",
      "source": "UK Government2023",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0798_UK Government2023",
      "name": "Sufficient Time for External Evaluators",
      "description": "Give external evaluators sufficient time. As expected risks from models increase or models get more complex to evaluate, the time afforded for evaluation may need to increase as well.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0799_UK Government2023",
      "name": "Fine-tuning by External Evaluators",
      "description": "Give external evaluators the ability to securely \u201cfine-tune\u201d the AI systems being tested. Evaluators cannot fully assess risks associated with widespread model distribution if they cannot fine-tune the model. This may involve providing external evaluators with access to capable infrastructure to enable fine-tuning.",
      "source": "UK Government2023",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0800_UK Government2023",
      "name": "External Evaluation of Model without Safety Regulations",
      "description": "Give external evaluators access to versions of the model that lack safety mitigations. Where possible, sharing these versions of a model gives evaluators insight into the risks that might be created if users find ways to circumvent safeguards (i.e. \u201cjailbreak\u201d the model). If the model is open-sourced, leaked, or stolen, users may also simply be able to remove or bypass the safety mitigations.",
      "source": "UK Government2023",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0801_UK Government2023",
      "name": "External Evaluation of Model Families",
      "description": "Give external evaluators access to model families and internal metrics. Frontier AI organisations often develop \u201cmodel families\u201d where multiple models differ along only one or two dimensions \u2013 such as parameters, data, or training compute. Evaluating such a model family would enable scaling analysis to better forecast future performance, capabilities and risks.",
      "source": "UK Government2023",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0802_UK Government2023",
      "name": "External Evaluation of Deployed System Components",
      "description": "Give external evaluators the ability to study all of the components of deployed systems, where possible. Deployed AI systems typically combine a core model with smaller models and other software components, including moderation filters, user interfaces to incentivise particular user behaviour, and plug-ins for extension capabilities like web browsing or code execution. For example, a red team cannot find all the flaws in the defences of a system if they aren\u2019t able to test all of its different components. It is important to consider the need to balance external evaluators\u2019 ability to access all components of the system against the need to protect information that would allow bypassing model defences.",
      "source": "UK Government2023",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0803_UK Government2023",
      "name": "Discretionary Evaluator Disclosure",
      "description": "Allow evaluators to share and discuss the results of their evaluations, with potential restrictions where necessary e.g. not sharing proprietary information, information whose spread could lead to substantial harm or information that would have an adverse effect on competition in the market. Sharing the results of evaluations can allow governments, regulators, users, and other frontier AI organisations to make informed decisions.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0804_UK Government2023",
      "name": "Development and testing of Model Evaluation Methods",
      "description": "Support the development and testing of model evaluation methods. For many relevant properties of models, there do not yet exist accepted evaluation methods. It also remains unclear how reliable or predictive current evaluation methods are. This could involve frontier AI organisations developing model evaluation methods themselves or facilitating the efforts of others, such as by providing access to capable infrastructure for evaluation.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0805_UK Government2023",
      "name": "Disclosure of Model Evaluation Research and Development",
      "description": "Share the products of their model evaluation research and development, except when sharing the results might be harmful. In some cases, findings (e.g. about how to elicit dangerous capabilities) could be harmful if spread. When the expected harm is sufficiently small, the AI research community, other frontier AI organisations, and relevant government bodies could benefit from being informed of their work.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0806_UK Government2023",
      "name": "Disclosure of Risk Assessment Processes",
      "description": "Share details of risk assessment processes and risk mitigation measures with relevant government authorities and other AI companies, as set out in Responsible Capability Scaling.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0807_UK Government2023",
      "name": "Disclosure of Information about Internal Governance Processes",
      "description": "Share information about how internal governance processes are set up with relevant government authorities. This will ensure that risks are appropriately identified, communicated and mitigated, and allow government and other external actors to identify gaps that might lead to risks being overlooked. This information could be updated regularly (e.g. every 12 months). This information could also be made public, provided sensitive details are removed.",
      "source": "UK Government2023",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0808_UK Government2023",
      "name": "Reporting Security Details to Government Authorities",
      "description": "Report any details of security or safety incidents or near-misses to relevant government authorities. This includes any compromise to the security of the organisation or its systems, or any incident where an AI system \u2013 deployed or not \u2013 causes substantial harm or is close to doing so. This will enable government authorities to build a clear picture of when safety and security incidents occur and make it easier to anticipate and mitigate future risks. Incident reports could include a description of the incident, the location, start and end date, details of any parties affected and harms occurred, any specific models involved, any relevant parties responsible for managing and responding to the incident, as well as ways in which the incident could have been avoided. It is important that incidents indicative of more severe risks are reported as soon as possible after they occur. High-level details of safety and security incidents - with sensitive information removed - could also be made public, such as have been shared in the AI incident database.",
      "source": "UK Government2023",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0809_UK Government2023",
      "name": "Pre-training Disclosure of High-Level Model Details to Government Authorities",
      "description": "Before training, share high-level model details with the relevant government authorities and justify why the training run does not impose unacceptable risk. This includes: 1) A high-level description of the model (including high-level information on intended use cases, intended users, training data, and model architecture); 2) Compute details (including the maximum the organisation plans to use, as well as information about its location and who provides it); 3) Description of the data that will be used to train the model; 4) Evidence from scaling small models that the full training run does not pose unacceptably high risks; 5) Descriptions of specific internal and external risk assessments and mitigation efforts,3 and an overall safety assessment justifying why and how the training run is sufficiently low-risk to execute; 6) Description of which, if any, domain experts and impacted stakeholders have been involved in the project\u2019s design, as well as risk and impact assessment; 7) Plans for model evaluations during and after training, as well as predicted dangerous capabilities",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0810_UK Government2023",
      "name": "Updates to Government Authorities During Training",
      "description": "During training, update information provided to relevant government authorities with any significant changes to the model itself or its risk profile. This could include: 1) Updates on model development at each evaluation checkpoint as well as any significant updates to the development plan; 2) Results from model evaluations, including details of emergent dangerous capabilities and whether these were expected; 3) Whether and how the risk context has changed (e.g. if other AI tools have been published that the model could interact with)",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0811_UK Government2023",
      "name": "Disclosure of Risk Context to Government Authorities at Deployment",
      "description": "At the point of deployment, share details of the model, any risks the model might pose and what steps have been taken to mitigate those risks with relevant government authorities and the wider public. Information can be provided in full to the relevant government authority, ensuring that robust security measures are in place to protect sensitive information. Some of this information can be made available to the public by publishing a transparency report (e.g. a model card) and providing general overviews of model purpose and risk assessment evaluation results. Information that exposes model vulnerabilities or facilitates the spread of dangerous capabilities should be redacted from the transparency report, unless sharing this information publicly would be sufficiently helpful for mitigating the risks the model poses. Information that is commercially sensitive (e.g. detailed information on training data) or exposes the capabilities or vulnerabilities of the model should be redacted. Commercially sensitive information could be shared with government or regulators, who could share this information with industry in an aggregated or anonymised form. Information shared at deployment could include: 1) A description of the model; 2) Information about safe practices for model usage (including domains of inappropriate and appropriate use, or guidelines on determining whether a use is appropriate); 3) Training details, including a detailed description of the training data and any biases they may encode; 4) The model\u2019s biases, risks and limitations; 5) Pre-development and pre-deployment risk and impact assessment procedures; 6) Details of the evaluation process the frontier AI organisation conducted, including time and resources spent, information about the expertise and independence of people conducting the evaluations, the level of access given to evaluators, and anticipated limitations of the evaluations used; 7) Details about which, if any, domain experts and impacted stakeholders have been involved in the project\u2019s design and risk and impact assessment; 8) The results of any internal or external evaluations that were conducted; 9) Holistic assessments of the models\u2019 robustness, interpretability, and controllability, drawing on more specific evaluation results; 9) Significant measures taken to mitigate potential harmful consequences of deployment, including accountability and verification mechanisms put in place, and internal governance processes carried out; 10) Capabilities and risks of the final, public-release model before and after safety mitigations, including a description of the mitigations to prevent accidents and misuse (e.g. available tools and their expected effectiveness); 11) Plans for ongoing, post-deployment monitoring of risks and capabilities and how the organisation will respond to future incidents (unless releasing this information would allow bad actors to circumvent post-deployment safety measures); 12) Descriptions of post-deployment access controls; 13) Expected compute requirements for running the model during deployment",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0812_UK Government2023",
      "name": "Adjucation around Publicly Sharing Risks",
      "description": "Before sharing information, consider the risks of sharing this information and judge whether it is inappropriate to share certain pieces of information. In particular, it is important to consider potential harms from publicly sharing information about dangerous capabilities and methods for eliciting them, as this information could motivate or help other actors to acquire these capabilities. It is also important to consider potential harms from publicly sharing detailed information about how models were produced, as this may lower barriers to producing similar models. If the models have or could be modified to possess dangerous capabilities (e.g. biological capabilities or surveillance capabilities), then facilitating widespread distribution of the model in this way may be harmful. The National Protective Security Authority (NPSA) guidance on a security-minded approach to information management may prove helpful for sharing information appropriately.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0813_UK Government2023",
      "name": "Principled Policies for Disclosure",
      "description": "Develop principled policies about what information to share publicly, with governments, or not at all. These policies could specify situations under which sharing some piece of information is subject to a risk assessment, along with guidelines for conducting the risk assessment and responding to it. It is important, however, to avoid creating risk assessment criteria and procedures that are overly strict and intensive to prevent excessive opacity. These policies could be guided and overseen by an independent review panel of multidisciplinary experts to ensure decisions made about or against information sharing are justifiable and oriented to optimal transparency.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0814_UK Government2023",
      "name": "Disclosure of Information with AI-focused Government Authorities",
      "description": "Share complete forms of this information with central AI-focused government authorities (including central government bodies, security agencies, and regulators) to enable robust government oversight of potentially high-risk areas of AI development and the processes in place to identify and manage those risks. These authorities could then further share information selectively with additional government bodies where relevant. Some particularly security-relevant pieces of information may need to be shared directly with security agencies, such as the cybersecurity measures being used in model development (which would make it easier for those agencies to identify potential security risks), or specific physical or cybersecurity threat incidents. Robust security measures are in place when sharing more sensitive information.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0815_UK Government2023",
      "name": "Sharing of Limited Information",
      "description": "Share more limited information with other AI organisations in order to facilitate learning and the development of best practices. This could include best practices and lessons learned in the development of risk assessment and mitigation measures and risk governance processes, some details about safety and security incidents (to enable increased awareness while protecting intellectual property), and highlights and lessons learned from risk assessments and capability evaluation. In general, it will be easier for organisations to share model-agnostic information with one another than model-specific information, which may be more commercially sensitive. It will also be important to consider that there is no privileged access to information that may give a competitive advantage to some firms.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0816_UK Government2023",
      "name": "Sharing with Independent Third-Parties",
      "description": "Share specific information with independent third-parties where this aids evaluation and technical audit (see Model Evaluations and Red Teaming). This may require sharing much of the same information that is shared with governments in full but may be shared on a case-by- case basis.",
      "source": "UK Government2023",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0817_UK Government2023",
      "name": "Information Sharing with Downstream Users",
      "description": "Share specific information with downstream users of the model to enable more effective risk mitigation across the AI supply chain and build consumer confidence. This could include uses of the model that are against their terms of service, and could be provided to users through user terms or published information about the model.",
      "source": "UK Government2023",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0818_UK Government2023",
      "name": "Sharing General Information with the Public",
      "description": "Share more general versions of the information outlined with the public to enable public scrutiny and build public confidence in the safety and reliability of AI systems. This could include high-level summaries of risk assessment and mitigation processes, high-level details of safety and security incidents, summaries of risk governance processes, and general overviews of model purpose, use cases, and the results of risk assessments and capability evaluations.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0819_UK Government2023",
      "name": "Application of Infrastructure Principles",
      "description": "Apply good infrastructure principles to every part of this process from design to decommissioning. This is important as threats can occur at different stages of an AI project\u2019s life cycle.",
      "source": "UK Government2023",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0820_UK Government2023",
      "name": "Regular Assessment of Supply Chain Security",
      "description": "Regularly assess the security of their supply chains and ensure suppliers adhere to the same standards their own organisation applies. Ensuring data, software components and hardware are obtained from trusted sources will help mitigate supply chain risk.",
      "source": "UK Government2023",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0821_UK Government2023",
      "name": "Model Robustness to Adversarial Attacks",
      "description": "Evaluate the robustness of models to different classes of adversarial attack (such as poisoning, model inversion and model stealing), based on priorities derived from threat modelling. This could involve a combination of benchmarking and red teaming.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0822_UK Government2023",
      "name": "Documentation of the Cyber security threats",
      "description": "Assess and document the cyber security threats against the AI system overall and mitigate the impact of vulnerabilities. Good documentation and monitoring will inform your overall risk posture and help you respond in the event of a security incident.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0823_UK Government2023",
      "name": "Value of AI-related Assets",
      "description": "Understand the value of AI-related assets such as models, data (including user feedback), prompts, software, documentation, and assessments (including information about potentially unsafe capabilities and failure modes) to their organisation. Protect these different categories of information as appropriate.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0824_UK Government2023",
      "name": "Security Integration into Business Decisions",
      "description": "Ensure security is factored into all business decisions and AI-related assets are identified and protected with proportionate cyber, physical and personnel security measures. Secure Innovation guidance from NCSC and NPSA is available to help companies and investors to protect their technology .",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0825_UK Government2023",
      "name": "Tracking Tools for Security and Version Control Assets",
      "description": "Have processes and tools to track, authenticate, secure and version control assets and be able to roll back to a known safe state in the event of a compromise. This is important as data and models do not remain static during the whole lifespan of an AI project.",
      "source": "UK Government2023",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0826_UK Government2023",
      "name": "Data Documentation",
      "description": "Document data, models, prompts, evaluation materials and other assets, using commonly used structures such as data cards, model cards and software bills of materials (SBOMs). This will allow you to identify and share key information, including particular security concerns, quickly and easily.",
      "source": "UK Government2023",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0827_UK Government2023",
      "name": "Training Relevant Actors in AI Security",
      "description": "Train developers, system owners and senior leaders in secure AI practices. It is crucial to establish a positive security culture where leaders demonstrate good security practice and staff from across a project understand enough about AI security to understand the potential consequences of decisions they take.",
      "source": "UK Government2023",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0828_UK Government2023",
      "name": "Awareness of Security Threats",
      "description": "Maintain an awareness of security threats and failure modes, in particular data scientists and developers. AI development and cyber security are two different skill sets and building a team at the intersection of the two will require effort and time.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0829_UK Government2023",
      "name": "Model of System Threats in Misuse Scenarios",
      "description": "Model the threats to your system to understand the impacts to the system, users, organisation and wider society if the model is misused or behaves unexpectedly. This can help build systems where unanticipated model outputs are handled safely by other parts of a data pipeline.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0830_UK Government2023",
      "name": "Incident Response, Escalation, and Remeditation Plans",
      "description": "Develop an organisational incident response plan. The inevitability of security incidents affecting systems is reflected in organisational incident response planning. A well-planned response will help minimise the damage caused by an attack and support recovery.",
      "source": "UK Government2023",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0831_UK Government2023",
      "name": "Recurrent Performance analysis of AI models",
      "description": "Measure the performance of AI models and systems on an ongoing basis. A decline in model performance could be an indication of an attack or could indicate that a model is encountering data that is different from that which it was trained on. Either way, further investigation may be required.",
      "source": "UK Government2023",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0832_UK Government2023",
      "name": "Monitoring Inputs to AI Systems for Audit Development",
      "description": "Monitor and log inputs to AI systems to enable audit, investigation and remediation in the case of compromise. Some attacks against AI systems rely on repeated querying. Proper logging will help you audit your system and identify any anomalous inputs.",
      "source": "UK Government2023",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0833_UK Government2023",
      "name": "Mitigation Actions for AI-related security incidents",
      "description": "Take action to mitigate and remediate issues and document any AI-related security incidents and vulnerabilities.",
      "source": "UK Government2023",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0834_UK Government2023",
      "name": "Security Responsible User Communication",
      "description": "Communicate clearly to users which elements of security they are responsible for and where and how their data may be used or accessed.",
      "source": "UK Government2023",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0835_UK Government2023",
      "name": "Default Integration of Secure Settings",
      "description": "Integrate the most secure settings within your products by default. Where configuration is necessary, default options should be broadly secure against common threats.",
      "source": "UK Government2023",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0836_UK Government2023",
      "name": "Default Necessary Security Updates",
      "description": "Ensure necessary security updates are a default part of every product and use secure, modular update procedures to distribute them. This will help your products remain secure in the face of new and developing threats.",
      "source": "UK Government2023",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0837_UK Government2023",
      "name": "Release After Security Evaluations",
      "description": "Only release models after they have been through security evaluations, including benchmarking and red teaming.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0838_UK Government2023",
      "name": "Open Lines of Communication for Product Security Feedback",
      "description": "Maintain open lines of communication for feedback regarding product security, both internally and externally to your organisation, including mechanisms for security researchers to report vulnerabilities and receive legal safe harbour for doing so, and for escalating issues to the wider community. Helping to share knowledge and threat information will strengthen the overall community\u2019s ability to respond to AI security threats.",
      "source": "UK Government2023",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0839_UK Government2023",
      "name": "Identification of Assets of Organizational Value",
      "description": "Identify assets & systems that are important for the delivery of effective operations, or are of specific organisational value (e.g. commercially sensitive information)",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0840_UK Government2023",
      "name": "Categorization of Assets",
      "description": "Categorise and classify assets in order to ensure that the correct level of resource is used in implementing risk mitigations. - risk management",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0841_UK Government2023",
      "name": "Threat Identification",
      "description": "Identify threats. These may include terrorism or hostile state threats and/or more local and specific threats, and use a range of internal and external resources.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0843_UK Government2023",
      "name": "Protective Security Risk Register",
      "description": "Build a protective security risk register to record, in sufficient detail, all the data gathered during this risk management process, ensuring compatibility with existing organisational risk management registers and processes.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0844_UK Government2023",
      "name": "Development of Protective Security Strategy",
      "description": "Develop a protective security strategy for mitigating the risks identified, which reviews protective security measures in relation to a prioritised list of risks. Where mitigations are assessed as inadequate, additional measures could be proposed for approval by the decision maker(s).",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0845_UK Government2023",
      "name": "Development and Implementation Plans",
      "description": "Produce development & implementation plans. Aim to arrive at a clear, prioritised list of protective security mitigations, which span physical, personnel and cyber security disciplines, and are linked to the technical guidance needed to implement them.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0846_UK Government2023",
      "name": "Regular Risk Management Review",
      "description": "Review risk management measures regularly and when required e.g. on a change in threat or change to operational environment, or to assess the suitability of new measures implemented. More detailed description of protective security risk management is provided on the NPSA website.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0847_UK Government2023",
      "name": "Board-level Responsibility for Protective Security",
      "description": "Ensure board-level responsibility for protective security with regular engagement with key stakeholders from across the business and a firm understanding of the risks the organisation faces. Ensure stakeholder engagement throughout the business for specialist insight and development and implementation of an insider risk mitigation programme.",
      "source": "UK Government2023",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0848_UK Government2023",
      "name": "Suitable Level of Screening",
      "description": "Apply a suitable level of screening, informed by a role-based risk assessment, to all individuals who are provided access to organisational assets including permanent, temporary and contract workers.",
      "source": "UK Government2023",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0849_UK Government2023",
      "name": "Role-Based Security Risk Assessment",
      "description": "Use Role-Based Security Risk Assessment to identify physical, personnel or cyber security measures that need to be applied in order to mitigate insider risk",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0850_UK Government2023",
      "name": "Proportionate Policies",
      "description": "Put in place proportionate policies, clear reporting procedures and escalation guidelines that are accessible, understood and consistently enforced.",
      "source": "UK Government2023",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0851_UK Government2023",
      "name": "Appropriate Security Training",
      "description": "Provide appropriate security education and training for all workers. Without effective education and training individuals cannot be expected to know what procedures are in place to maintain security.",
      "source": "UK Government2023",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0852_UK Government2023",
      "name": "Program of Monitoring for Security Issues",
      "description": "Ensure that a programme of monitoring and review is in place to enable potential security issues, or personal issues that may impact on an employee's work, to be recognised and dealt with effectively throughout their career",
      "source": "UK Government2023",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0853_UK Government2023",
      "name": "Use of Established Security Guidance",
      "description": "Use established, evidence based guidance to fully address personnel security risks e.g. NPSA guidance on Personnel Security",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0854_UK Government2023",
      "name": "Jailbreaking Method Vulnerability Reports",
      "description": "This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including: Reports for inducing models to bypass moderation features, which the frontier AI organisation has attempted to prevent through the use of filters or fine-tuning",
      "source": "UK Government2023",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0855_UK Government2023",
      "name": "Prompt Injection Vulnerability Reports",
      "description": "This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including:Reports used by malicious actors to induce models to exhibit behaviours they want by presenting models with prompts that contain instructions to perform these behaviours",
      "source": "UK Government2023",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0856_UK Government2023",
      "name": "Privacy Attacks Vulnerability Reports",
      "description": "This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including: methods for extracting information that should be private from models (e.g. sensitive information from training data or users\u2019 private conversations with models)",
      "source": "UK Government2023",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0857_UK Government2023",
      "name": "Vulnerability Reports for Unaddressed Misuse Opportunities",
      "description": "This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including: methods for using the capabilities of models to cause harm, which have not already been addressed",
      "source": "UK Government2023",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0858_UK Government2023",
      "name": "Vulnerability Reports for Poisoning Attacks",
      "description": "This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including: Reports for when an adversary has manipulated training data in order to degrade model performance",
      "source": "UK Government2023",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0859_UK Government2023",
      "name": "Vulnerability Reports for Bias and Discrimination",
      "description": "This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including:Reports for when a model is exhibiting behaviours that reveal specific biases or discrimination regarding known protected characteristics",
      "source": "UK Government2023",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0860_UK Government2023",
      "name": "Vulnerability Reports for Performance Issues",
      "description": "This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including: when a model performs inadequately for a situation it is being deployed in e.g. a healthcare chatbot AI that provides incorrect information and causes harm to patients",
      "source": "UK Government2023",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0861_UK Government2023",
      "name": "Vulnerability Reports for Controllability Issues (i.e., \"Misalignment\")",
      "description": "This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including: when models apply their capabilities in ways that substantially diverge from what users intend or desire",
      "source": "UK Government2023",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0862_UK Government2023",
      "name": "Model Vulnerability Reports",
      "description": "This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including: Establish clear, user-friendly, and publicly described processes for receiving model vulnerability reports drawing on established software vulnerability reporting processes. These processes can be built into \u2013 or take inspiration from \u2013 processes that organisations have built to receive reports of traditional software vulnerabilities. It is crucial that these policies are made publicly accessible and function effectively.",
      "source": "UK Government2023",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0863_UK Government2023",
      "name": "Cost-Benefit Analysis of Sharing Vulnerability Reports",
      "description": "Consider the ways in which sharing information about vulnerabilities can both exacerbate and mitigate risks. Sharing can alert would-be attackers, but also alert would-be victims and actors with the power to create defences. One particularly important factor is how easily fixable the model vulnerability is. If a vulnerability will take a very long time to fix, or cannot be fixed, then public reporting may not be justified.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0864_UK Government2023",
      "name": "Development of Protocols for Disclosure of Model Vulnerability Information",
      "description": "Developing \u2013 and publicly describe \u2013 protocols for deciding how to share model vulnerability information. These protocols may, for example, outline conditions under which information is to be shared with different actors depending on the type of harm or vulnerability identified.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0865_UK Government2023",
      "name": "Mechanisms to Disclose Vulnerability Reports to Third Parties",
      "description": "Put in place mechanisms to disclose information about vulnerabilities to relevant government authorities, law enforcement, and other affected organisations. This may be particularly relevant for vulnerabilities where public disclosure might increase the risk of harm.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0866_UK Government2023",
      "name": "Public Sharing of Model Vulnerability Reporting Programs",
      "description": "Publicly share general lessons learned from model vulnerability reporting programs. This might include, for example, lessons about challenges faced and the relative efficacy of different incentive strategies. Such sharing could be done via a mechanism similar to the National Institute for Science and Technology\u2019s National Vulnerability Database in the US.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0867_UK Government2023",
      "name": "Identification of AI-generated content",
      "description": "Research techniques that allow AI-generated content to be identified. Invest in researching how AI-generated content may be watermarked, including AI- generated text, photos and videos, and experiment with the implementation of such techniques. It is particularly technically difficult to attach watermarks and prove the provenance of text.One method could involve making the model more statistically likely to use certain phrases or words in a way that is unnoticeable to humans, but can be picked up by a detector, provided a long enough sequence of text. However, this approach may not be robust to attempts to scrub off the watermark e.g. by having another AI model paraphrase the text.",
      "source": "UK Government2023",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0868_UK Government2023",
      "name": "Robust Watermarks for AI generated content",
      "description": "Explore the use of watermarks for AI generated content that are robust to various perturbations. Explore the use of watermarks for AI generated content that are robust to various perturbations after their creation, including attempts at removal. To make the removal of watermarks more difficult, developers of generative AI models may need to consider how they distribute certain information about their watermarking methods or open-sourcing their classifiers. This may also include monitoring ways in which adversarial users are attempting to scrub off their watermarks and patching, where relevant, such means of circumvention. It also includes a recognition that watermarking however may not be appropriate in all circumstances given the limitations.",
      "source": "UK Government2023",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0869_UK Government2023",
      "name": "AI Output Databases",
      "description": "Explore databases of content generated or manipulated by a model to identify AI- generated content. These databases could be queried by third parties, including auditors and regulators, facilitating identification of potentially AI-generated content. Such databases could include only a subset of generated content, which is flagged as potentially important. To ensure user privacy, privacy-preserving techniques could be explored in conjunction with such databases, such as hashing technologies. This allows for the identification of AI-generated content without the need to store the actual content, thereby respecting user privacy. Additionally, common standards between different databases from various frontier AI organisations could facilitate a unified search, allowing for the identification of AI-generated content across all frontier AI organisations simultaneously.",
      "source": "UK Government2023",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0871_UK Government2023",
      "name": "Evaluation Research",
      "description": "Improving our ability to assess the capabilities, limitations, and safety- relevant features of AI systems",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0872_UK Government2023",
      "name": "Robustness Research",
      "description": "Improving the resilience of AI systems e.g. against attacks intended to disrupt their proper functioning",
      "source": "UK Government2023",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0873_UK Government2023",
      "name": "Reliability and controllability (or \u201calignment\u201d) research",
      "description": "Improving the consistency of an AI system in adhering to the specifications it was programmed to carry out and operating in accordance with the designer\u2019s intentions, and decreasing its potential to behave in ways its user or developer does not want (e.g. producing offensive or biased responses, not refusing harmful requests, or employing harmful capabilities without prompting)",
      "source": "UK Government2023",
      "subcategoryId": "model-alignment-2.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "reinforcement-learning",
        "foundation"
      ]
    },
    {
      "id": "A0876_UK Government2023",
      "name": "Cybersecurity Research",
      "description": "Improving our ability to ensure the security of AI systems",
      "source": "UK Government2023",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0877_UK Government2023",
      "name": "Criminality Research",
      "description": "Improving our ability to prevent criminal behaviour through the use of AI systems (e.g. fraud, online child sexual abuse)",
      "source": "UK Government2023",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0878_UK Government2023",
      "name": "Research into other Societal Harms",
      "description": "Improving our ability to prevent other societal harms arising from the use of AI systems, including psychological harm, misinformation, and other societal harms.",
      "source": "UK Government2023",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0879_UK Government2023",
      "name": "Built-in Tools for Harm Mitigation",
      "description": "When a probable and consequential harm from a frontier AI organisation\u2019s system is identified, investigate whether there are tools that can be built to mitigate this harm. For instance, recognizing the rise in AI-generated child exploitation and abuse content, some social media platforms are developing tools for identifying and removing child sexual abuse content.",
      "source": "UK Government2023",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0881_UK Government2023",
      "name": "Defense Tool Availability Before system release",
      "description": "The larger the risk is and the more effective tools may be, the more important it is to prepare defensive tools ahead of time. It may be important to delay system releases until appropriate defensive tools are ready.",
      "source": "UK Government2023",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0882_UK Government2023",
      "name": "Discretion around sharing Defense Tools",
      "description": "Disseminate defensive tools responsibly, sometimes sharing them publicly and sometimes sharing them only with particular actors. In some cases, making a tool freely available (e.g. by open-sourcing it) may reduce its effectiveness by allowing malicious actors to study it and circumvent it.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0883_UK Government2023",
      "name": "Continuous Updating of Defensive Tools",
      "description": "Continuously update defensive tools as workarounds are discovered. In some cases, this may be an ongoing effort that requires sustained investment.",
      "source": "UK Government2023",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0884_UK Government2023",
      "name": "Societal Impact Research",
      "description": "Study the societal impacts of AI systems they have deployed, particularly by collaborating with external researchers, independent research organisations, and third party data owners. By collaborating and joining their data with that of third parties, such as internet platforms, frontier AI organisations can assess the impacts of their AI systems. Privacy-enhancing technologies could be employed to enable data sharing between frontier AI organisations, third parties, and external researchers while protecting confidential information. As well as data, frontier AI organisations could also facilitate research on the societal impacts of their AI systems by providing access to the necessary infrastructure and compute.",
      "source": "UK Government2023",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0885_UK Government2023",
      "name": "Third Party Knowledge for AI systems' Downstream societal impacts",
      "description": "Draw on multidisciplinary expertise and the lived experience of impacted communities to assess the downstream societal impacts of their AI systems. Impact assessments that account for a wide range of potential societal impacts and meaningfully involve affected stakeholder groups could help to anticipate further downstream societal impacts.",
      "source": "UK Government2023",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0886_UK Government2023",
      "name": "Societal Impact Informed Risk Assessments",
      "description": "Use assessments of downstream societal impacts to inform and corroborate risk assessments. Downstream societal impacts, such as threats to democracy, widespread unemployment, and environmental impacts, could be considered in risk assessments of AI systems, alongside more direct risks. See the Responsible Capability Scaling section for more information on good practices for risk assessment.",
      "source": "UK Government2023",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0887_UK Government2023",
      "name": "Equitable Access to Frontier AI Systems",
      "description": "Transparent and fair processes for researchers to get restricted access to AI systems are important. To ensure systems are appropriately understood, particular attention could be paid to promote academic freedom and diversity of thought, for example, not withholding access based on previous or expected criticism and encouraging different types of academics, civil society groups and independent researchers to study AI systems.",
      "source": "UK Government2023",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0888_UK Government2023",
      "name": "Public Sharing of Societal Impact Assessments",
      "description": "In the absence of sufficiently substantial downsides to sharing, frontier AI organisations are encouraged to share the products of this work broadly.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0889_UK Government2023",
      "name": "Misuse Mapping",
      "description": "Understand how their own models \u2013 as well as how models released by other groups \u2013 are being misused. Knowing that some people have begun using a competitor\u2019s model to conduct phishing attacks, for example, may lead a frontier AI organisation to become more concerned that its own model will be misused in this way.",
      "source": "UK Government2023",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0890_UK Government2023",
      "name": "Reporting of Broad Patterns of Misuse",
      "description": "Report information on broad patterns of misuse. This information can help governments, the public, and other frontier AI organisations to better understand risks. Reports may focus on population-level metrics related to API usage, such as the rate of harmful inputs filtered or the number of users banned for misuse. This information may benefit from being presented in a clear and understandable way and made optimally accessible.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0891_UK Government2023",
      "name": "Appropriate Retention Schedules for Usage Logs",
      "description": "Determine appropriate retention schedules for usage logs, balancing safety and privacy considerations. In severe cases of misuse, access to logs from several months prior may be necessary to understand in maximal detail the causes of the misuse. However, in some cases extended retention schedules may disproportionately affect privacy.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0892_UK Government2023",
      "name": "Content Filters",
      "description": "Apply content filters to both model inputs and model outputs. The input filters can block harmful requests (e.g. requests for advice on building weapons) from being processed by the model. Content modifiers could adjust harmful prompts to elicit non-harmful responses. The output filters can block harmful model outputs (e.g. instructions on building weapons) from being sent back to the user.",
      "source": "UK Government2023",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0893_UK Government2023",
      "name": "Evaluation of Different Content Filters",
      "description": "Explore and compare the efficacy of multiple approaches to developing content filters. This may involve comparing available methods, such as those in the following section; selecting the most effective ones; and applying them in combination if doing so substantially increases efficacy. Best practice could be shared with other frontier AI organisations or published broadly.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0894_UK Government2023",
      "name": "Robust Content Filters",
      "description": "Invest in making content filters robust to \u201cjailbreaking\u201d attempts. Work to ensure filters are robust to jailbreaking attempts, for instance by including examples of jailbreaking attempts in the datasets used to develop filters.",
      "source": "UK Government2023",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0895_UK Government2023",
      "name": "Discretion for Development, Storage, and Sharing of Content Filters",
      "description": "Exercise appropriate caution when developing, storing, or sharing content filters \u2013 and any specialised datasets used to produce them. In some cases, components of filters or datasets used to create them can be used to train higher-risk models. For instance, a classifier that evaluates the aggressiveness of a model\u2019s outputs may be used to train the model to be more aggressive.",
      "source": "UK Government2023",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0896_UK Government2023",
      "name": "Fine-Tuning Models",
      "description": "Fine-tune models to reduce the tendency to produce harmful outputs. This may involve using reinforcement learning from human or AI-generated feedback regarding the appropriateness of different model outputs e.g. a constitutional based approach where human input to the fine-tuning process is provided by a list of principles. It may also involve fine-tuning models on curated datasets of appropriate responses to prompts. Where human feedback is used, the mental wellbeing of moderators may need to be considered.",
      "source": "UK Government2023",
      "subcategoryId": "model-alignment-2.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "reinforcement-learning",
        "foundation"
      ]
    },
    {
      "id": "A0897_UK Government2023",
      "name": "Model Prompting",
      "description": "Prompt models to avoid harmful behaviour. This may involve, for instance, using a \u201cmeta prompt\u201d to instruct a model that it should ignore requests to cause harm. Alternatively, prompt distillation or other methods can be used to fine-tune their models to behave as though they have received particular prompts.",
      "source": "UK Government2023",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0898_UK Government2023",
      "name": "Rejection Sampling",
      "description": "Consider also applying \u201crejection sampling\u201d methods to model outputs. These methods involve generating several outputs, scoring them on their harmfulness, and then only presenting the least harmful outputs to the user.",
      "source": "UK Government2023",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0899_UK Government2023",
      "name": "User-based API Access Restrictions",
      "description": "When deploying models through APIs, consider reducing access to users who display suspicious usage patterns. For instance, if a content filter blocks a user\u2019s requests several times in short succession, this is evidence that they are attempting to misuse the model or find ways to circumvent its filters. Appropriate measures may include warnings, further investigation, rate limitations, more restrictive filters, and bans. Care should be taken to avoid restricting genuine use, for example to legitimate AI safety researchers attempting to examine model behaviour, or to users struggling to access legitimate sources of help on difficult topics like self-harm.",
      "source": "UK Government2023",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0900_UK Government2023",
      "name": "Communication of and Appeal for API access Reduction Policies",
      "description": "Communicate API access reduction policies \u2013 and allow users to appeal access reductions. Users should be able to understand the reasons why their access may be reduced and be able to appeal access reductions if they believe policies have not been applied correctly. Alternatively, users may be able to perform actions to mitigate misuse risks, such as providing evidence to justify behaviour.",
      "source": "UK Government2023",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0901_UK Government2023",
      "name": "Know Your Customer (KYC) Checks",
      "description": "Implement tiered Know Your Customer (KYC) checks for API users. KYC checks can help prevent users from simply creating new accounts when their access is reduced. More intense checks, such as identify verification, could be implemented where the risk is higher, such as for models with more dangerous capabilities, access to models with fewer safeguards, or high- volume usage of the model. It is important to weigh up KYC checks against potential privacy and access tradeoffs of requiring registrations.",
      "source": "UK Government2023",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0902_UK Government2023",
      "name": "Trusted Categories API access",
      "description": "Consider restricting certain API access tiers only to users in \u201ctrusted\u201d categories. For example, it may be appropriate to offer high usage rates, fine-tuning access, permissive content filters, as well as access to models with high misuse potential only to verified users in established enterprises, non-profits, and universities.",
      "source": "UK Government2023",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0903_UK Government2023",
      "name": "Protocols for Sharing API users Information with government authorities",
      "description": "Establish protocols for deciding when and how to share information about API users with relevant government authorities. These protocols may include covering circumstances under which information about a user is proactively shared with government bodies (e.g. cases where there is reason to think a user may be attempting to cause grave harm), and how to respond to government requests for data.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0904_UK Government2023",
      "name": "Worst-case scenario Response",
      "description": "Implement processes and technical requirements to enable rapid rollback or withdrawal of models in case of egregious, widespread or consistent harms. Rolling back to a previous version of a model that does not suffer from the same misuse threats, or withdrawing access to a model altogether, may be proportionate actions in such situations of extreme threat or consistent misuse. Running periodic dry runs could increase preparedness for such situations.",
      "source": "UK Government2023",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0905_UK Government2023",
      "name": "User Informed Rapid Rollback of models",
      "description": "Inform end users that rapid rollback or withdrawal of models may be necessary, and that the disruption to its end users will be minimised as far as safely possible. This is especially important where models are deployed in safety critical domains.",
      "source": "UK Government2023",
      "subcategoryId": "user-rights-4.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0906_UK Government2023",
      "name": "Governance Responses to Worst-case of Misuse Scenarios",
      "description": "Establish governance processes to ensure internal clarity in response to worst-case or consistent misuse scenarios. Such processes could draw on similar accountability and governance mechanisms used within a Responsible Capability Scaling policy. It may be valuable to clarify the approvals needed to roll back a model to a previous version or withdraw a model, the types of misuse that would warrant such actions, and the timeframe under which such actions would occur.",
      "source": "UK Government2023",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0907_UK Government2023",
      "name": "Monitoring and Safeguard Efficacy",
      "description": "Regularly assess monitoring and safeguard efficacy and continually invest in improvements. Regular assessment of monitoring efficacy can build an understanding of the success rate and speed of issue detection, on which deployment decisions may be based. These assessments may draw, for instance, on the results of internal and external red-teaming efforts, random audits of usage logs, what may be \u2018best practice\u2019, as well as available information about real-world misuse. Risks are expected to increase as AI capabilities increase. Hence, active increases in investment in safety, security, and monitoring may be valuable.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0908_UK Government2023",
      "name": "Exploration of New Safeguards",
      "description": "Explore new safeguards and countermeasures in response to patterns of misuse, recognising that appropriate measures may vary based on model type, usage patterns, and our technical understanding of model capabilities.",
      "source": "UK Government2023",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0909_UK Government2023",
      "name": "Diverse Monitoring Techniques",
      "description": "Employ diverse monitoring techniques to balance comprehensiveness, scalability, and privacy. A strong monitoring setup will generally combine automated and human review, in recognition that automated reviews may miss intricate issues, while human reviews are not always scalable and can pose privacy issues.",
      "source": "UK Government2023",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0910_UK Government2023",
      "name": "Assessment of Safeguard Costs for Users",
      "description": "Regularly assess the costs of safeguards to users, including bias and loss of privacy, as well as restrictions on users\u2019 freedom to discuss sensitive topics with AI agents in appropriate contexts. This may involve, for instance, estimating the false positive rates of filters by sampling blocked inputs and outputs; comparing the capabilities, efficiency, and user ratings of models that lack certain safeguards against models that have the safeguards in place; and conducting user interviews to understand how concerned informed users are about losses in privacy and the emergence of biases in algorithmically filtered content.",
      "source": "UK Government2023",
      "subcategoryId": "societal-impact-1.7",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "predictive",
        "classification",
        "recommendation",
        "computer-vision",
        "supervised-ml"
      ]
    },
    {
      "id": "A0911_UK Government2023",
      "name": "Reduction of Costs for Users",
      "description": "Explore strategies for reducing the costs of safeguards to users, such as \u201cstructured transparency\u201d methods to reduce the cost of usage monitoring to user privacy, or tiered access to models with different safeguards for differently qualified users.",
      "source": "UK Government2023",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0912_UK Government2023",
      "name": "Accounting of Applicable Regulatory Frameworks",
      "description": "Before collecting training data, take account of applicable regulatory frameworks. This may involve, for example, establishing a legal basis to process training data and understanding any copyright considerations that might apply. This could help to mitigate risks further down the line, such as the system revealing personally identifiable information.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0913_UK Government2023",
      "name": "Principle of Data Minimisation",
      "description": "Data minimisation can reduce the risk of harmful content making it into training data. Frontier AI organisations could explore the practice of \u201cdata pruning\u201d, which has been shown to improve data quality and system performance while minimising the quantity of pre-training data required.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0914_UK Government2023",
      "name": "Datatset Auditing",
      "description": "Audit datasets used for pre-training but also those used for fine-tuning, classifiers, and other tools. Inappropriate datasets could result in systems that fail to disobey harmful instructions.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0915_UK Government2023",
      "name": "Technical Tools for Auditing",
      "description": "Use technical tools \u2013 such as classifiers and filters \u2013 to audit large datasets to support scalability and privacy. These could be used in combination with human oversight, which can verify and augment these assessments.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0916_UK Government2023",
      "name": "Assessment of Training Data",
      "description": "Assess the overall composition of training data. This could include the data sources, the provenance of the data, indicators of data quality and integrity, and measures of bias and representativeness. The amount and variety of data are simple, reliable predictors of risk, and provide an additional line of defence where more targeted assessments are limited.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0917_UK Government2023",
      "name": "Audits for Dangerous System Capabilities",
      "description": "Information that might enhance dangerous system capabilities, such as information about weapons manufacturing or terrorism.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0918_UK Government2023",
      "name": "Audits for Private or Sensitive Information",
      "description": "AI systems may be subject to data extraction attacks, where determined users can prompt systems to reveal pieces of training data, or may even reveal this information accidentally. This makes it important to know whether datasets include private or sensitive information e.g. names, addresses, or security vulnerabilities.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0919_UK Government2023",
      "name": "Audits for Biases in the Data",
      "description": "Training data that is imbalanced or inaccurate can result in an AI system being less accurate for people with certain personal characteristics or providing a skewed picture of particular groups. Ensuring a better balance in the training data could help to address this.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0920_UK Government2023",
      "name": "Audits for Harmful Content",
      "description": "Harmful content, such as child sexual abuse materials, hate speech, or online abuse. Having a better understanding of harmful content in datasets can inform safety measures (e.g. by highlighting domains where additional safeguards like content filters should be applied).",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0921_UK Government2023",
      "name": "Audits for Misinformation",
      "description": "Training an AI system on inaccurate information increases the likelihood the outputs of the system will be inaccurate and could lead to harm.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0922_UK Government2023",
      "name": "External Expertise for Input Data Audits",
      "description": "Draw on external expertise in conducting input data audits. For example, biosecurity experts could be consulted to identify information relevant to biological weapons manufacturing, which may not be readily obvious to non-experts.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0923_UK Government2023",
      "name": "Data Audits for AI System Behavior",
      "description": "Use data audits to improve understanding of how training data affects AI system behaviour. For example, if model evaluations reveal a potentially dangerous capability, data audits can help ascertain the extent to which the training data contributed to it.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0924_UK Government2023",
      "name": "Data Audits for AI System Behavior",
      "description": "Use data audits to improve understanding of how training data affects AI system behaviour. For example, if model evaluations reveal a potentially dangerous capability, data audits can help ascertain the extent to which the training data contributed to it.",
      "source": "UK Government2023",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0925_UK Government2023",
      "name": "Audits on Datasets for Customer Fine-Tuning",
      "description": "Conduct audits on datasets used by their customers to fine-tune AI systems. Customers are often allowed to fine-tune systems on their own datasets. By carrying out audits to ensure that customers are not encouraging undesirable behaviours, frontier AI organisations can use their expertise and insight into the AI system\u2019s original training data to identify potential harms upstream. It is important that frontier AI organisations are mindful of privacy concerns and make use of privacy preserving techniques, where appropriate.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0926_UK Government2023",
      "name": "Documentation of Results of Input Data Audits",
      "description": "Document the results of input data audits, including metadata. Frontier AI organisations could look to emerging standards when documenting the results of input data audits, such as datasheets for datasets",
      "source": "UK Government2023",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0927_UK Government2023",
      "name": "Input Data Audits for Pre-Development/deployment Risk Assessments",
      "description": "Use input data audits to inform pre-development and pre-deployment risk assessments. Data audits may be particularly valuable in pre-development risk assessments, where direct evidence of system behaviour will not be available. Input data audits can also be used to improve context-based understanding of how data impacts system capabilities, which will strengthen future risk assessments and mitigation techniques.",
      "source": "UK Government2023",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0928_UK Government2023",
      "name": "Removal of Harmful Input Data",
      "description": "Remove potentially harmful or undesirable input data, where appropriate. Given the increasingly strong generalisation abilities of AI systems, data curation may prove insufficient to prevent dangerous system behaviour but could provide an additional layer of defence alongside other measures such as fine-tuning and content filters.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0929_UK Government2023",
      "name": "Harmful Data for Reduction of Dangerous Capabilities",
      "description": "For some kinds of risks, explore options to use harmful data to reduce AI systems\u2019 dangerous capabilities or to help develop mitigation tools. For example, by fine-tuning a system using labelled harmful content to refuse requests related to the harmful information.",
      "source": "UK Government2023",
      "subcategoryId": "model-alignment-2.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "reinforcement-learning",
        "foundation"
      ]
    },
    {
      "id": "A0930_UK Government2023",
      "name": "Sourcing or Generation of Additional Data for Training Dataset",
      "description": "Consider sourcing or generating additional data and adding it to the training dataset, where it is determined that data is missing or inadequate. Improving the representativeness of training data can improve performance and reduce potential negative societal impacts and discriminatory effects. However, additional data should only be sought through appropriate means that respect and empower those individuals who are missing from the data.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0931_UK Government2023",
      "name": "Limitations of Input Data Audit",
      "description": "Acknowledge the limitations of input data audit. Techniques for understanding the impacts of data and filtering out specific data are limited (e.g. excluding information from a dataset does not always prevent the AI system from reasoning about or discovering that information). Other risk mitigation measures will be required and further research on improving data input audit techniques is important.",
      "source": "UK Government2023",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0932_UK Government2023",
      "name": "Independent Data Input Audits",
      "description": "Facilitate independent data input audits from external parties. Since training data constitutes sensitive intellectual property, it is important to implement appropriate technical and organisational safeguards to ensure privacy and security when sharing the training data. In order to protect sensitive information, frontier AI organisations could explore the possibility of providing synthetic datasets to auditors with sensitive information removed, or providing auditors with privacy-preserving access to the training data.",
      "source": "UK Government2023",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0933_UK Government2023",
      "name": "Disclosure of Input Data Audits with External Stakeholders",
      "description": "Share information about input data audits with users and external stakeholders. Some information could be included in transparency reports, such as high-level information about training data (including data sources), data auditing procedures, and measures taken to reduce risk (see Model Reporting and Information Sharing). More sensitive information may be shared directly with regulators and external auditors.",
      "source": "UK Government2023",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0934_Gipi\u0161kis2024",
      "name": "Use of Synthetic Data",
      "description": "Synthetic data refers to data that is not collected from the real world. It is used to train AI models as an alternative to, or augmentation of, natural data. Effective use and generation of synthetic data allows for more oversight by the trainer on the training dataset because they have more control over its statistical properties. Synthetic data can help against dataset bias by having more samples from a particular distribution or minority group. It can also help in privacy by having more samples to mask sensitive data",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0935_Gipi\u0161kis2024",
      "name": "Restrict web access during AI training",
      "description": "Developers can restrict or disable AI systems\u2019 internet access during training. For example, developers can restrict web access to read-only (e.g., by disabling write-access through HTTP POST requests and access to web forms) or limit the access of the AI system to a local network",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0936_Gipi\u0161kis2024",
      "name": "Adversarial training",
      "description": "Adversarial training [83] is a technique for training AI models in which adver- sarial inputs are generated for a model, and the model is then trained to give the correct outputs for those adversarial inputs. Adversarial training can involve adversarial examples generated by human experts, human users, or other AI systems.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0938_Gipi\u0161kis2024",
      "name": "Calibrated confidence measures for model pre- dictions",
      "description": "Incorporating calibrated confidence measures alongside a model\u2019s predictions and standard performance metrics, such as accuracy, can help users identify in- stances of overconfidence in incorrect predictions or underconfidence in correct ones [85]. These additional measures can provide users with more information to better interpret the model\u2019s decisions and assess whether its predictions can be trusted.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "documentation-4.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0939_Gipi\u0161kis2024",
      "name": "Incorporating the estimation of atypical input samples or classes for better model reliability",
      "description": "Incorporating the estimation of rare atypical input samples or classes might improve a model\u2019s reliability, both with respect to its predictions and confidence calibration. Model predictions for rare inputs and classes may have a tendency of being overconfident and have worse accuracy scores [232]. For LLMs, the negative log-likelihood can be used as an atypicality measure. For discriminative models, Gaussian Mixture Models can be employed to estimate conditional and marginal distributions, which are then used in atypicality measurement.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0940_Gipi\u0161kis2024",
      "name": "Data cleaning",
      "description": "Providers can filter out the training dataset via multiple layered techniques, ranging from rule-based filters to anomaly detection via data point influence or statistical anomalies of individual data points [213]. For example, a data cleaning procedure can involve the use of filename checkers to detect duplicates or wrongly formatted data, which then moves to flagging the most influential data samples from the dataset via influence functions for anomaly detection.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0941_Gipi\u0161kis2024",
      "name": "Internal data poisoning diagnosis",
      "description": "Providers can have an internal framework to identify what specific data poison- ing attack their model may be a victim of based on a set of symptoms, such as analysis of target algorithm and architecture, perturbation scope and dimen- sion, victim model, and data type [39]. This framework includes known defenses against the diagnosed attack, which providers can then apply to the model.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0942_Gipi\u0161kis2024",
      "name": "Scope red-teaming activities based on deploy- ment context",
      "description": "Red-teaming activities can be tailored and compared based on the specific deployment circumstances of an AI system. This involves adapting the scope, depth, and focus of red-teaming efforts to match the intended use case, potential risks, and operational context of the AI system. Points of consideration include: \u2022 The diversity of potential users and use cases \u2022 The sensitivity and impact of the application domain \u2022 The scale of deployment and potential reach \u2022 Known vulnerabilities or concerns specific to the model or similar systems",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0943_Gipi\u0161kis2024",
      "name": "Red teaming to test the resilience of open-weights models to fine-tuning",
      "description": "Before the release of open-weights models, red teamers can test the resilience of safety training against fine-tuning. Safety training may be partially or fully overridden by fine-tuning intentionally (e.g., by malicious actors) or uninten- tionally (e.g., by fine tuning an AI model for a specific use case)",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0944_Gipi\u0161kis2024",
      "name": "Pre-deployment access by third-party auditors",
      "description": "Prior to full deployment of general-purpose AI models, a group of third-party auditors who are not selected by the GPAI model provider could get early access to AI models in order to evaluate them from a variety of different perspectives and with diverse interests [30, 157]. This prevents cases where the developers of AI models select auditors that are especially favorable to the developers, which could result in biased or incomplete evaluations, or contribute to an unjustified public perception of the capabilities and risks of the model.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0945_Gipi\u0161kis2024",
      "name": "Audits with specific scoped goals",
      "description": "Audits of AI systems will be easier to perform, and have clearer results if the scope and goals of the evaluation are formulated as precisely as possible [157], or have a connection to concrete existing policy.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0946_Gipi\u0161kis2024",
      "name": "Evaluating explainability method sensitivity to data inputs and model parameters",
      "description": "Model parameter and data randomization tests can be employed as sanity checks [2] to determine the relationship between a model, its inputs, and the explainability method. If an explanation is independent from or insensitive to the underlying model or the input data, it would indicate a lack of reliability of the explainability method.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0947_Gipi\u0161kis2024",
      "name": "Enforcing model output interpretability post-training",
      "description": "While a resulting trained model may be opaque with respect to its predictions, the final output in a system that involves the model can nonetheless be com- pletely interpretable. For example, a neuro-symbolic system for robot navigation can use a language model to generate potential navigation plans, then have a deterministic solver simulate executing valid plans [128]. The optimal plan found is executed in the real world and is interpretable.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0948_Gipi\u0161kis2024",
      "name": "Paraphrasing to reduce hidden information",
      "description": "Paraphrasing can mitigate steganography and encoded reasoning by reducing the physical size of the hidden information encoded in text [166].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0949_Gipi\u0161kis2024",
      "name": "Testing for erroneous or irrelevant features through concept learning",
      "description": "Interpretability techniques, particularly concept learning [77], can be used to test whether a model is learning erroneous features or relying on irrelevant features in its predictions. This can help identify and mitigate potential risks associated with incorrect or non-informative features influencing the model\u2019s outputs.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0950_Gipi\u0161kis2024",
      "name": "Dashboard of model properties",
      "description": "A dashboard [41] displays all the relevant information about the model\u2019s internal state and the model\u2019s physical properties to the user. It is used to ensure that the user is informed about factors that influence the behavior of the model and to ensure that the user maintains control over the model. Allowing only the user to access the dashboard can aid in information asymmetry between the user and model, thus supporting the user oversight over the model. Examples of the model\u2019s internal state include its representations of the world, representation of users, and the strategies it is currently pursuing; while examples of the model\u2019s physical properties include the model\u2019s compute and energy consumption, physical storage occupied, and physical networks it is connected to.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0951_Gipi\u0161kis2024",
      "name": "Modification of model internal representation",
      "description": "Model providers can modify the internal representation of the model [135, 237] up to the granular level and in consultation with other tools that aid in under- standing what the internal representation of the model is. For example, if a model that is meant to give factual health data learned an incorrect fact, the model provider can use Linear Artificial Tomography (LAT) [237] to identify the representations responsible for the incorrect fact, and then modify that representation via modifying individual weights, or modify the en- tire representation itself by modifying entire layers.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0952_Gipi\u0161kis2024",
      "name": "Incorporating goal uncertainty into AI systems to mitigate risky behaviors",
      "description": "Incorporating uncertainty into AI system goals can prevent rushed decision- making and incentivize such systems to gather additional information or to refer to human oversight when faced with ambiguity [228].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0953_Gipi\u0161kis2024",
      "name": "Evaluations for truthful outputs",
      "description": "AI systems can be evaluated for truthfulness when answering questions, includ- ing in contexts where humans tend to give incorrect but widely-accepted answers (i.e., popular misconceptions). Evaluations detect incorrect facts learned about the world, inadequate capabilities of the AI system, and misleading outputs by the AI system [121].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0954_Gipi\u0161kis2024",
      "name": "Interpretability techniques that target deception",
      "description": "Interpretability techniques can be used for finding the root causes of outputs of an AI model that reliably lead to false beliefs for its users (e.g., deceptive behavior) [180]. It is often difficult to distinguish a deceptive AI model from an honest AI model, since absence of deception and very sophisticated (hard to detect) deception may appear behaviorally similar. Interpretability techniques and tools can be used to detect whether AI model outputs arise from internal computations representing deception. This can apply in cases of purposely trained deception by the developer or if it emerges unintentionally during training. These interpretability tools can come from mechanistic interpretability, such as identification of features involved in generating the outputs, or attribution of parts of the input most important in generating the output [129].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0955_Gipi\u0161kis2024",
      "name": "Benchmarks for Situational Awareness",
      "description": "A model is situationally aware if it internally represents that it is a machine learning model and if it can accurately infer or act on model-relevant facts - e.g., if it is currently in training, testing, evaluation or deployment, or the desired outcome of an evaluation. Some benchmarks exist for situational awareness of AI models, which test whether the AI models can classify stereotypical inputs from training, test- ing, evaluation and deployment as such, and whether the AI model can use this information correctly to take actions in the world [25, 111].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0956_Gipi\u0161kis2024",
      "name": "Evaluating AI systems\u2019 performance on self-proliferation-related tasks",
      "description": "To prevent AI systems from self-proliferating, developers of AI systems can evaluate those systems for their capability to engage in self-proliferation-related tasks. This type of evaluation might assess an AI system\u2019s ability to replicate its com- ponents (including model weights, structural scaffolding, etc.) onto other local or cloud infrastructures prior to deployment. Additionally, it may test the system\u2019s capacity to purchase cloud credits and configure virtual machines on a cloud platform. The evaluation could offer a predefined environment (such as a virtual container) to facilitate self-replication, providing access to the system\u2019s own components, a network connection to a resource-equipped external computer, and other necessary resources [12, 106]. Self-proliferation evaluations can be conducted in a secure environment to pre- vent a self-replicating AI system from affecting other computers.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0957_Gipi\u0161kis2024",
      "name": "Demonstrating a \u201cmargin of safety\u201d for the worst plausible system failures",
      "description": "Model developers can demonstrate that there is an acceptable \u201cmargin of safety\u201d between the current version of the model and a plausible version with dangerous capabilities or potential system failures, whether these arise from the model itself or through scaffolding. This \u201cmargin of safety\u201d can be tracked and evaluated based on the model\u2019s performance on either component tasks or proxy tasks with varying levels of difficulty [44], and it is particularly relevant for general- purpose models with emergent properties, where some of the risks, use cases, and model capabilities may be unknown even at the time of deployment. Margin of safety (also called \u201csafety factor\u201d) is a common practice in many in- dustries - particularly in physical structures. It is common for this margin to be very conservative when feasible (e.g., 4+ in fasteners on critical structures). In situations where a high margin of safety is impractical, it may be supplemented by more frequent inspections and additional process controls. A lower safety factor can also be managed by adopting conservative assumptions regarding worst-case conditions.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0958_Gipi\u0161kis2024",
      "name": "Employing qualitative assessments in difficult- to-measure domains",
      "description": "Qualitative evaluation can be used in cases when quantitative measurement is not feasible. This can give additional insights about the system which would not be available if no measurement was performed due to its difficulty [206].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0959_Gipi\u0161kis2024",
      "name": "Scenario analysis",
      "description": "Scenario analysis involves development of several plausible future scenarios, where these scenarios may be generated from varying the assumptions of a small set of driving forces. The scenarios developed can be used to take further actions to improve overall preparedness [107]. For example, scenarios to explore frontier AI risks can be developed, where they can involve different assumptions on factors such as AI capability, ownership, safety, usage, and geopolitical contexts, as well as its implications on key policy issues [203].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0960_Gipi\u0161kis2024",
      "name": "Fishbone diagram",
      "description": "The fishbone diagram, or a \u201ccause-and-effect diagram\u201d [107], can be used to show the potential causes of an undesirable event. The diagram is created by first placing a specific risk event at the \u201chead\u201d of the diagram, typically facing the right. Then, to the left of the risk event, the \u201cribs\u201d branch off from the \u201cbackbone\u201d to represent major causes, which further branch into sub-branches to represent root-causes, extending to as many levels as required. This is typically done via backward-reasoning, where various potential causes are explored after the risk event has been selected for analysis using this method. For example, the risk event \u201cAI systems generate toxic content\u201d can be placed at the \u201chead\u201d of the diagram, where the branches may include causal factors like \u201cAI trained on data containing toxicity\u201d and \u201csuccessful jailbreaking of AI despite fine-tuning.\u201d",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0961_Gipi\u0161kis2024",
      "name": "Causal mapping",
      "description": "Causal mapping is a technique used to explore and map complex interactions between cause and effect of risks. It involves coming up with potential events related to an undesirable issue, with each event represented by a text box, then clustering similar events according to themes, and finally drawing arrows to illustrate the causal relationship between the different events. The completed causal map can then be analyzed to identify central events, clusters of events, feedback loops, and other relevant patterns [107]. For example, causal mapping can be used to explore factors that lead to high- level model capabilities (e.g., \u201cmachine intelligence\u201d), and the nodes may in- clude factors such as \u201cconcept formation\u201d and \u201cflexible memory,\u201d where certain nodes may be found to be especially crucial if they have more outgoing arrows connecting them to other nodes.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0962_Gipi\u0161kis2024",
      "name": "Delphi technique",
      "description": "The Delphi technique is a multi-round forecasting process based on a structured framework on collecting and collating multiple expert judgments. It brings the benefits of anonymous and remote participation which may result in increased likelihood estimation accuracy compared to merely averaging individual estima- tions or simple group discussions [107].Given a panel of experts, at each round, the experts are presented with an ag- gregated summary of the results from the previous round, and are then allowed to update their answers accordingly. The process ends when either a consen- sus is reached or the responses in later rounds no longer change significantly. This method enables elicitation of expert judgment while utilizing wisdom of the crowd in the process. A potential application of the Delphi technique is to solicit expert judgment on the likelihood of systemic risks from AI development, where crucial variables identified during each round of questionnaire can be further studied for the purpose of risk mitigation.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0963_Gipi\u0161kis2024",
      "name": "Cross-impact analysis",
      "description": "Cross impact analysis is a forecasting methodology that analyzes the likelihood of a particular issue using expert analysis (i.e., Delphi technique) in combination with analysis of events correlated with the said issue. It involves decomposing an issue into discrete and correlated events, and then collecting expert opinion on each of those events. Analysis of each event from multiple viewpoints can yield potential future scenarios [107]. For example, an issue may be \u201cadvances in AI,\u201d which can be broken down into two correlated events like \u201cadvances in hardware\u201d or \u201cadvances in algorithms,\u201d where the likelihood of occurrences of each event can be estimated via the Delphi technique while taking into account their interactions with other events.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0964_Gipi\u0161kis2024",
      "name": "Bow-tie analysis",
      "description": "Bow-tie analysis is a method to assess the utility of implemented controls against a particular risk event. It involves centering the unwanted risk event within a diagram. On the left, the factors that can cause the event are listed, followed by the controls that will prevent or minimize the likelihood of the event. On the right, the event is assumed to happen, and the potential effects and the relevant post-hoc controls that could minimize their impact are listed [107]. For example, given a hazard where an AI model has the capability of generating potentially harmful outputs, a risk event may be an AI providing information on how to create dangerous bioweapons. The risk factors could include the use of dangerous data for training as well as a lack of fine-tuning prior to deploy- ment. The risk effects may include creation and use of bioweapons using the AI generated information. Once these risk factors and risk effects are in place, both preventive controls and post-hoc controls can be planned, such as appropriate filtering of training data and rigorous red teaming prior to model deployment as preventive barriers, as well as know-your-customer policies and model output censoring techniques as reactive barriers.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0965_Gipi\u0161kis2024",
      "name": "System-theoretic process analysis (STPA)",
      "description": "STPA is a method to assess the utility of implemented controls against a partic- ular risk within a complex system. Unlike bow-tie analysis, STPA factors in the interactions between components as events that can cause the risk in question [107]. First, the system and its boundaries to the environment are defined. The sys- tem is primarily delineated from the environment because there is at least some partial control over it. Second, several items are enumerated, including (i) un- wanted risk events (\u201closses\u201d), (ii) system states that cause losses (\u201csystem-level hazards\u201d), and (iii) system states that do not cause losses (\u201csystem-level con- straints\u201d). Third, a diagram mapping the system, environment, their different controls, and the interactions between these elements is created. This diagram must be comprehensive in listing the different losses and possible interactions that can cause each loss. Finally, the diagram can be used to identify \u201cunsafe control actions\u201d (UCAs), which are the causal pathways between a control and system-level hazards, including all interactions involved. For example, in the context of text-to-image models, losses may include \u2018loss of diversity\u2019 and \u2018loss of quality\u2019; hazards may include \u2018low quality text-image pairs within training dataset\u2019 and \u2018harmful content within training dataset\u2019; and the controls may include human controllers and automated controllers (e.g., annotators, data owners, data crawlers) [165]. Subsequent analysis may result in identification of UCAs such as \u2018current data filtering actions\u2019 and neglecting current filter thresholds\u2019 which are linked to specific hazards. Specific actions that counter or prevent such UCAs can reduce the associated losses.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0966_Gipi\u0161kis2024",
      "name": "Risk matrices",
      "description": "A risk matrix is a method for risk evaluation. It is a heatmap that, for each cell, shows the severity score weighted by the likelihood score of a particular risk, usually from a scale of 1-5. Two rankings are required to construct a risk matrix: a ranking for the severity of risks, and a corresponding ranking of the likelihood of risks [107]. AI-related risks can be generated using appropriate taxonomies, and placed into the relevant cells according to their assessed likelihood and severity based on predefined criteria (e.g., likelihood level 1 corresponds to < 1% chance, and likelihood level 5 corresponds to > 90% chance; while severity level 1 corresponds to mild inconveniences to the user, and severity level 5 corresponds to a fatality or financial damage upwards of $10 million, etc.), such that particular focus can be given to mitigating risks with higher weighted scores (i.e., likelihood multiplied by severity).",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0967_Gipi\u0161kis2024",
      "name": "Pre-allocate sufficient resources for risk man- agement",
      "description": "The process of conducting thorough risk management is potentially time-con- suming. Pre-allocating sufficient resources, in terms of personnel count and schedule allowances, to conduct necessary risk management activities prior to model deployment is crucial [6]. For example, a red-teaming exercise requires creative approaches to identify weaknesses of the AI system against potential adversaries, which alone may require hundreds of hours from several experts.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0968_Gipi\u0161kis2024",
      "name": "Staged release of model weights",
      "description": "When a model is developed by a provider for use in a certain AI system, it may also be useful to release the model itself more widely. Such developers can follow a staged release approach, in which they first grant access to the model via an API to trusted partners or the public, in order to scope the models\u2019 capabilities and detect harmful or dangerous features [195]. After a period of an initial closed release and potentially further safety-training, the developers of the AI model can then release the weights, if they are confident that the AI model poses minimal systemic risk.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "staged-deployment-3.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0969_Gipi\u0161kis2024",
      "name": "Gradual or incremental monitored release of model access",
      "description": "AI systems can be released for access incrementally, starting with a small and selected deployer base before progressively being released to a wider user base. Initially, usage to a hosted API can be restricted with access given to specific deployers only, where all instances of the system can be easily updated or de- commissioned with minimal disruption should there be any problems identified. Gradual releases provide more time to monitor for vulnerabilities and other problems. Even when such vulnerabilities are detected, the resulting harms may be more limited compared to a scenario in which a more capable version is released with the same vulnerabilities [195].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "staged-deployment-3.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0970_Gipi\u0161kis2024",
      "name": "Limit deployment scope",
      "description": "AI models can be restricted in terms of its use cases, where providers can require the deployers to limit its deployment to a predefined scope [81], such that models built for specific purposes and tested under specific environments are not used in environments or for purposes that are potentially unsafe.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0971_Gipi\u0161kis2024",
      "name": "Restricted usage terms for open-source models",
      "description": "Developers of open-weights and open-source AI models can vet and restrict the users of their AI systems by requiring them to sign a Terms of Service agreement before getting access to the model weights. Such agreements can include limitations to the usage, modification, and proliferation of the AI model [88]. Such agreements have the advantage that users only need to be vetted once before getting model access, but are often limited in practice in preventing unauthorised use or distribution.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0972_Gipi\u0161kis2024",
      "name": "Release strategy disagreement between developers",
      "description": "Developers of restricted-access models with similar capabilities may disagree about the strategy or precautions to take for model release, especially in the case of competitive pressure or minimal safety regulation oversight. In such a case, if only a single developer releases an equally capable model unrestricted, malicious actors can use it instead of restricted-access alternatives [88].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0973_Gipi\u0161kis2024",
      "name": "Pop-up interventions in LLMs",
      "description": "Supplementary information can be shown to the user in specific query topics where factual accuracy is critical. This intervention can effectively divert users from potential inaccuracies generated by AI models in sensitive contexts. For example, during electoral processes, where model hallucinations can be particu- larly costly or have a negative impact on society, LLMs can offer their users the option of being redirected to accurate and up-to-date information sources [11]. Since pop-up interventions can be intrusive to workflows, they are best used in situations where the benefits of the information outweigh the cost of distraction.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0974_Gipi\u0161kis2024",
      "name": "AI identification",
      "description": "AI identifiers can be used to indicate that an AI is involved in a process or an interaction [40]. For AI systems that interact directly with users, a visible output may be used, e.g., a displayed text message saying \u201cI am an AI language model\u201d, accompanied with the appropriate warnings and caveats relevant to the user, [40]. Whereas, for AI systems that interact with other systems or applications, other forms of watermark or unique identifiers can be used. In either situation, agent cards can serve as an identifier, where further details about the underlying AI system, the specific instance of the AI agent, and other information relevant to the development of the agent, can be included.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0975_Gipi\u0161kis2024",
      "name": "AI output watermaking",
      "description": "Output produced by or with AI assistance can be marked to clearly identify its origin. Verification of the watermark can involve the use of statistical tests or having the mark immediately visible to a human inspector. Ideally, the watermark does not significantly alter the utility of the output, and is robust against digital and physical manipulation that results in data degradation [216, 145].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0976_Gipi\u0161kis2024",
      "name": "AI output metadata",
      "description": "Output produced or whose production is aided by AI can contain metadata to record its origin and the transformations it has undergone. Metadata is evidence that subsequent versions or its derivatives come from this original version. The metadata can include the original AI model source, along with ownership, and its subsequent edits [169]. For example, an image produced by an AI model can contain metadata showing the date of creation and the AI model that produced it. Subsequent versions can reference this information and, if they are intermediary versions, can include descriptions of any editing that has taken place.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "content-safety-2.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "generative-non-llm",
        "multi-modal",
        "foundation"
      ]
    },
    {
      "id": "A0977_Gipi\u0161kis2024",
      "name": "Monitoring of model capabilities",
      "description": "AI models are often trained to develop specific capabilities by using appropriate training data and training goals. However, models may develop capabilities that they were not specifically trained for. One subset of this is emergent capabilities, i.e., capabilities that emerged in larger models but not smaller models given a similar training process [215]. These capabilities can be monitored, allowing models to be tested not only for their intended capabilities but also for capabilities that are not intended.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0978_Gipi\u0161kis2024",
      "name": "AI model-assisted oversight of AI systems",
      "description": "AI model-assisted oversight can help monitor and supervise the training of increasingly capable GPAI systems, which may become difficult to oversee at scale by human supervisors during training or testing. Monitoring and supervision may become especially difficult in cases where increasingly advanced GPAIs perform near or above human level in some specialized domains, where supervision quality might fail to keep pace with capabilities improvement. The training signal may include labeled data, reward function, and user feedback on produced outputs. Currently, there are two broad approaches to provide scalable training signals to such systems: 1. Scalable oversight: Improving of the supervisor\u2019s capabilities to supervise, such that they can provide accurate training signals quickly and at scale [31]. For example, a debate format can be used between two GPAI systems (two instances of the same GPAI, or similarly capable systems). A Human supervisor judges the debate, making it easier to assess correct responses in domains which might otherwise require significant time investment of domain specific expertise [102]. 2. Weak-to-strong generalization: Enhance the training signals while ensur- ing that the enhanced signal remains faithful to the intentions of the orig- inal human-provided signal [37]. For example, a hierarchical (\u201cbootstrapping\u201d) oversight approach can be implemented: A series of GPAI models with increasing capabilities are used, where each model in the hierarchy provides oversight for the next more capable model. The least capable model at the base of the hierarchy is the only one directly overseen by human supervisors, as it is easier to oversee than the more capable models.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "model-alignment-2.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "reinforcement-learning",
        "foundation"
      ]
    },
    {
      "id": "A0979_Gipi\u0161kis2024",
      "name": "Post-deployment ongoing monitoring for unexpected use cases",
      "description": "Ongoing monitoring of AI systems can uncover emergent or newly identified capabilities and limitations, in particular when new use cases are found, or in a large-scale deployment with a diverse population of users. These new capa- bilities or limitations can inform ongoing risk analysis. New use-cases can be discovered via monitoring publications, online forums, or APIs [131]. For example, a LLM might have unexpectedly high competence at giving con- vincing medical advice, despite not being directly developed for that purpose, nor verified for accuracy. In that case, the potential risks of this newly found competence can be assessed.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0980_Gipi\u0161kis2024",
      "name": "Monitoring protocols for po- tentially adversarial GPAI systems",
      "description": "Similar to cybersecurity protocols, AI providers can prepare in advance for po- tential adversarial action from AI systems by developing and stress-testing protocols for potential adversarial action [179]. Such adversarial action may range from hiding code exploits, to strategic deception or loss of control related be- haviors. AI systems may be adversarial due to malicious actors (e.g., data poisoning), or due to unintended issues of control (lack of alignment with human intent). Alongside improving the system alignment, external monitoring and restrictions may be deployed in advance to \u201ccatch\u201d these adversarial model malfunctions before harm is done. Like with cybersecurity red teaming, any protocol which assumes adversarial action can be stress tested in advance. For example, this can be done by pro- jecting the plausible capabilities and affordances of the adversarial actor (e.g., in the case of a tech company - a hacker, or a disgruntled employee). From this process, measures that better anticipate possible adversarial action can be deployed, and catch unsuccessful attempts at adversarial behavior. If such unsuccessful attempts are caught, they can be reported in incident re- ports, and serve as an input to further risk analysis. Depending on their severity, it may be advisable to recall the system if deployed. Examples of these protocols include [84]: \u2022 Flagging of suspicious frontier AI system code generation for human over- sight, with the help of better proven (\u201ctrusted\u201d) AI systems or automated keyword detection. \u2022 Using a better proven (\u201ctrusted\u201d) AI system to edit the code produced by a frontier AI system if suspicious.\u2022 Monitoring of AI system with another instance of this same system, with filters to prevent collusion. \u2022 Restriction of access of the AI system to private information.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0981_Gipi\u0161kis2024",
      "name": "Encouraging downstream provider to evaluate models for deployment-specific failure modes",
      "description": "In some cases, AI system deployers are better positioned to perform certain risk management measures on the AI model in a provided AI system, relative to upstream model providers. For example, they understand their use case better and are more easily able to predict foreseeable misuse or failure modes. These evaluations can inform upstream model providers, or inform supplementary mitigations by the deployer.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0982_Gipi\u0161kis2024",
      "name": "Encourage reporting of critical vulnerabilities to the upstream provider or other relevant stakeholders",
      "description": "Downstream AI system deployers can report critical vulnerabilities or incidents to the upstream model provider and other relevant regulators. This can con- tribute to safe use, and allow other downstream deployers to be informed about any potential problems.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0983_Gipi\u0161kis2024",
      "name": "Least Privilege access",
      "description": "Deployers of an AI system can restrict its permissions to a whitelisted set of pre- determined options, such that all options not on the whitelist are not accessible to the AI [200, 146]. The entries on the whitelist can be chosen to be as small as possible for the AI system to fulfill its intended purpose, to reduce the attack surface of external attackers, and to decrease the probability that the AI system accidentally takes actions with large unintended side-effects. For example, such whitelisting can apply to network connections, execution of other programs, access to knowledge bases, and the action space of the AI system. It can be implemented through running the AI system on an OS-level virtualization, on networks behind a firewall, and in extreme cases on air-gapped machines.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0984_Gipi\u0161kis2024",
      "name": "Protect proprietary or unreleased AI model ar- chitecture and parameters",
      "description": "The developers of AI models can invest in cybersecurity to prevent compute re- sources, training source code, model weights, and other critical resources from being accessed and copied by unauthorized third parties (e.g., through insider threats or supply chain attacks). Access to model source code and weights can be restricted through an access control scheme, such as role-based access control. If access to model outputs by third parties is required, it can be provided through an API. Air gaps can block unauthorized remote access. In the case of necessary interaction with an external network, network bandwidth limitations can also be enforced to increase the detection window of potential breaches [108].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0985_Gipi\u0161kis2024",
      "name": "Hardware limitations on data center network connections",
      "description": "Hardware-enforced bandwidth limitations on data center network connections can protect AI model weights from unauthorized access or exfiltration, by lim- iting the speed of model weight access on the connections between data centers and the outside world. Such limitations can be put in place in multiple ways, for example by only constructing connections with a specific bandwidth. The output rate on all data channels can be set low enough that copying the weights is possible in principle (e.g., to enable regular backups), but would take so long that an unauthorized exfiltration of the weights could be detected and prevented. Such rate-limiting is only effective if it applies to all output connections for all storage locations on which the weights of the model are stored [139].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0986_Gipi\u0161kis2024",
      "name": "Structured access to a model",
      "description": "Structured access refers to methods which limit users\u2019 or deployers\u2019 direct access to a model\u2019s parameters by constraining access to a model through a centralized access point (e.g., an API) [88]. This access point can be monitored for usage, and access can be revoked to users or downstream deployers in cases of misuse [105]. Within this centralized access point, automated filtering-based monitoring can be done on both inputs and outputs to ensure the model\u2019s intended use is pre- served [36]. This filtering can sometimes be supplemented by human oversight, depending on desired robustness levels.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0987_Gipi\u0161kis2024",
      "name": "Sandboxing of AI Systems",
      "description": "AI systems can be developed and tested within a sandbox, (a secure and iso- lated environment used for separating running programs), such that outside access to information within the sandbox is restricted. Within this environ- ment, resources such as storage and memory space, and network access, would be disallowed or heavily restricted [15]. With sandboxing, dangerous or harmful outputs generated during testing will be contained.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0988_Gipi\u0161kis2024",
      "name": "Redundant systems not reliant on GPAI",
      "description": "Redundant systems provide continuation of a given system\u2019s processes in case of failure of the given system. Importantly, redundant systems should not rely on the factors that caused the original system to fail in the first place, which can include an AI system [47]. For example, if an AI system is incorporated into the landing gear system of an aircraft, such as during autonomous control of the aircraft, redundant systems in the form of mechanical or hydraulic mechanisms must be present to allow for deployment of the landing gear in case of AI system failure.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0989_Gipi\u0161kis2024",
      "name": "Diverse data labeling and algorithm fairness audits",
      "description": "To mitigate biases in AI models, model providers may want to prioritize di- versity among data labelers and conduct regular fairness audits on their algo- rithms. Data labeling teams that represent different backgrounds and demo- graphic groups can help create more balanced datasets.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0990_Gipi\u0161kis2024",
      "name": "Debiasing methods",
      "description": "Providers of AI models can apply techniques to reduce the biases of their models. Current debiasing methods focus on three main types of bias: \u2022 Racial and religious bias - Stereotypes based on religious beliefs or racial beliefs. \u2022 Gender bias - Stereotypes tied to gender roles and expectations. \u2022 Political and cultural bias - Propagation of dominant ideologies or extrem- ist attitudes. Debiasing methods can be categorized based on their application during AI development: \u2022 Data pre-processing - Removing or correcting unwanted and biased data, and augmenting quality data to offset data bias, such as rebalancing datasets with counterfactual data augmentation. \u2022 During training - Intervening on the training dynamics of the AI model, such as introducing debiasing terms in the objective function or by nega- tively reinforcing biased outputs. \u2022 Post-training - Applying techniques to correct a trained but biased model, such as modifying the embedding space.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0991_Gipi\u0161kis2024",
      "name": "Knowledge unlearning techniques",
      "description": "Knowledge unlearning techniques allow specific information to be \u201cforgotten\u201d without the need for retraining the entire model, preserving its general capabil- ities. These techniques can be used to reduce privacy risks and protect against copyrighted or harmful content [96, 188].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0992_Gipi\u0161kis2024",
      "name": "Differential privacy",
      "description": "Differential privacy techniques [8] can be used to protect users\u2019 privacy by ensuring that sensitive information is not leaked from a training dataset, even after thorough statistical analysis. With differential privacy, noise is added to the dataset or the model\u2019s output in such a way that one cannot deduce the presence or absence of a particular data point within the dataset. This provides individuals with plausible deniability and prevents their information from being exposed.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0993_Gipi\u0161kis2024",
      "name": "Quantifying privacy risks of AI models",
      "description": "Measuring privacy risks of an AI model allows the provider and user to calibrate their expectations on where the model can be applied, and it allows them to take the necessary steps to reduce such risks. For example, some metrics include: \u2022 Success rate of membership inference attacks [186] - Measures the rate that an attack correctly predicts a given record is part of the training dataset used to train a given AI model. \u2022 Discoverable memorization [38] - Theoretical upper-bound of the amount of training data that a given model memorizes. Assuming full knowledge of the training data, it measures the percentage of items that, for a given incomplete data point, a model outputs the remaining (memorized) part.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0994_Gipi\u0161kis2024",
      "name": "More energy-efficient models or techniques",
      "description": "Deploying more energy-efficient models can reduce their environmental impact. Different model architectural choices result in varying environmental costs, and identifying and adopting more energy-efficient options can result in significant environmental savings, especially when implemented at scale. Consideration should be given to both training energy usage, and deployment (\u201cinference\u201d) usage for the expected model lifecycle.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "environmental-1.6",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0995_Gipi\u0161kis2024",
      "name": "Disclosure of energy consumption by AI systems to authorities",
      "description": "Disclosures can direct more necessary attention and scrutiny to projects that consume significant energy. Disclosure involves releasing a summary of key details of the energy consumption of the AI system by all users, including the compute resources used, the amount of power consumed, the measures to reduce excess energy consumption that were in place, and energy sources [133].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "environmental-1.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0996_Gipi\u0161kis2024",
      "name": "Using low carbon intensity energy grids",
      "description": "Moving model training to energy grids with low carbon intensity can reduce the negative environmental impact [30]. The efficiency of energy grids can vary greatly depending on location. Models can be trained in different locations, as latency is not an issue.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "environmental-1.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A0997_Gipi\u0161kis2024",
      "name": "Red team access to the final version of a model pre-deployment",
      "description": "Granting red teams access to the final pre-release version of the model can help with identifying potentially dangerous model properties. These properties might not be identified if red teaming is only performed on earlier versions of the model, as late fine-tuning procedures may introduce new vulnerabilities. Red-teaming AI models before they are released to the public can reduce the model non-decomissionability risk. The model\u2019s release can be postponed or even prevented if previously unidentified flaws are detected during the testing [65].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0998_Gipi\u0161kis2024",
      "name": "Red teaming for GPAI system evaluation",
      "description": "Red teaming refers to simulated adversarial attacks performed to identify and evaluate the model\u2019s vulnerabilities as well as its in-domain and out-of-domain performance.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A0999_Gipi\u0161kis2024",
      "name": "Dynamic benchmarking",
      "description": "Dynamic benchmarks are benchmarks that can be continuously updated with new human-generated data. By having one or more target models \u201cin the loop,\u201d examples for benchmarking can be generated with the intent of fooling these target models, or to assess if these models express an appropriate level of uncer- tainty [103]. As the dataset in the benchmark grows, previously benchmarked models can also be reassessed against the updated dataset to reflect its perfor- mance in a more representative manner. Examples of such dynamic benchmarks include DynaSent [153] for sentiment analysis, LFTW [208] for hate speech, and Human-Adversarial VQA [182] for images.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1000_Gipi\u0161kis2024",
      "name": "Benchmark dataset auditing",
      "description": "Auditing benchmark datasets allows for verification of the utility and limitations of the datasets [158]. This allows the provider to more accurately measure AI model capabilities and safety. Auditing includes the evaluation of such datasets by independent third-party organizations and the release of benchmark dataset metadata to the auditors.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1001_Gipi\u0161kis2024",
      "name": "Informative and powerful benchmarks",
      "description": "Developers of GPAI systems can select benchmarks that are difficult enough to be informative about the capabilities of their AI systems, and cover a large spec- trum of domains in order to signal areas where the GPAI system is performing poorly [120]. Suitable benchmarks contain no label errors, are not vulnerable to being bench- mark contaminants, and are often audited by independent domain experts if they contain domain-specific questions. For multimodal GPAI systems, good benchmarks cover every modality, especially the interaction of different modalities.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1002_Gipi\u0161kis2024",
      "name": "Statistical data quality reports for benchmarks",
      "description": "If a benchmark dataset is too large to allow for the identification and removal of all flawed instances, statistical reports on the data composition can be added. Random sampling of benchmark data points can be performed to evaluate and report the frequency and types of errors found [54].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1003_Gipi\u0161kis2024",
      "name": "Avoiding benchmark data with publicly avail- able solutions and releasing contextual information for internet-derived data",
      "description": "Evaluators can improve the integrity of benchmarks by avoiding data with pub- licly available solutions. When using internet-derived data, supplementing it with contextual information (such as entity linking) may help to better docu- ment and assess the integrity of the collected data [95]. This could help in de- tecting instances where contextual information (which may have been included in the training data) might inadvertently reveal details about the solution, en- suring that the data is suitable for accurate benchmarking.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1004_Gipi\u0161kis2024",
      "name": "Reporting data decontamination efforts",
      "description": "Decontamination analysis may involve comparing the training dataset with the benchmark data, and publishing a report with statistics such as data overlap [235]. Especially for already trained models, including contamination statistics can allow experts to reweigh the success of the model on affected benchmarks.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1005_Gipi\u0161kis2024",
      "name": "Tracking benchmark leakage",
      "description": "Constant monitoring and documentation of benchmark leakage can help with the early detection of benchmark leaks [224, 95].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "post-deployment-3.5",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1006_Gipi\u0161kis2024",
      "name": "Preventing or mitigating data contamination and leakage",
      "description": "Developers of GPAIS and creators of benchmarks can take actions to prevent AI models from being trained on contaminated or leaked data, or mitigate such data contamination and leakage. For example, developers of AI models can try to find and remove contaminated or leaked data from the training corpus, and creators of benchmarks can help them by adding globally unique \u201ccanary strings\u201d to the documents containing their benchmarks, which makes them easier to find [197]. More involved inter- ventions by benchmark-creators include restricting access to benchmarks over an API, or continually updating benchmarks to focus on recent data.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1007_Gipi\u0161kis2024",
      "name": "Contamination detection",
      "description": "Contamination detection refers to techniques for assessing whether and to what extent a given model has benchmark data in its training dataset [161, 170]. This can involve a set of technical and regulatory interventions to prevent or identify a model trained on contaminated data. For example, with web-crawled data, contamination detection can involve com- paring the data\u2019s web sources against a dynamic, publicly available blocklist of websites known to generate new benchmarks. Additional measures may in- clude excluding data with improper metadata from the training dataset and conducting overlap analyses between the training data and all known standard benchmark datasets.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1008_Gipi\u0161kis2024",
      "name": "Frequent benchmarking to identify when red teaming is needed",
      "description": "Benchmarks, once created, are inexpensive to apply but may be less informative than red teaming. One reason is that sensitive data (e.g., relating to CBRN- related capabilities) cannot be included in the public questions and answers of benchmarks. On the other hand, red teaming can be more accurate given par- ticipants with diverse attack strategies, but it requires more resources to execute than benchmarking. If there is a correlation between benchmarking and red- teaming scores, then employing frequent benchmarking during the development of the model can identify arising vulnerabilities and inform the developers when more thorough red teaming is required [21]. Benchmarks can act as early warning signs of a larger issue, and red teaming can then be employed to investigate the severity and extent of such an issue.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1009_Gipi\u0161kis2024",
      "name": "Test robustness of GPAI system on relevant benchmarks",
      "description": "Various benchmarks [52, 236] have been developed to assess the robustness of GPAI systems when deployed in environments or scenarios that differ from their training conditions. These benchmarks typically evaluate the model\u2019s ability to handle variations in inputs, unexpected data distributions, or adversarial examples, aiming to ensure reliable performance outside the original training domain.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1010_Gipi\u0161kis2024",
      "name": "Benchmarking",
      "description": "Benchmarking is an evaluation method where different models are compared against a standardized dataset or a predetermined task. It allows comparison both across different models and over time, providing a reference point for model assessment. Benchmarks are usually open, where their question-answer pairs are publicly available [235].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1011_Gipi\u0161kis2024",
      "name": "GPAI models explaining model outputs in a zero-sum debate game",
      "description": "Debate is a technique that aims to produce reliable explanations of AI model outputs that are too complicated for humans to understand, by letting two GPAI models role-playing in a debate produce an explanation in a dialogue [102]. For example, an AI model may produce an output which is time-consuming for humans to verify as doing so may require going through extensive sources. Given such an output, the developer can use two natural language AI systems in an adversarial two-player setup to explain the output. These two natural language AI systems can be copies of the AI model that produced the output. In this setup, one AI system gives a short explanation of the output. The second AI system responds to this explanation with a counter-explanation or an argument why the first explanation was not correct. This continues for a fixed number of turns, with the two AI systems pointing out inconsistencies in each others\u2019 explanations. After this sequence of statements, a human or an AI judge evaluates the ex- planations of both AI systems to determine which one is more convincing. The results of this debate can then be used for further training of the models via reinforcement learning, where outputs that are more truthful and convincing arguments and explanations are positively reinforced, while misleading or false outputs are negatively reinforced.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "model-alignment-2.2",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "llm",
        "reinforcement-learning",
        "foundation"
      ]
    },
    {
      "id": "A1012_Gipi\u0161kis2024",
      "name": "Using an AI model to evaluate AI model outputs",
      "description": "In cases where the outputs of AI models cannot be easily evaluated, AI mod- els can be used to evaluate their outputs or the outputs of other AI models [82, 16, 17, 91]. The evaluations can then provide a training signal to improve the original model\u2019s performance or offer explanations of the output for human users.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1013_Gipi\u0161kis2024",
      "name": "Frequent testing when scaling model or dataset",
      "description": "Testing models after significant increases in compute, data, or model parame- ters. Even relatively small changes to model or dataset size can introduce new properties (\u201cemergent abilities\u201d) and failure modes. Identifying them early can prevent the models from being released prematurely",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1014_Gipi\u0161kis2024",
      "name": "Tamper-resistant safeguards for open-weight models",
      "description": "Training and implementing safeguards can improve the robustness of open- weight models against modifications from fine-tuning or other methods to change the learned weights of the models, especially those aimed at removing safety re- strictions. These safeguards can be resilient even after extensive fine-tuning, ensuring that the model retains its protective measures [199].",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1015_Gipi\u0161kis2024",
      "name": "Robustness Certificates",
      "description": "A model can be certified to withstand adversarial attacks given specific data- point constraints, model constraints, and attack vectors [156, 124]. Certification means that it can be both analytically proven and shown empirically that the model will withstand such attacks up to a certain threshold. Currently, robustness certification methods are limited to certifying against at- tacks via manipulation of pixels on specific lp norms, canonically the l2 (Eu- clidean) norm, up to a certain neighborhood radius.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1016_Gipi\u0161kis2024",
      "name": "Documentation of data collection, annotation, maintenance practices",
      "description": "Dataset collection, annotation, and maintenance processes can be documented in detail, including potential unintentional misuse scenarios and corresponding recommendations for data usage [80, 175, 99]. This contributes to transparency, ensures that inherent dataset limitations are known in advance, and helps in selecting the right datasets for intended use cases.",
      "source": "Gipi\u0161kis2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1018_Uuk2024",
      "name": "Advanced model access for vetted external researchers",
      "description": "Examples of advanced access rights could include any of the following: increased control over sampling, access to fine-tuning functionality, the ability to inspect and modify model internals, access to training data, or additional features like stable model versions.",
      "source": "Uuk2024",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1019_Uuk2024",
      "name": "Adversarial Robustness",
      "description": "State-of-the-art methods such as adversarial training to make models robust to adversarial attacks (e.g., jailbreaking).",
      "source": "Uuk2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1020_Uuk2024",
      "name": "Bug bounty programs",
      "description": "Clear and user-friendly bug bounty programs that acknowledge and reward individuals for reporting model vulnerabilities and dangerous capabilities.",
      "source": "Uuk2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1021_Uuk2024",
      "name": "Capability restrictions",
      "description": "Restricting risky capabilities of deployed models, such as advanced autonomy (e.g., self-assigning new sub-goals, executing long-horizon tasks) or tool use functionalities (e.g., function calls, web browsing).",
      "source": "Uuk2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1022_Uuk2024",
      "name": "Data curation",
      "description": "Careful data curation prior to all development stages (including fine-tuning) to filter out high-risk content and ensure the training data is sufficiently high-quality.",
      "source": "Uuk2024",
      "subcategoryId": "data-governance-3.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1023_Uuk2024",
      "name": "Deploying powerful models in stages",
      "description": "Starting with a small number of applications and fewer users, and gradually scaling up API-access as rigorous monitoring increases confidence in the model's safety. An API-mediated staged release would also be required before open-sourcing a model.",
      "source": "Uuk2024",
      "subcategoryId": "staged-deployment-3.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1024_Uuk2024",
      "name": "External assessment of testing procedure",
      "description": "Bringing in external AI evaluation firms before deployment to assess and red-team the company's execution of dangerous capabilities evaluations.",
      "source": "Uuk2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1025_Uuk2024",
      "name": "Fine-tuning restrictions",
      "description": "Restricting or closely monitoring fine-tuning access to models to ensure safeguards remain intact.",
      "source": "Uuk2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A1026_Uuk2024",
      "name": "Harmlessness training",
      "description": "State-of-the-art reinforcement learning and fine-tuning techniques, such as Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization (DPO), to ensure models do not engage in unsafe behavior.",
      "source": "Uuk2024",
      "subcategoryId": "model-alignment-2.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "reinforcement-learning",
        "foundation"
      ]
    },
    {
      "id": "A1027_Uuk2024",
      "name": "Input and output filtering",
      "description": "Monitoring for dangerous outputs (e.g., code that appears to be malware or viral genome sequences) and inputs that violate acceptable use policies to ensure models do not engage in harmful behavior.",
      "source": "Uuk2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1028_Uuk2024",
      "name": "Intolerable risk thresholds",
      "description": "Assessing and monitoring AI models with regard to red-line risk or capability thresholds set by a third-party, such as a standardization organization or regulator. Companies would further need to make technical, legal, and organizational preparations to halt development and deployment immediately when a breach occurs.",
      "source": "Uuk2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1029_Uuk2024",
      "name": "KYC screenings",
      "description": "Know-your-customer (KYC) screenings before granting access to models with very high misuse potential or to users producing large amounts of output.",
      "source": "Uuk2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A1030_Uuk2024",
      "name": "Pre-deployment risk assessments",
      "description": "Comprehensive risk assessments before deployment that would assess reasonably foreseeable misuse and include dangerous capability evaluations that incorporate post-training enhancements and collaborations with domain experts. Risk assessments would inform deployment decisions.",
      "source": "Uuk2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1031_Uuk2024",
      "name": "Pre-development risk assessments",
      "description": "Comprehensive risk assessments based on forecasted capabilities before training new models. Risk assessments would inform impactful development decisions.",
      "source": "Uuk2024",
      "subcategoryId": "risk-management-1.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1032_Uuk2024",
      "name": "Pre-registration of large training runs",
      "description": "Registering upcoming training runs above a certain size with an appropriate state actor. Such reports could include descriptions of architecture, training compute, data collection and curation, training objectives and techniques, and planned risk management procedures.",
      "source": "Uuk2024",
      "subcategoryId": "risk-disclosure-4.2",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1033_Uuk2024",
      "name": "Prohibiting high-stakes applications",
      "description": "Enforcing use policies that prohibit high-stakes applications. Requires Know-Your-Customer procedures.",
      "source": "Uuk2024",
      "subcategoryId": "access-management-3.3",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "llm",
        "foundation",
        "multi-modal"
      ]
    },
    {
      "id": "A1034_Uuk2024",
      "name": "Risk-focused governance structures",
      "description": "Companies adopt practices typical of high-reliability organizations (HROs), including board risk committees, chief risk officers, multi-party authorization requirements, ethics boards for reviewing development and deployment decisions, and internal audit teams that report directly to the board, tasked with auditing risk management practices.",
      "source": "Uuk2024",
      "subcategoryId": "board-oversight-1.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1035_Uuk2024",
      "name": "Safety Drills",
      "description": "Regularly practicing the implementation of an emergency response plan to stress test the organization's ability to respond to reasonably foreseeable, fast-moving emergency scenarios.",
      "source": "Uuk2024",
      "subcategoryId": "incident-response-3.6",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1036_Uuk2024",
      "name": "Safety incident reports and security information sharing",
      "description": "Disclosing reports about AI incidents, such as concrete harms and near misses as well as cyber threat intelligence and security incident reports with appropriate stakeholders such as select governments.",
      "source": "Uuk2024",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1037_Uuk2024",
      "name": "Safety incident reports and security information sharing",
      "description": "Disclosing reports about AI incidents, such as concrete harms and near misses as well as cyber threat intelligence and security incident reports with appropriate stakeholders such as select governments.",
      "source": "Uuk2024",
      "subcategoryId": "incident-reporting-4.3",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1038_Uuk2024",
      "name": "Safety vs. capabilities investments",
      "description": "A significant fraction of employees and computational resources are dedicated to enhancing model safety rather than advancing its capabilities.",
      "source": "Uuk2024",
      "subcategoryId": "safety-frameworks-1.5",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1039_Uuk2024",
      "name": "Sharing safety cases",
      "description": "Disclosing to a regulator how high-stakes decisions regarding model development and deployment are made.",
      "source": "Uuk2024",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1040_Uuk2024",
      "name": "Third party pre-deployment model audits",
      "description": "External pre-deployment assessment to provide a judgment on the safety of a model. Auditors, which could be governments or independent third parties, would receive access to a fine-tuning API for testing, or further appropriate technical means.",
      "source": "Uuk2024",
      "subcategoryId": "testing-auditing-3.1",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1041_Uuk2024",
      "name": "Transparent governance structures",
      "description": "Disclosing to a regulator how high-stakes decisions regarding model development and deployment are made.",
      "source": "Uuk2024",
      "subcategoryId": "governance-disclosure-4.4",
      "phases": [
        "phase-1",
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1042_Uuk2024",
      "name": "Unlearning Techniques",
      "description": "Removing specific harmful capabilities (e.g., pathogen design) from models using unlearning techniques.",
      "source": "Uuk2024",
      "subcategoryId": "safety-engineering-2.3",
      "phases": [
        "phase-1"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1043_Uuk2024",
      "name": "Vetted researcher access",
      "description": "Giving good faith, public interest evaluation researchers access to black-box research APIs that provide technical and legal safe harbours to limit barriers imposed by usage policy enforcement, logging, and stringent terms of service.",
      "source": "Uuk2024",
      "subcategoryId": "third-party-access-4.5",
      "phases": [
        "phase-2"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1044_Uuk2024",
      "name": "Whistleblower protections",
      "description": "Refraining from restrictive non-disparagement agreements and instantiating comprehensive whistleblower protection policies that clearly outline relevant reporting processes, protection mechanisms, and non-retaliation assurances.",
      "source": "Uuk2024",
      "subcategoryId": "whistleblower-1.4",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    },
    {
      "id": "A1045_Uuk2024",
      "name": "Advanced information security",
      "description": "Implementing advanced cybersecurity measures and insider threat safeguards to protect proprietary and unreleased model weights.",
      "source": "Uuk2024",
      "subcategoryId": "infrastructure-security-2.1",
      "phases": [
        "phase-2",
        "phase-3"
      ],
      "techTypes": [
        "all"
      ]
    }
  ]
}