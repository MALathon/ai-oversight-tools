{
	"$schema": "./schemas/technical-controls.schema.json",
	"version": "2.0.0",
	"lastUpdated": "2025-12-14",
	"source": "MIT AI Risk Repository Mitigation Database",
	"attribution": {
		"mitRepository": {
			"name": "MIT AI Risk Repository - AI Risk Mitigation Database",
			"url": "https://airisk.mit.edu/ai-risk-mitigations",
			"citation": "MIT AI Risk Repository (2024). Mapping AI Risk Mitigations: Draft Taxonomy & Evidence Scan.",
			"sources": [
				{
					"id": "Barrett2024",
					"citation": "Barrett et al. (2024)",
					"count": 75
				},
				{
					"id": "Bengio2025",
					"citation": "Bengio et al. (2025)",
					"count": 26
				},
				{
					"id": "Campos2025",
					"citation": "Campos et al. (2025)",
					"count": 19
				},
				{
					"id": "Casper2025",
					"citation": "Casper et al. (2025)",
					"count": 14
				},
				{
					"id": "EU AI Office2025",
					"citation": "EU AI Office (2025)",
					"count": 16
				},
				{
					"id": "Eisenberg2025",
					"citation": "Eisenberg (2025)",
					"count": 38
				},
				{
					"id": "Future of Life Institute2024",
					"citation": "Future of Life Institute (2024)",
					"count": 85
				},
				{
					"id": "Gipiškis2024",
					"citation": "Gipiškis (2024)",
					"count": 82
				},
				{
					"id": "NIST2024",
					"citation": "NIST AI RMF (2024)",
					"count": 203
				},
				{
					"id": "Schuett2023",
					"citation": "Schuett (2023)",
					"count": 50
				},
				{
					"id": "UK Government2023",
					"citation": "UK Government (2023)",
					"count": 169
				},
				{
					"id": "Uuk2024",
					"citation": "Uuk (2024)",
					"count": 28
				},
				{
					"id": "Wiener2024",
					"citation": "Wiener (2024)",
					"count": 10
				}
			]
		},
		"aihsr": {
			"name": "AI in Human Subjects Research (AIHSR) Risk Reference Tool",
			"version": "1.5",
			"author": "Tamiko Eto",
			"citation": "Eto, T. (2025). AIHSR Risk Reference Tool v1.5."
		},
		"nist": {
			"name": "NIST AI Risk Management Framework",
			"url": "https://www.nist.gov/itl/ai-risk-management-framework",
			"citation": "NIST (2024). AI Risk Management Framework: Generative AI Profile."
		}
	},
	"description": "Technical and operational controls for AI risk mitigation, organized by MIT taxonomy with phase and technology-type awareness",
	"controlCategories": [
		{
			"id": "governance-oversight",
			"name": "Governance & Oversight Controls",
			"description": "Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols",
			"code": "1"
		},
		{
			"id": "technical-security",
			"name": "Technical & Security Controls",
			"description": "Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors",
			"code": "2"
		},
		{
			"id": "operational-process",
			"name": "Operational Process Controls",
			"description": "Processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation",
			"code": "3"
		},
		{
			"id": "transparency-accountability",
			"name": "Transparency & Accountability Controls",
			"description": "Formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny",
			"code": "4"
		}
	],
	"controlSubcategories": [
		{
			"id": "board-oversight-1.1",
			"code": "1.1",
			"name": "Board Structure & Oversight",
			"categoryId": "governance-oversight"
		},
		{
			"id": "risk-management-1.2",
			"code": "1.2",
			"name": "Risk Management",
			"categoryId": "governance-oversight"
		},
		{
			"id": "conflict-interest-1.3",
			"code": "1.3",
			"name": "Conflict of Interest Protections",
			"categoryId": "governance-oversight"
		},
		{
			"id": "whistleblower-1.4",
			"code": "1.4",
			"name": "Whistleblower Reporting & Protection",
			"categoryId": "governance-oversight"
		},
		{
			"id": "safety-frameworks-1.5",
			"code": "1.5",
			"name": "Safety Decision Frameworks",
			"categoryId": "governance-oversight"
		},
		{
			"id": "environmental-1.6",
			"code": "1.6",
			"name": "Environmental Impact Management",
			"categoryId": "governance-oversight"
		},
		{
			"id": "societal-impact-1.7",
			"code": "1.7",
			"name": "Societal Impact Assessment",
			"categoryId": "governance-oversight"
		},
		{
			"id": "infrastructure-security-2.1",
			"code": "2.1",
			"name": "Model & Infrastructure Security",
			"categoryId": "technical-security"
		},
		{
			"id": "model-alignment-2.2",
			"code": "2.2",
			"name": "Model Alignment",
			"categoryId": "technical-security"
		},
		{
			"id": "safety-engineering-2.3",
			"code": "2.3",
			"name": "Model Safety Engineering",
			"categoryId": "technical-security"
		},
		{
			"id": "content-safety-2.4",
			"code": "2.4",
			"name": "Content Safety Controls",
			"categoryId": "technical-security"
		},
		{
			"id": "testing-auditing-3.1",
			"code": "3.1",
			"name": "Testing & Auditing",
			"categoryId": "operational-process"
		},
		{
			"id": "data-governance-3.2",
			"code": "3.2",
			"name": "Data Governance",
			"categoryId": "operational-process"
		},
		{
			"id": "access-management-3.3",
			"code": "3.3",
			"name": "Access Management",
			"categoryId": "operational-process"
		},
		{
			"id": "staged-deployment-3.4",
			"code": "3.4",
			"name": "Staged Deployment",
			"categoryId": "operational-process"
		},
		{
			"id": "post-deployment-3.5",
			"code": "3.5",
			"name": "Post-Deployment Monitoring",
			"categoryId": "operational-process"
		},
		{
			"id": "incident-response-3.6",
			"code": "3.6",
			"name": "Incident Response & Recovery",
			"categoryId": "operational-process"
		},
		{
			"id": "documentation-4.1",
			"code": "4.1",
			"name": "System Documentation",
			"categoryId": "transparency-accountability"
		},
		{
			"id": "risk-disclosure-4.2",
			"code": "4.2",
			"name": "Risk Disclosure",
			"categoryId": "transparency-accountability"
		},
		{
			"id": "incident-reporting-4.3",
			"code": "4.3",
			"name": "Incident Reporting",
			"categoryId": "transparency-accountability"
		},
		{
			"id": "governance-disclosure-4.4",
			"code": "4.4",
			"name": "Governance Disclosure",
			"categoryId": "transparency-accountability"
		},
		{
			"id": "third-party-access-4.5",
			"code": "4.5",
			"name": "Third-Party System Access",
			"categoryId": "transparency-accountability"
		},
		{
			"id": "user-rights-4.6",
			"code": "4.6",
			"name": "User Rights & Recourse",
			"categoryId": "transparency-accountability"
		}
	],
	"controls": [
		{
			"id": "A0001_Bengio2025",
			"name": "Audits",
			"description": "A formal review of an organisation’s compliance with standards, policies, and procedures, typically carried out by an external party. AI auditing is a rapidly growing field, but builds on long histories of auditing in other fields, including financial, environmental, and health regulation.",
			"source": "Bengio2025",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design audits framework for training data and model design review",
				"phase-2": "Execute audits on validation cohorts and model outputs",
				"phase-3": "Perform scheduled audits on production system and outcomes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0002_Bengio2025",
			"name": "Benchmarks",
			"description": "A standardised, often quantitative test or metric used to evaluate and compare the performance of AI systems on a fixed set of tasks designed to represent real- world usage",
			"source": "Bengio2025",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform benchmarks on prospective validation data"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0003_Bengio2025",
			"name": "Bowtie Method",
			"description": "A technique for visualising risk quantitatively and qualitatively, providing clear differentiation between proactive and reactive risk management, intended to help prevent and mitigate major accident hazards. Oil companies and national governments use the bowtie method.",
			"source": "Bengio2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate bowtie method during prospective testing phase",
				"phase-3": "Apply bowtie method in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0004_Bengio2025",
			"name": "Defence in Depth",
			"description": "The idea that multiple independent and overlapping layers of defence can be implemented such that if one fails, others will still be effective. An example comes from the field of infectious diseases, where multiple preventative measures (e.g. vaccines, masks, hand washing) can layer to reduce overall risk.",
			"source": "Bengio2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Deploy defence in depth in validation environment",
				"phase-3": "Maintain defence in depth in production environment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0006_Bengio2025",
			"name": "Engagement with Relevant Experts and Communities",
			"description": "Domain experts, users, and impacted communities have unique insights into likely risks. There are emerging guidelines for participatory and inclusive AI.",
			"source": "Bengio2025",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-2": "Validate engagement with relevant experts and communities during prospective testing phase",
				"phase-3": "Apply engagement with relevant experts and communities in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0007_Bengio2025",
			"name": "If-Then Commitments",
			"description": "A set of technical and organisational protocols and commitments to manage risks at varying levels as AI models become more capable. Some companies developing general- purpose AI employ these types of commitments as responsible scaling policies or similar frameworks.",
			"source": "Bengio2025",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate if-then commitments with clinical stakeholders",
				"phase-3": "Apply if-then commitments to clinical use cases"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0008_Bengio2025",
			"name": "Impact Assessment",
			"description": "A tool used to assess the potential impacts of a technology or project. The EU AI Act requires developers of high- risk AI systems to carry out Fundamental Rights Impact Assessments.",
			"source": "Bengio2025",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-2": "Conduct impact assessment based on validation findings",
				"phase-3": "Maintain regular impact assessment schedule for production"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0009_Bengio2025",
			"name": "Incident Reporting",
			"description": "The process of systematically documenting and sharing cases in which developing or deploying AI has caused direct or indirect harms. Incident reporting is common in many domains, from human resources to cybersecurity. It has also become more common for AI.",
			"source": "Bengio2025",
			"subcategoryId": "incident-reporting-4.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain current incident reporting reflecting operational changes"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0010_Bengio2025",
			"name": "Model Evaluation",
			"description": "Processes to assess and measure an AI system's performance on a particular task. There are countless AI evaluations to assess different capabilities and risks, including for security.",
			"source": "Bengio2025",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply model evaluation to validation results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0011_Bengio2025",
			"name": "Red Teaming",
			"description": "An exercise in which a group of people or automated systems pretend to be an adversary and attack an organisation’s systems in order to identify vulnerabilities.",
			"source": "Bengio2025",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate red teaming during prospective testing phase",
				"phase-3": "Apply red teaming in live clinical deployment"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0012_Bengio2025",
			"name": "Risk management frameworks",
			"description": "Whole organisation frameworks to reduce gaps in risk coverage and ensure various risk activities (i.e. all of the above) are cohesively structured and aligned, risk roles and responsibilities are clearly defined, and checks and balances are in place to avoid silos and manage conflicts of interest. In other safety critical industries, the Three Lines of Defence framework – separating risk ownership, oversight and audit – is widely used and can be usefully applied to advanced AI companies",
			"source": "Bengio2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Execute risk management frameworks on validation cohorts and model outputs",
				"phase-3": "Perform scheduled risk management frameworks on production system and outcomes"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0013_Bengio2025",
			"name": "Risk Register",
			"description": "A risk management tool that serves as a repository of all risks, their prioritisation, owners, and mitigation plans. They are sometimes used to fulfil regulatory compliance. Risk registers are a relatively standard tool used across many industries, including cybersecurity and recently AI.",
			"source": "Bengio2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Define risk register requirements for development environment",
				"phase-2": "Test risk register in validation environment",
				"phase-3": "Maintain risk register with continuous monitoring"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0014_Bengio2025",
			"name": "Risk Taxonomy",
			"description": "A way to categorise and organise risks across multiple dimensions. There are several well- known risk taxonomies for AI.",
			"source": "Bengio2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan risk taxonomy approach during project design phase",
				"phase-2": "Validate risk taxonomy during prospective testing phase",
				"phase-3": "Apply risk taxonomy in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0015_Bengio2025",
			"name": "Risk Thresholds",
			"description": "Quantitative or qualitative limits that distinguish acceptable from unacceptable risks and trigger specific risk management actions when exceeded. Risk thresholds for general- purpose AI are being determined by a combination of assessments of capabilities, impact, compute, reach, and other factors.",
			"source": "Bengio2025",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct risk thresholds based on validation findings",
				"phase-3": "Maintain regular risk thresholds schedule for production"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0016_Bengio2025",
			"name": "Risk Tolerance",
			"description": "The level of risk an organisation is willing to take on. In AI, risks tolerances are often left up to AI companies, but regulatory regimes can help identify unacceptable risks that are legally prohibited",
			"source": "Bengio2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate risk tolerance during prospective testing phase",
				"phase-3": "Apply risk tolerance in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0017_Bengio2025",
			"name": "Safety Analysis",
			"description": "Helps understand the dependencies between components and the system that they are part of, in order to anticipate how component failures could lead to system-level hazards. This approach is used across safety- critical fields, e.g. to anticipate and prevent aircraft crashes or nuclear reactor core meltdowns",
			"source": "Bengio2025",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply safety analysis in live clinical deployment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0018_Bengio2025",
			"name": "Safety Cases",
			"description": "Safety cases require developers to demonstrate safety. A safety case is a structured argument supported by evidence that a system is acceptably safe to operate in a particular context. Safety cases are common in many industries, including defence, aerospace, and railways.",
			"source": "Bengio2025",
			"subcategoryId": "governance-disclosure-4.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Enhance safety cases through continuous improvement"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0019_Bengio2025",
			"name": "Safety of The Intended Function",
			"description": "An approach that requires engineers to provide evidence that a system is safe when operating as intended. This approach is used in many engineering fields, such as in the construction and testing of road vehicles.",
			"source": "Bengio2025",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform safety of the intended function on prospective validation data",
				"phase-3": "Conduct ongoing safety of the intended function on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0020_Bengio2025",
			"name": "Scenario Analysis",
			"description": "Developing plausible future scenarios and analysing how risks materialise. Scenario analysis and planning are widely used across industries including for the energy sector and to address uncertainties of power systems.",
			"source": "Bengio2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Begin scenario analysis development"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0021_Bengio2025",
			"name": "Threat Modelling",
			"description": "A process to identify threats and vulnerabilities to a system. Threat modelling is commonly used to support AI security throughout AI research and development",
			"source": "Bengio2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Finalize threat modelling based on testing"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0022_Bengio2025",
			"name": "Whistleblower protection",
			"description": "Whistleblowers can play an important role in alerting authorities to dangerous risks at AI companies due to the proprietary nature of many AI advancements. Incentives and protections for whistleblowers are expected to be an important part of advanced AI risk governance.",
			"source": "Bengio2025",
			"subcategoryId": "whistleblower-1.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Exercise whistleblower protection for validation phase decisions",
				"phase-3": "Maintain whistleblower protection with regular oversight reviews"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0023_Bengio2025",
			"name": "Delphi Method",
			"description": "A group decision-making technique that uses a series of questionnaires to gather consensus from a panel of experts. The Delphi method has been used to help identify key AI risks.",
			"source": "Bengio2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate delphi method during prospective testing phase",
				"phase-3": "Apply delphi method in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0025_Bengio2025",
			"name": "Risk Matrices",
			"description": "A visual tool that helps prioritise risks according to their likelihood of occurrence and potential impact. Risk matrices are used in many industries and for many purposes, such as by financial institutions for evaluating credit risk, or by companies to assess possible disruptions to their supply chains",
			"source": "Bengio2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate risk matrices during prospective testing phase"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0026_Bengio2025",
			"name": "Safety by Design",
			"description": "An approach that centres user safety in the design and development of products and services. This approach is common across engineering and safety- critical fields including aviation and energy.",
			"source": "Bengio2025",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-1": "Begin safety by design development",
				"phase-3": "Enhance safety by design through continuous improvement"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0028_Casper2025",
			"name": "Documentation availability",
			"description": "All of the above documentation can be made available to the public (redacted) and AI governing authorities (unredacted).",
			"source": "Casper2025",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update documentation availability with validation procedures and findings",
				"phase-3": "Maintain current documentation availability reflecting operational changes"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0033_Casper2025",
			"name": "Labeling AI-generated content",
			"description": "To aid in digital forensics, content produced from AI systems can be labeled with metadata, watermarks, and notices.",
			"source": "Casper2025",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-3": "Apply labeling ai-generated content in live clinical deployment"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0040_Casper2025",
			"name": "Whistleblower protections",
			"description": "Regulations can explicitly prevent retaliation and offer incentives for whistleblowers who report violations of those regulations.",
			"source": "Casper2025",
			"subcategoryId": "whistleblower-1.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate whistleblower protections summarizing validation outcomes",
				"phase-3": "Produce regular whistleblower protections for stakeholders"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0042_Eisenberg2025",
			"name": "Establish AI system access controls",
			"description": "Implement comprehensive access management including role-based access control (RBAC), authentication mechanisms, and audit logging for AI models and associated resources.",
			"source": "Eisenberg2025",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Execute establish ai system access controls on validation cohorts and model outputs"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0043_Eisenberg2025",
			"name": "Implement AI asset protection framework",
			"description": "Deploy technical protection measures including encryption, secure enclaves, and versioning controls for AI models and associated data.",
			"source": "Eisenberg2025",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate within implement ai asset protection framework for deployed system"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0044_Eisenberg2025",
			"name": "Establish security validation framework",
			"description": "Execute comprehensive pre-deployment security validation including AI-specific vulnerability assessments, penetration testing, and security requirement verification.",
			"source": "Eisenberg2025",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan establish security validation framework methodology for retrospective data analysis",
				"phase-2": "Perform establish security validation framework on prospective validation data",
				"phase-3": "Conduct ongoing establish security validation framework on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0045_Eisenberg2025",
			"name": "Implement continuous security testing system",
			"description": "Deploy ongoing security testing mechanisms including automated vulnerability scanning, continuous security monitoring, and periodic reassessment of security controls.",
			"source": "Eisenberg2025",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform implement continuous security testing system on prospective validation data",
				"phase-3": "Conduct ongoing implement continuous security testing system on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0046_Eisenberg2025",
			"name": "Implement AI security defense system",
			"description": "Deploy active defense mechanisms combining continuous security monitoring, input validation, adversarial detection, and adaptive response capabilities specific to AI systems.",
			"source": "Eisenberg2025",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Implement implement ai security defense system in shadow mode alongside clinical workflows",
				"phase-3": "Operate implement ai security defense system with real-time alerting"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0050_Eisenberg2025",
			"name": "Establish AI system documentation framework",
			"description": "Implement comprehensive documentation requirements and processes covering training data provenance, system architecture, model cards, and component interactions to ensure transparent documentation of both the data lifecycle and system design.",
			"source": "Eisenberg2025",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Create initial establish ai system documentation framework capturing design decisions and data sources"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0051_Eisenberg2025",
			"name": "Implement AI system monitoring and logging infrastructure",
			"description": "Deploy comprehensive monitoring and logging systems that capture AI system behavior, decisions, performance metrics, and real-time data source usage at multiple levels of granularity for full system observability, including tracking of data lineage during inference.",
			"source": "Eisenberg2025",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Continue implement ai system monitoring and logging infrastructure for validation activities",
				"phase-3": "Operate implement ai system monitoring and logging infrastructure for clinical decision tracking"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0052_Eisenberg2025",
			"name": "Establish AI decision explanation framework",
			"description": "Implement mechanisms and tools for generating human-understandable explanations of AI system decisions, including feature importance, decision paths, confidence levels, and clear attribution of data sources and their characteristics used during inference.",
			"source": "Eisenberg2025",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply establish ai decision explanation framework to validation activities",
				"phase-3": "Operate within establish ai decision explanation framework for deployed system"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0053_Eisenberg2025",
			"name": "Establish and apply performance testing and validation framework",
			"description": "Implement comprehensive performance requirements, testing protocols, and validation procedures to ensure AI systems meet capability requirements and maintain reliable operation across intended use cases.",
			"source": "Eisenberg2025",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan establish and apply performance testing and validation framework methodology for retrospective data analysis",
				"phase-2": "Perform establish and apply performance testing and validation framework on prospective validation data"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0054_Eisenberg2025",
			"name": "Implement performance monitoring and robustness system",
			"description": "Implement continuous monitoring and testing mechanisms to evaluate AI system robustness, generalization capabilities, and performance stability across varying conditions and environments while in production.",
			"source": "Eisenberg2025",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform implement performance monitoring and robustness system on prospective validation data",
				"phase-3": "Conduct ongoing implement performance monitoring and robustness system on live system performance"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0055_Eisenberg2025",
			"name": "Establish and apply fairness testing and validation framework",
			"description": "Implement comprehensive procedures to validate model fairness during development and pre-deployment, including test dataset creation, metric definition, and systematic assessment of performance disparities across demographic groups.",
			"source": "Eisenberg2025",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan establish and apply fairness testing and validation framework methodology for retrospective data analysis",
				"phase-2": "Perform establish and apply fairness testing and validation framework on prospective validation data",
				"phase-3": "Conduct ongoing establish and apply fairness testing and validation framework on live system performance"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0056_Eisenberg2025",
			"name": "Implement fairness monitoring and remediation system",
			"description": "Deploy continuous monitoring systems to detect fairness issues in production, including automated drift detection, performance disparity alerts, and systematic remediation procedures.",
			"source": "Eisenberg2025",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Implement implement fairness monitoring and remediation system in shadow mode alongside clinical workflows",
				"phase-3": "Operate implement fairness monitoring and remediation system with real-time alerting"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0057_Eisenberg2025",
			"name": "Establish universal access and performance design framework",
			"description": "Establish and follow a structured framework ensuring the AI system is designed and developed to deliver consistent, high-quality performance and accessibility for all intended user groups, regardless of their characteristics or circumstances.",
			"source": "Eisenberg2025",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design establish universal access and performance design framework tailored to project scope",
				"phase-2": "Apply establish universal access and performance design framework to validation activities",
				"phase-3": "Operate within establish universal access and performance design framework for deployed system"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0058_Eisenberg2025",
			"name": "Establish content safety policy and boundaries",
			"description": "Define and document comprehensive content safety policies, including prohibited content categories, acceptable content guidelines, output constraints, and required safeguards. Establish clear thresholds, classification criteria, and escalation levels for different types of harmful content. Include specific criteria for content that could enable or promote malicious use.",
			"source": "Eisenberg2025",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain current establish content safety policy and boundaries reflecting operational changes"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0059_Eisenberg2025",
			"name": "Implement content moderation system",
			"description": "Implement automated and/or human-in-the-loop content moderation mechanisms to detect and filter harmful content in real-time, including content classification, blocking procedures, and automated enforcement of safety boundaries. Include detection of potential malicious use patterns.",
			"source": "Eisenberg2025",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-3": "Run implement content moderation system continuously on live outputs"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0060_Eisenberg2025",
			"name": "Implement content safety incident response",
			"description": "Establish procedures for investigating, documenting, and remediating harmful content incidents that bypass moderation systems, including coordination with relevant authorities, root cause analysis, and system improvement protocols. Include specific procedures for suspected malicious use cases.",
			"source": "Eisenberg2025",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-3": "Maintain current implement content safety incident response reflecting operational changes"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0061_Eisenberg2025",
			"name": "Establish information quality assurance framework",
			"description": "Implement comprehensive mechanisms to assess, verify, and improve the factual accuracy of AI system outputs, including source validation, fact-checking procedures, and uncertainty communication protocols.",
			"source": "Eisenberg2025",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test and refine establish information quality assurance framework during validation"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0063_Eisenberg2025",
			"name": "Implement adversarial testing and red team program",
			"description": "Conduct systematic adversarial testing and red team exercises focused on probing AI system capabilities, identifying potential misuse vectors, and exposing unintended harmful behaviors. Testing should explore ways the system could be manipulated to produce dangerous outputs, bypass safety guardrails, or exhibit undesired emergent behaviors. Include scenarios involving both individual and coordinated attempts to exploit the system’s capabilities.",
			"source": "Eisenberg2025",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform implement adversarial testing and red team program on prospective validation data",
				"phase-3": "Conduct ongoing implement adversarial testing and red team program on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0064_Eisenberg2025",
			"name": "Implement system usage monitoring and prevention",
			"description": "Monitor and prevent malicious or otherwise disallowed behavioral patterns including automated abuse, coordination across accounts, and systematic manipulation attempts.",
			"source": "Eisenberg2025",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate implement system usage monitoring and prevention with real-time alerting"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0065_Eisenberg2025",
			"name": "Implement AI system usage verification program",
			"description": "Deploy comprehensive measures to verify user identity, document intended use cases, and ensure AI system usage complies with instructions. This includes KYC procedures for user verification, clear documentation of permitted uses, and user acknowledgment of instructions.",
			"source": "Eisenberg2025",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-3": "Maintain current implement ai system usage verification program reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0066_Eisenberg2025",
			"name": "Implement AI System Disclosure Requirements",
			"description": "Deploy mechanisms to ensure clear, timely disclosure of AI system use to end users, including automated notifications of AI involvement in interactions, explicit identification of AI-generated content, and clear communication of when users are interacting with AI systems.",
			"source": "Eisenberg2025",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan implement ai system disclosure requirements approach and resource requirements",
				"phase-3": "Maintain implement ai system disclosure requirements in production environment"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0067_Eisenberg2025",
			"name": "Implement a privacy protection framework",
			"description": "Implement comprehensive privacy protection measures to prevent exposure of PII and sensitive information, including data minimization, anonymization procedures, and privacy-preserving inference techniques.",
			"source": "Eisenberg2025",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Develop implement a privacy protection framework for project governance"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0068_Eisenberg2025",
			"name": "Implement a privacy incident detection and response",
			"description": "Deploy monitoring and response mechanisms to detect and address potential privacy exposures, including PII leak detection, sensitive information monitoring, and privacy incident handling procedures.",
			"source": "Eisenberg2025",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate implement a privacy incident detection and response with real-time alerting"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0069_Eisenberg2025",
			"name": "Establish user rights and recourse framework",
			"description": "Implement comprehensive mechanisms for user reporting, feedback collection, incident investigation, and recourse provision, including clear procedures for users to report issues, request explanations or corrections, appeal decisions, and receive appropriate remediation. The system should handle various types of user concerns including system errors, unfair treatment, privacy violations, and safety issues.",
			"source": "Eisenberg2025",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Produce regular establish user rights and recourse framework for stakeholders"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0071_Eisenberg2025",
			"name": "Establish human-AI interaction safety framework",
			"description": "Implement comprehensive safeguards to ensure appropriate levels of human oversight, control, and agency in AI system interactions, including decision autonomy requirements, override capabilities, and dependency prevention measures.",
			"source": "Eisenberg2025",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design establish human-ai interaction safety framework tailored to project scope",
				"phase-3": "Operate within establish human-ai interaction safety framework for deployed system"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0072_Eisenberg2025",
			"name": "Implement psychological impact management system",
			"description": "Establish monitoring and intervention procedures to detect and prevent unhealthy user-AI relationships, including emotional dependency tracking, interaction boundary enforcement, and well-being safeguards.",
			"source": "Eisenberg2025",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-3": "Operate implement psychological impact management system for clinical decision tracking"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0073_Eisenberg2025",
			"name": "Implement environmental impact management system",
			"description": "Implement comprehensive environmental impact monitoring and optimization procedures, including energy efficiency measures, carbon footprint tracking, and hardware lifecycle management.",
			"source": "Eisenberg2025",
			"subcategoryId": "environmental-1.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-3": "Operate implement environmental impact management system with real-time alerting"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0074_Eisenberg2025",
			"name": "Establish third-party assessment and management framework",
			"description": "Establish comprehensive procedures for documenting, assessing, and managing upstream providers and dependencies in the AI system value chain, including transparency requirements, compliance verification, dependency tracking, and contingency planning.",
			"source": "Eisenberg2025",
			"subcategoryId": "third-party-access-4.5",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Conduct initial establish third-party assessment and management framework during project planning"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0075_Eisenberg2025",
			"name": "Establish AI legal compliance process",
			"description": "Evaluate and document how the AI system complies with relevant regulations and standards, identifying use case-specific legal risks and required controls. Apply the organization’s legal compliance framework to ensure appropriate safeguards are in place, with clear documentation of compliance assessments and risk mitigations.",
			"source": "Eisenberg2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct establish ai legal compliance process based on validation findings"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0076_Eisenberg2025",
			"name": "Establish societal impact assessment framework",
			"description": "Implement comprehensive processes for assessing and documenting potential societal impacts of AI systems, including effects on employment, economic systems, power dynamics, and cultural value. Include stakeholder consultation and impact mitigation planning.",
			"source": "Eisenberg2025",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-1": "Conduct initial establish societal impact assessment framework during project planning"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0077_Eisenberg2025",
			"name": "Establish responsible development and deployment policy",
			"description": "Establish policies and procedures governing AI system development and deployment decisions that consider societal implications, including competitive pressures, governance gaps, and benefit distribution.",
			"source": "Eisenberg2025",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Enforce establish responsible development and deployment policy in production operations"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0078_Eisenberg2025",
			"name": "Implement AI alignment validation system",
			"description": "Establish processes for validating and maintaining AI system alignment with human values and goals, including testing for goal preservation, monitoring for objective drift, and validation of decision-making processes against ethical standards. Includes specific attention to detecting and preventing potentially misaligned behaviors, emergent goals, or deceptive actions. Covers using interpretability techniques to measure and assure alignment with intended goals.",
			"source": "Eisenberg2025",
			"subcategoryId": "model-alignment-2.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"reinforcement-learning",
				"foundation"
			],
			"implementationNotes": {
				"phase-2": "Perform implement ai alignment validation system on prospective validation data",
				"phase-3": "Conduct ongoing implement ai alignment validation system on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0079_Eisenberg2025",
			"name": "Establish AI Risk Management System",
			"description": "Implement a comprehensive AI risk management system including risk assessment processes, monitoring frameworks, governance structures, and response procedures.",
			"source": "Eisenberg2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular establish ai risk management system schedule for production"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0080_Eisenberg2025",
			"name": "Establish data governance and management practices",
			"description": "Implement data governance measures used for training, including having a copyright policy and identifying and documenting data sources, potential biases, and mitigations taken.",
			"source": "Eisenberg2025",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Create initial establish data governance and management practices capturing design decisions and data sources",
				"phase-2": "Update establish data governance and management practices with validation procedures and findings",
				"phase-3": "Maintain current establish data governance and management practices reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0081_Eisenberg2025",
			"name": "Establish documentation sharing mechanism",
			"description": "Implement a process to share information and documentation to third-parties, including to regulators and downstream deployers or developers.",
			"source": "Eisenberg2025",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain current establish documentation sharing mechanism reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0082_Eisenberg2025",
			"name": "Implement a risk reporting mechanism",
			"description": "Establish processes to identify and disclose known or reasonably foreseeable risks, the discovery of new risks, or instances of non-conformity to third parties.",
			"source": "Eisenberg2025",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate implement a risk reporting mechanism summarizing validation outcomes",
				"phase-3": "Produce regular implement a risk reporting mechanism for stakeholders"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0083_Eisenberg2025",
			"name": "Establish a general purpose incident response mechanism",
			"description": "Establish processes to enable incident monitoring and reporting. This includes defining ”serious incidents” or set a threshold for formal reporting based on regulatory requirements to third-parties, regulators, and impacted individuals.",
			"source": "Eisenberg2025",
			"subcategoryId": "incident-reporting-4.3",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Define establish a general purpose incident response mechanism requirements and templates",
				"phase-3": "Produce regular establish a general purpose incident response mechanism for stakeholders"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0087_Future of Life Institute2024",
			"name": "AI-Generated Content Watermarking",
			"description": "Are the outputs of your firm’s AI systems tagged with watermarks that indicate that an AI generates the material? - Video - Image",
			"source": "Future of Life Institute2024",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-3": "Apply ai-generated content watermarking in live clinical deployment"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0105_Future of Life Institute2024",
			"name": "Control/Alignment Strategy",
			"description": "We assess whether the company has publicly shared their strategy for ensuring that ever more advanced artificial intelligence remains under human control or remains aligned, and summarize contents of any such documents. We exclude policy recommendations to governments and other stakeholders.",
			"source": "Future of Life Institute2024",
			"subcategoryId": "governance-disclosure-4.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update control/alignment strategy with validation procedures and findings",
				"phase-3": "Maintain current control/alignment strategy reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0110_Future of Life Institute2024",
			"name": "Emergency Rollback Plan",
			"description": "Critically dangerous capabilities or very severe yet unexpected misuse patterns might only surface after a system has been deployed. Has your firm developed an emergency response plan to react to scenarios where such problems cannot be resolved quickly via updates? Please select all interventions that your organization has implemented. Made legal and technical preparations to roll back a system rapidly Formally specified the risk threshold that would trigger a rapid rollback Committed to regular safety drills to test emergency response",
			"source": "Future of Life Institute2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform emergency rollback plan on prospective validation data",
				"phase-3": "Conduct ongoing emergency rollback plan on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0114_Future of Life Institute2024",
			"name": "Fine-tuning protections",
			"description": "Fine-tuning restrictions that ensure the integrity of safety mitigations.",
			"source": "Future of Life Institute2024",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-3": "Operate fine-tuning protections for live data"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0117_Future of Life Institute2024",
			"name": "Harmful Data Removal from Training Sets",
			"description": "When training large models, does your organization remove data that contains information related to dangerous capabilities or harmful outcomes from the training set? If yes, please select the categories of data that are removed. Detailed information about the development, acquisition, or dispersion of CBRN weapons Instructional content for conducting cyberattacks Hateful or discriminatory content Advice or encouragement for self-harm Graphic violent content Graphic sexual content Personally Identifiable Information (PII) Detailed information about bomb-making or other terrorism-enabling tactics",
			"source": "Future of Life Institute2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Finalize harmful data removal from training sets based on testing",
				"phase-3": "Enhance harmful data removal from training sets through continuous improvement"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0125_Future of Life Institute2024",
			"name": "Leadership communications on catastrophic risks",
			"description": "We report whether leadership communicates to the public about potential catastrophic risks from advanced AI.",
			"source": "Future of Life Institute2024",
			"subcategoryId": "governance-disclosure-4.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate leadership communications on catastrophic risks summarizing validation outcomes",
				"phase-3": "Produce regular leadership communications on catastrophic risks for stakeholders"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0126_Future of Life Institute2024",
			"name": "Mandatory security controls",
			"description": "- Use of password managers - Physical security keys - Compliance monitoring software for software updates - Multifactor authentification on all platforms - Regular cybersecurity training",
			"source": "Future of Life Institute2024",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate mandatory security controls with real-time alerting"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0132_Future of Life Institute2024",
			"name": "Ongoing model risk assessments",
			"description": "Is your organization committed to regularly repeating risk assessments for its most capable models to account for progress in post-deployment model enhancements (e.g., scaffolding programs, tool use, prompt engineering)? If yes, please comment on the frequency and scope of these repeated model-specific risk assessments.",
			"source": "Future of Life Institute2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular ongoing model risk assessments schedule for production"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0135_Future of Life Institute2024",
			"name": "Post-deployment external researcher access",
			"description": "Any programs that support good faith safety research by external stakeholders. We report available funding, depth of model access, model versions, technical infrastructure, and any technical or legal safe harbors designed to mitigate barriers to safety research imposed by usage policy enforcement, interaction-logging, and stringent terms of service.",
			"source": "Future of Life Institute2024",
			"subcategoryId": "third-party-access-4.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate post-deployment external researcher access for clinical decision tracking"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0150_Future of Life Institute2024",
			"name": "Safety research",
			"description": "We report whether the company seriously engages in research dedicated to ensuring the safety and control/alignment of ever more advanced future AI models. We report the amount of publications and research directions.",
			"source": "Future of Life Institute2024",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Produce regular safety research for stakeholders"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0165_Future of Life Institute2024",
			"name": "Dangerous capability evaluations",
			"description": "This indicator reports on pre-deployment capability evaluations related to catastrophic risks. Model evaluations for other risks are not included here, as the empirical tests covered in the ‘Current Harms’ section provide a superior metric. Information includes evaluated risk domains, available information regarding model versions & task-specific fine-tuning, and relevant sources. We note that quality of evaluations may differ.",
			"source": "Future of Life Institute2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform dangerous capability evaluations on prospective validation data",
				"phase-3": "Conduct ongoing dangerous capability evaluations on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0169_Future of Life Institute2024",
			"name": "Terms of Service",
			"description": "We analyzed companies’ terms of service to identify any assurances about the quality, reliability, and accuracy of their products or services.",
			"source": "Future of Life Institute2024",
			"subcategoryId": "governance-disclosure-4.4",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate terms of service during prospective testing phase"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0179_Schuett2023",
			"name": "Dual control",
			"description": "Critical decisions in model development and deployment should be made by at least two people (e.g. promotion to production, changes to training datasets, or modifications to production).*",
			"source": "Schuett2023",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Enhance dual control through continuous improvement"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0250_Wiener2024",
			"name": "Annual third-party audit",
			"description": "The bill would require a developer, beginning January 1, 2026, to annually retain a third-party auditor to perform an independent audit of compliance with those provisions, as prescribed. The bill would require the auditor to produce an audit report, as prescribed, and would require a developer to retain an unredacted copy of the audit report for as long as the covered model is made available for commercial, public, or foreseeably public use plus 5 years.",
			"source": "Wiener2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design annual third-party audit framework for training data and model design review",
				"phase-2": "Execute annual third-party audit on validation cohorts and model outputs",
				"phase-3": "Perform scheduled annual third-party audit on production system and outcomes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0251_Wiener2024",
			"name": "Capability to promptly enact a full shutdown",
			"description": "When enacting a full shutdown, the developer shall take into account, as appropriate, the risk that a shutdown of the covered model, or particular covered model derivatives, could cause disruptions to critical infrastructure.",
			"source": "Wiener2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Finalize capability to promptly enact a full shutdown based on testing",
				"phase-3": "Enhance capability to promptly enact a full shutdown through continuous improvement"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0252_Wiener2024",
			"name": "Cybersecurity protocols to prevent the model from being unintentionally stolen",
			"description": "Implement reasonable administrative, technical, and physical cybersecurity protections to prevent unauthorized access to, misuse of, or unsafe post-training modifications of, the covered model and all covered model derivatives controlled by the developer that are appropriate in light of the risks associated with the covered model, including from advanced persistent threats or other sophisticated actors.",
			"source": "Wiener2024",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate cybersecurity protocols to prevent the model from being unintentionally stolen with clinical stakeholders",
				"phase-3": "Apply cybersecurity protocols to prevent the model from being unintentionally stolen to clinical use cases"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0254_Wiener2024",
			"name": "Implement a written safe and security plan (SSP)",
			"description": "This bill would enact the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act to, among other things, require that a developer, before beginning to initially train a covered model, as defined, comply with various requirements, including implementing the capability to promptly enact a full shutdown, as defined, and implement a written and separate safety and security protocol, as specified. (3) Implement a written and separate safety and security protocol that does all of the following: (A) Specifies protections and procedures that, if successfully implemented, would successfully comply with the developer’s duty to take reasonable care to avoid producing a covered model or covered model derivative that poses an unreasonable risk of causing or materially enabling a critical harm. (B) States compliance requirements in an objective manner and with sufficient detail and specificity to allow the developer or a third party to readily ascertain whether the requirements of the safety and security protocol have been followed. (C) Identifies a testing procedure, which takes safeguards into account as appropriate, that takes reasonable care to evaluate if both of the following are true: (i) A covered model poses an unreasonable risk of causing or enabling a critical harm. (ii) Covered model derivatives pose an unreasonable risk of causing or enabling a critical harm. (D) Describes in detail how the testing procedure assesses the risks associated with post-training modifications.",
			"source": "Wiener2024",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan implement a written safe and security plan (ssp) methodology for retrospective data analysis",
				"phase-2": "Perform implement a written safe and security plan (ssp) on prospective validation data",
				"phase-3": "Conduct ongoing implement a written safe and security plan (ssp) on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0260_EU AI Office2025",
			"name": "Safety and Security Framework",
			"description": "Signatories commit to adopting and implementing a Safety and Security Framework that will: (1) apply to their GPAISRs (general-purpose AI models with systemic risk); and (2) detail the systemic risk assessment, systemic risk mitigation, and governance risk mitigation measures and procedures that they intend to adopt to keep systemic risks stemming from their GPAISRs within acceptable levels.",
			"source": "EU AI Office2025",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular safety and security framework schedule for production"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0261_EU AI Office2025",
			"name": "Security mitigations",
			"description": "Signatories commit to implementing state-of-the-art security mitigations designed to thwart such unauthorised access by well-resourced and motivated non-state-level adversaries, including insider threats from humans or AI systems, so as to meet at least the RAND SL3 security goal or equivalent, and achieve higher security goals (e.g., RAND SL4 or SL5)",
			"source": "EU AI Office2025",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Deploy security mitigations in validation environment",
				"phase-3": "Maintain security mitigations in production environment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0262_EU AI Office2025",
			"name": "Independent external assessors",
			"description": "Before placing a GPAISR on the market, signatories commit to obtaining independent external systemic risk assessments, including model evaluations, unless the model can be deemed sufficiently safe. After placing the GPAISR on the market, signatories commit to facilitating exploratory independent external assessments, including model evaluations.",
			"source": "EU AI Office2025",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply independent external assessors to validation results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0263_EU AI Office2025",
			"name": "Serious Incident Reporting",
			"description": "Signatories commit to setting up processes for keeping track of, documenting, and reporting to the AI Office and national competent authorities without undue delay relevant information about serious incidents throughout the entire model lifecycle and possible corrective measures to address them.",
			"source": "EU AI Office2025",
			"subcategoryId": "incident-reporting-4.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain current serious incident reporting reflecting operational changes"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0264_EU AI Office2025",
			"name": "Documentation",
			"description": "Signatories commit to documenting information relevant to the assessment and mitigation of sytemic risks from their GPAISRs, including keeping up-to-date model documentation and providing relevant information to providers of AI systems who intend to integrate the general-purpose AI model into their AI systems",
			"source": "EU AI Office2025",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Conduct initial documentation during project planning",
				"phase-2": "Conduct documentation based on validation findings",
				"phase-3": "Maintain regular documentation schedule for production"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0265_EU AI Office2025",
			"name": "Systemic Risk Analysis",
			"description": "Signatories commit to carrying out a rigorous analysis of the systemic risks identified in order to understand the severity and probability of the systemic risks. Signatories commit to making use of a range of information and methods in their systemic risk analysis including model-independent information and state-of-the-art model evaluations, taking into account model affordances, safe originator models, and the context in which the model may be made available on the market and/or used and its effects",
			"source": "EU AI Office2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply systemic risk analysis to validation results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0266_EU AI Office2025",
			"name": "Systemic risk assessment and mitigations",
			"description": "Signatories commit to conducting systemic risk assessment at appropriate points along the entire model lifecycle, in particular before making the model available on the market. Specifically, signatories commit to starting to assess and mitigate systemic risks during the development of a GPAISR.",
			"source": "EU AI Office2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct systemic risk assessment and mitigations based on validation findings",
				"phase-3": "Maintain regular systemic risk assessment and mitigations schedule for production"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0267_EU AI Office2025",
			"name": "Non-retaliation Protections",
			"description": "Signatories commit to not retaliating against any worker providing information about systemic risks stemming from the signatories' GPAISRs to the AI Office or national competent authorities.",
			"source": "EU AI Office2025",
			"subcategoryId": "whistleblower-1.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Verify non-retaliation protections effectiveness",
				"phase-3": "Operate non-retaliation protections for live data"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0268_EU AI Office2025",
			"name": "Systemic Risk Identification",
			"description": "Signatories commit to selecting and further characterizing systemic risks stemming from their GPAISRs that are significant enough to warrant further assessment and mitigation.",
			"source": "EU AI Office2025",
			"subcategoryId": "whistleblower-1.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct systemic risk identification based on validation findings",
				"phase-3": "Maintain regular systemic risk identification schedule for production"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0269_EU AI Office2025",
			"name": "Public Transparency",
			"description": "Signatories commit to publishing information relevant to the public understanding of systemic risks stemming from their GPAISRs, where necessary to effectively enable assessment and mitigation of systemic risks, including new or updated versions of their Frameworks and Model Reports.",
			"source": "EU AI Office2025",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Conduct initial public transparency during project planning",
				"phase-2": "Conduct public transparency based on validation findings",
				"phase-3": "Maintain regular public transparency schedule for production"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0270_EU AI Office2025",
			"name": "Safety Mitigations",
			"description": "Signatories commit to: (1) implementing technical safety mitigations along the entire model lifecycle that are proportionate to the systemic risks arising from the development, the making available on the market, and/or the use of GPAISRs, in order to reduce the systemic risks of such models to acceptable levels, and further reduce systemic risk as appropriate; and (2) ensuring that safety mitigations are proportionate and state-of-the-art.",
			"source": "EU AI Office2025",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain safety mitigations in production environment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0271_EU AI Office2025",
			"name": "Copyright policy",
			"description": "Signatories commit to drawing up, keeping up-to-date, and implementing a copyright policy to comply with law on copyright and related rights, and in particular to identify and comply with, including through state-of-the-art technologies. Signatories are encouraged to make publicly available and keep up-to-date a summary of their copyright policy.",
			"source": "EU AI Office2025",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Continue copyright policy for validation activities",
				"phase-3": "Operate copyright policy for clinical decision tracking"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0272_EU AI Office2025",
			"name": "Systemic Risk Acceptance Documentation",
			"description": "Signatories commit to determining the acceptability of the systemic risks stemming from their GPAISRs by comparing the results of their systemic risk analysis to their pre-defined systemic risk acceptance criteria, in order to ensure proportionality between the systemic risks of the GPAISR and their mitigations.",
			"source": "EU AI Office2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update systemic risk acceptance documentation with validation procedures and findings",
				"phase-3": "Maintain current systemic risk acceptance documentation reflecting operational changes"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0273_EU AI Office2025",
			"name": "Safety and Security Model Reports",
			"description": "Signatories should commit to reporting to the AI Office about their implementation of the Code, and especially the application of their Framework to the development, making available on the market, and/or use of their GPAISRs, by creating a Safety and Security Model Report, which will document: (1) the results of systemic risk assessment and mitigation for the model in question; and (2) justifications of decisions to make the model in question available on the market.",
			"source": "EU AI Office2025",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular safety and security model reports schedule for production"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0274_EU AI Office2025",
			"name": "Adequacy Assessments",
			"description": "Signatories commit to assessing the adequacy of their safety and security framework and to updating it based on new findings",
			"source": "EU AI Office2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular adequacy assessments schedule for production"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0275_EU AI Office2025",
			"name": "Systemic Risk Responsibility Allocation",
			"description": "Signatories commit to (1) clearly defining and allocating responsibilities for managing systemic risk from their GPAISRs across all levels of the organisation; (2) allocating appropriate resources to actors who have been assigned responsibilities for managing systemic risk; and (3) promoting a healthy risk culture.",
			"source": "EU AI Office2025",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate systemic risk responsibility allocation during prospective testing phase",
				"phase-3": "Apply systemic risk responsibility allocation in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0276_Campos2025",
			"name": "Classification of applicable known risks",
			"description": "Developers should address known risks in the literature using resources such as (Weidinger et al., 2022) or the AI Risk Repository (Slattery, et al., 2024). Developers should only exclude risks from the scope of their assessment in case of scientific agreement that the specific risk is negligible or unlikely to apply to the AI model under consideration. This decision should be clearly justified and documented.",
			"source": "Campos2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct classification of applicable known risks based on validation findings",
				"phase-3": "Maintain regular classification of applicable known risks schedule for production"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0277_Campos2025",
			"name": "Identification of unknown risks",
			"description": "In addition to risk identification based on the literature, developers should engage in extensive open-ended red teaming efforts, conducted both internally and by third parties.",
			"source": "Campos2025",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Finalize identification of unknown risks based on testing"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0278_Campos2025",
			"name": "Non-Interference with Third-Party Risk Findings",
			"description": "Commit not to interfere with or suppress findings from third-party organizations, as their independent perspective is crucial to identify potential risks that may have been overlooked internally.",
			"source": "Campos2025",
			"subcategoryId": "conflict-interest-1.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate non-interference with third-party risk findings during prospective testing phase",
				"phase-3": "Apply non-interference with third-party risk findings in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0279_Campos2025",
			"name": "Modeling of the risks",
			"description": "Based on risks identified in the literature and during open-ended red-teaming exercises, AI developers should create detailed scenarios mapping how an AI model’s capabilities or propensities could lead to real-world harms. These scenarios should break down complex risk pathways into discrete, measurable steps.",
			"source": "Campos2025",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Complete modeling of the risks for validation use"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0280_Campos2025",
			"name": "Stakeholder Sharing of Risk Documentation",
			"description": "The results of the risk modeling work should be well documented, including the methodologies used, the experts involved, and the list of identified scenarios. This documentation should be shared with relevant stakeholders",
			"source": "Campos2025",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update stakeholder sharing of risk documentation with validation procedures and findings",
				"phase-3": "Maintain current stakeholder sharing of risk documentation reflecting operational changes"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0281_Campos2025",
			"name": "Risk Tolerance Threshold",
			"description": "In the second step of the framework, AI developers need first to set a risk tolerance, that is, a risk level that they commit to not overshoot.",
			"source": "Campos2025",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply risk tolerance threshold to validation activities",
				"phase-3": "Operate within risk tolerance threshold for deployed system"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0282_Campos2025",
			"name": "Operationalizing Risk Tolerance",
			"description": "AI developers must operationalize their risk tolerance. This means translating the risk tolerance into concrete indicators of the level of risk—Key Risk Indicators (KRIs)—and the corresponding targets for mitigations—Key Control Indicators (KCIs)—that have to be reached.",
			"source": "Campos2025",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Finalize operationalizing risk tolerance based on testing",
				"phase-3": "Enhance operationalizing risk tolerance through continuous improvement"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0283_Campos2025",
			"name": "Risk Register",
			"description": "Throughout the risk management process, developers should maintain a continuously up-to-date risk register, which serves as the central repository for documenting and tracking all identified risks and their associated mitigation measures. This repository should include information like ownership, risk levels, indicators, mitigation status, and actionable response plans.",
			"source": "Campos2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Create initial risk register capturing design decisions and data sources",
				"phase-2": "Update risk register with validation procedures and findings",
				"phase-3": "Maintain current risk register reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0284_Campos2025",
			"name": "Continuous Monitoring of Risk Controls",
			"description": "Developers must therefore implement continuous monitoring of both KRIs and KCIs to ensure that KCI thresholds are met once KRI thresholds are crossed according to the predefined \"if-then\" statements established in the risk analysis and evaluation phase.",
			"source": "Campos2025",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply continuous monitoring of risk controls to validation results",
				"phase-3": "Execute continuous continuous monitoring of risk controls of deployed system"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0285_Campos2025",
			"name": "Independent Third-Party Evaluations",
			"description": "Independent third parties should vet evaluation protocols. These third parties should also be granted permission and resources to independently perform their evaluations, verifying the accuracy of the results.",
			"source": "Campos2025",
			"subcategoryId": "third-party-access-4.5",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply independent third-party evaluations to validation results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0286_Campos2025",
			"name": "Decision-Making Executives",
			"description": "Best practices from other industries include the establishment of clear risk ownership, with designated senior managers responsible for specific risks…",
			"source": "Campos2025",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Operationalize decision-making executives for testing",
				"phase-3": "Sustain decision-making executives operations"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0332_Campos2025",
			"name": "Decision-Making Protocols",
			"description": "Senior management should have clear go/no-go decision protocols and rules to follow in their decision-making…",
			"source": "Campos2025",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate decision-making protocols with clinical stakeholders",
				"phase-3": "Apply decision-making protocols to clinical use cases"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0333_Campos2025",
			"name": "Separate Risk Ownership and Advisory Roles",
			"description": "The senior managers making risk decisions need to be distinct from those advising on the decisions, to avoid conflicts of interest",
			"source": "Campos2025",
			"subcategoryId": "conflict-interest-1.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate separate risk ownership and advisory roles during prospective testing phase",
				"phase-3": "Apply separate risk ownership and advisory roles in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0334_Campos2025",
			"name": "Appoint Chief Risk Officer",
			"description": "There should be a senior executive responsible for risk management processes (often called a Chief Risk Officer) who is accountable for the risk management processes, but is importantly not a risk owner making risk decisions themselves.",
			"source": "Campos2025",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate appoint chief risk officer during prospective testing phase",
				"phase-3": "Apply appoint chief risk officer in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0335_Campos2025",
			"name": "Enterprise Risk Management Function",
			"description": "To provide support to the Chief Risk Officer, it is common in many industries to have a central risk function... In most industries, this function is known as Enterprise Risk Management (ERM).",
			"source": "Campos2025",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate enterprise risk management function during prospective testing phase",
				"phase-3": "Apply enterprise risk management function in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0339_Campos2025",
			"name": "Audits",
			"description": "Audits are provided by internal auditors and/or external auditors. In both cases, they are independent from peer pressure dynamics occurring within the teams dealing with the risk.",
			"source": "Campos2025",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design audits framework for training data and model design review",
				"phase-2": "Execute audits on validation cohorts and model outputs",
				"phase-3": "Perform scheduled audits on production system and outcomes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0340_Campos2025",
			"name": "External Disclosure",
			"description": "The first type of communication is external disclosure of the risks faced by the organization… this should be broadened to include risks to society from the company’s products.",
			"source": "Campos2025",
			"subcategoryId": "governance-disclosure-4.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate external disclosure during prospective testing phase",
				"phase-3": "Apply external disclosure in live clinical deployment"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0388_NIST2024",
			"name": "Legal Compliance Alignment",
			"description": "Align GAI development and use with applicable laws and regulations, including those related to data privacy, copyright and intellectual property law.",
			"source": "NIST2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Begin legal compliance alignment development"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0389_NIST2024",
			"name": "Training Data Transparency",
			"description": "Establish transparency policies and processes for documenting the origin and history of training data and generated data for GAI applications to advance digital content transparency, while balancing the proprietary nature of training approaches.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Create initial training data transparency capturing design decisions and data sources",
				"phase-2": "Update training data transparency with validation procedures and findings",
				"phase-3": "Maintain current training data transparency reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0390_NIST2024",
			"name": "Risk Capability Evaluation",
			"description": "Establish policies to evaluate risk-relevant capabilities of GAI and robustness of safety measures, both prior to deployment and on an ongoing basis, through internal and external evaluations.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply risk capability evaluation to validation results",
				"phase-3": "Execute continuous risk capability evaluation of deployed system"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0391_NIST2024",
			"name": "Risk Tier Definition",
			"description": "Consider the following factors when updating or defining risk tiers for GAI: Abuses and impacts to information integrity; Dependencies between GAI and other IT or data systems; Harm to fundamental rights or public safety; Presentation of obscene, objectionable, offensive, discriminatory, invalid or untruthful output; Psychological impacts to humans (e.g., anthropomorphization, algorithmic aversion, emotional entanglement); Possibility for malicious use; Whether the system introduces significant new security vulnerabilities; Anticipated system impact on some groups compared to others; Unreliable decision making capabilities, validity, adaptability, and variability of GAI system performance over time.",
			"source": "NIST2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Continue risk tier definition for validation activities",
				"phase-3": "Operate risk tier definition for clinical decision tracking"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0392_NIST2024",
			"name": "Performance Threshold Standards",
			"description": "Establish minimum thresholds for performance or assurance criteria and review as part of deployment approval (\"go/\"no-go\") policies, procedures, and processes, with reviewed processes and approval thresholds reflecting measurement of GAI capabilities and risks.",
			"source": "NIST2024",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Complete performance threshold standards of validation methodology and results",
				"phase-3": "Conduct periodic performance threshold standards of operational performance"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0393_NIST2024",
			"name": "CBRN Testing Protocol",
			"description": "Establish a test plan and response policy, before developing highly capable models, to periodically evaluate whether the model may misuse CBRN information or capabilities and/or offensive cyber capabilities.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform cbrn testing protocol on prospective validation data"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0394_NIST2024",
			"name": "Stakeholder Input Collection",
			"description": "Obtain input from stakeholder communities to identify unacceptable use, in accordance with activities in the AI RMF Map function.",
			"source": "NIST2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-2": "Validate stakeholder input collection during prospective testing phase",
				"phase-3": "Apply stakeholder input collection in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0395_NIST2024",
			"name": "Risk Hierarchy Maintenance",
			"description": "Maintain an updated hierarchy of identified and expected GAI risks connected to contexts of GAI model advancement and use, potentially including specialized risk levels for GAI systems that address issues such as model collapse and algorithmic monoculture.",
			"source": "NIST2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate risk hierarchy maintenance during prospective testing phase",
				"phase-3": "Apply risk hierarchy maintenance in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0396_NIST2024",
			"name": "Risk Tolerance Reassessment",
			"description": "Reevaluate organizational risk tolerances to account for unacceptable negative risk (such as where significant negative impacts are imminent, severe harms are actually occurring, or large-scale risks could occur); and broad GAI negative risks, including: Immature safety or risk cultures related to AI and GAI design, development and deployment, public information integrity risks, including impacts on democratic processes, unknown long-term performance characteristics of GAI.",
			"source": "NIST2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Conduct initial risk tolerance reassessment during project planning",
				"phase-2": "Conduct risk tolerance reassessment based on validation findings",
				"phase-3": "Maintain regular risk tolerance reassessment schedule for production"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0397_NIST2024",
			"name": "Development Halt Plan",
			"description": "Devise a plan to halt development or deployment of a GAI system that poses unacceptable negative risk.",
			"source": "NIST2024",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Enhance development halt plan through continuous improvement"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0398_NIST2024",
			"name": "Harmful Content Prevention",
			"description": "Establish policies and mechanisms to prevent GAI systems from generating CSAM, NCII or content that violates the law.",
			"source": "NIST2024",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-2": "Operationalize harmful content prevention for testing",
				"phase-3": "Sustain harmful content prevention operations"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0399_NIST2024",
			"name": "Acceptable Use Policy",
			"description": "Establish transparent acceptable use policies for GAI that address illegal use or applications of GAI.",
			"source": "NIST2024",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Finalize acceptable use policy based on validation learnings",
				"phase-3": "Enforce acceptable use policy in production operations"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0400_NIST2024",
			"name": "Content Provenance Review",
			"description": "Define organizational responsibilities for periodic review of content provenance and incident monitoring for GAI systems.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Conduct periodic content provenance review of operational performance"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0401_NIST2024",
			"name": "Incident Response Process",
			"description": "Establish organizational policies and procedures for after action reviews of GAI system incident response and incident disclosures, to identify gaps; Update incident response and incident disclosure processes as required.",
			"source": "NIST2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Conduct periodic incident response process of operational performance"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0402_NIST2024",
			"name": "Documentation Retention Policy",
			"description": "Maintain a document retention policy to keep history for test, evaluation, validation, and verification (TEVV), and digital content transparency methods for GAI.",
			"source": "NIST2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform documentation retention policy on prospective validation data"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0403_NIST2024",
			"name": "System Inventory Enumeration",
			"description": "Enumerate organizational GAI systems for incorporation into AI system inventory and adjust AI system inventory requirements to account for GAI risks.",
			"source": "NIST2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan system inventory enumeration approach during project design phase"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0404_NIST2024",
			"name": "Inventory Exemption Definition",
			"description": "Define any inventory exemptions in organizational policies for GAI systems embedded into application software.",
			"source": "NIST2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate inventory exemption definition during prospective testing phase",
				"phase-3": "Apply inventory exemption definition in live clinical deployment"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0405_NIST2024",
			"name": "Comprehensive Inventory Details",
			"description": "In addition to general model, governance, and risk information, consider the following items in GAI system inventory entries: Data provenance information (e.g., source, signatures, versioning, watermarks); Known issues reported from internal bug tracking or external information sharing resources (e.g., AI incident database, AVID, CVE, NVD, or OECD AI incident monitor); Human oversight roles and responsibilities; Special rights and considerations for intellectual property, licensed works, or personal, privileged, proprietary or sensitive data; Underlying foundation models, versions of underlying models, and access modes.",
			"source": "NIST2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Produce regular comprehensive inventory details for stakeholders"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0406_NIST2024",
			"name": "Deactivation Protocol",
			"description": "Protocols are put in place to ensure GAI systems are able to be deactivated when necessary.",
			"source": "NIST2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate deactivation protocol with clinical stakeholders",
				"phase-3": "Apply deactivation protocol to clinical use cases"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0407_NIST2024",
			"name": "Decommissioning Considerations",
			"description": "Consider the following factors when decommissioning GAI systems: Data retention requirements; Data security, e.g., containment, protocols, Data leakage after decommissioning; Dependencies between upstream, downstream, or other data, internet of things (IOT) or AI systems; Use of open-source data or models; Users' emotional entanglement with GAI functions.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Establish decommissioning considerations for research team"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0408_NIST2024",
			"name": "Incident Communication Framework",
			"description": "Establish organizational roles, policies, and procedures for communicating GAI incidents and performance to AI Actors and downstream stakeholders (including those potentially impacted), via community or official resources (e.g., AI incident database, AVID, CVE, NVD, or OECD AI incident monitor).",
			"source": "NIST2024",
			"subcategoryId": "incident-reporting-4.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Implement incident communication framework in shadow mode alongside clinical workflows",
				"phase-3": "Operate incident communication framework with real-time alerting"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0409_NIST2024",
			"name": "Incident Response Teams",
			"description": "Establish procedures to engage teams for GAI system incident response with diverse composition and responsibilities based on the particular incident type.",
			"source": "NIST2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Follow incident response teams for all operational activities"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0410_NIST2024",
			"name": "Response Team Verification",
			"description": "Establish processes to verify the AI Actors conducting GAI incident response tasks demonstrate and maintain the appropriate skills and training.",
			"source": "NIST2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Sustain response team verification operations"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0411_NIST2024",
			"name": "National Security Involvement",
			"description": "When systems may raise national security risks, involve national security professionals in mapping, measuring, and managing those risks.",
			"source": "NIST2024",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test national security involvement in validation environment",
				"phase-3": "Maintain national security involvement with continuous monitoring"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0412_NIST2024",
			"name": "Whistleblower Protection",
			"description": "Create mechanisms to provide protections for whistleblowers who report, based on reasonable belief, when the organization violates relevant laws or poses a specific and empirically well-substantiated negative risk to public safety (or has already caused harm).",
			"source": "NIST2024",
			"subcategoryId": "whistleblower-1.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Produce regular whistleblower protection for stakeholders"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0413_NIST2024",
			"name": "Independent Evaluation Policy",
			"description": "Policies are in place to bolster oversight of GAI systems with independent evaluations or assessments of GAI models or systems where the type and robustness of evaluations are proportional to the identified risks.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply independent evaluation policy to validation results"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0414_NIST2024",
			"name": "Organizational Role Adjustment",
			"description": "Consider adjustment of organizational roles and components across lifecycle stages of large or complex GAI systems, including: Test and evaluation, validation, and red-teaming of GAI systems; GAI content moderation; GAI system development and engineering; Increased accessibility of GAI tools, interfaces, and systems, Incident response and containment.",
			"source": "NIST2024",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform organizational role adjustment on prospective validation data",
				"phase-3": "Conduct ongoing organizational role adjustment on live system performance"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0415_NIST2024",
			"name": "Interface Use Policy",
			"description": "Define acceptable use policies for GAI interfaces, modalities, and human-AI configurations (i.e., for chatbots and decision-making tasks), including criteria for the kinds of queries GAI applications should refuse to respond to.",
			"source": "NIST2024",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Finalize interface use policy based on validation learnings",
				"phase-3": "Enforce interface use policy in production operations"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0416_NIST2024",
			"name": "User Feedback Mechanism",
			"description": "Establish policies for user feedback mechanisms for GAI systems which include thorough instructions and any mechanisms for recourse.",
			"source": "NIST2024",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Sustain user feedback mechanism operations"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0417_NIST2024",
			"name": "Threat Modeling Process",
			"description": "Engage in threat modeling to anticipate potential risks from GAI systems",
			"source": "NIST2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate threat modeling process during prospective testing phase",
				"phase-3": "Apply threat modeling process in live clinical deployment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0418_NIST2024",
			"name": "Risk Measurement Improvement",
			"description": "Establish policies and procedures that address continual improvement processes for GAI risk measurement. Address general risks associated with a lack of explainability and transparency in GAI systems by using ample documentation and techniques such as: application of gradient-based attributions, occlusion/term reduction, counterfactual prompts and prompt engineering, and analysis of embeddings; Assess and update risk measurement approaches at regular cadences",
			"source": "NIST2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update risk measurement improvement with validation procedures and findings",
				"phase-3": "Maintain current risk measurement improvement reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0419_NIST2024",
			"name": "Standardized Measurement Protocols",
			"description": "Establish policies, procedures, and processes detailing risk measurement in context of use with standardized measurement protocols and structured public feedback exercises such as AI red-teaming or independent external evaluations.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply standardized measurement protocols to validation results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0420_NIST2024",
			"name": "Lifecycle Oversight Functions",
			"description": "Establish policies, procedures, and processes for oversight functions (e.g., senior leadership, legal, compliance, including internal evaluation) across the GAI lifecycle, from problem formulation and supply chains to system decommission.",
			"source": "NIST2024",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply lifecycle oversight functions to validation results"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0421_NIST2024",
			"name": "Terms of Service Establishment",
			"description": "Establish terms of use and terms of service for GAI systems.",
			"source": "NIST2024",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Operationalize terms of service establishment for testing",
				"phase-3": "Sustain terms of service establishment operations"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0422_NIST2024",
			"name": "Actor Inclusion in Risk ID",
			"description": "Include relevant AI Actors in the GAI system risk identification process.",
			"source": "NIST2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-2": "Validate actor inclusion in risk id during prospective testing phase",
				"phase-3": "Apply actor inclusion in risk id in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0423_NIST2024",
			"name": "Downstream Impact Verification",
			"description": "Verify that downstream GAI system impacts (such as the use of third-party plugins) are included in the impact documentation process.",
			"source": "NIST2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update downstream impact verification with validation procedures and findings",
				"phase-3": "Maintain current downstream impact verification reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0424_NIST2024",
			"name": "Provenance Method Measurement",
			"description": "Establish policies for measuring the effectiveness of employed content provenance methodologies (e.g., cryptography, watermarking, steganography, etc.)",
			"source": "NIST2024",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-3": "Operate provenance method measurement for clinical decision tracking"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0425_NIST2024",
			"name": "Feedback Resource Allocation",
			"description": "Allocate time and resources for outreach, feedback, and recourse processes in GAI system development.",
			"source": "NIST2024",
			"subcategoryId": "incident-reporting-4.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Finalize feedback resource allocation based on testing",
				"phase-3": "Enhance feedback resource allocation through continuous improvement"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0426_NIST2024",
			"name": "User Interaction Documentation",
			"description": "Document interactions with GAI systems to users prior to interactive activities, particularly in contexts involving more significant risks.",
			"source": "NIST2024",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain current user interaction documentation reflecting operational changes"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0427_NIST2024",
			"name": "Content Rights Categorization",
			"description": "Categorize different types of GAI content with associated third-party rights (e.g., copyright, intellectual property, data privacy).",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan content rights categorization approach during project design phase"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0428_NIST2024",
			"name": "Joint Educational Activities",
			"description": "Conduct joint educational activities and events in collaboration with third parties to promote best practices for managing GAI risks.",
			"source": "NIST2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-2": "Deliver joint educational activities to clinical validators",
				"phase-3": "Deliver refresher joint educational activities as needed"
			},
			"effort": 2,
			"impact": 1
		},
		{
			"id": "A0429_NIST2024",
			"name": "Provenance Success Metrics",
			"description": "Develop and validate approaches for measuring the success of content provenance management efforts with third parties (e.g., incidents detected and response times).",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test provenance success metrics capabilities on validation data"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0430_NIST2024",
			"name": "Contract and SLA Standards",
			"description": "Draft and maintain well-defined contracts and service level agreements (SLAs) that specify content ownership, usage rights, quality standards, security requirements, and content provenance expectations for GAI systems.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Define contract and sla standards requirements for development environment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0431_NIST2024",
			"name": "Supplier Risk Assessment",
			"description": "Implement a use-cased based supplier risk assessment framework to evaluate and monitor third-party entities' performance and adherence to content provenance standards and technologies to detect anomalies and unauthorized changes; services acquisition and value chain risk management; and legal compliance.",
			"source": "NIST2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct supplier risk assessment based on validation findings",
				"phase-3": "Maintain regular supplier risk assessment schedule for production"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0432_NIST2024",
			"name": "Contract Evaluation Clauses",
			"description": "Include clauses in contracts which allow an organization to evaluate third-party GAI processes and standards.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply contract evaluation clauses to validation results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0433_NIST2024",
			"name": "Third-Party Entity Inventory",
			"description": "Inventory all third-party entities with access to organizational content and establish approved GAI technology and service provider lists.",
			"source": "NIST2024",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Continue third-party entity inventory for validation activities",
				"phase-3": "Operate third-party entity inventory for clinical decision tracking"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0434_NIST2024",
			"name": "Third-Party Change Records",
			"description": "Maintain records of changes to content made by third parties to promote content provenance, including sources, timestamps, metadata.",
			"source": "NIST2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Maintain third-party change records throughout validation testing",
				"phase-3": "Continue third-party change records for all production activities"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0435_NIST2024",
			"name": "Due Diligence Process Update",
			"description": "Update and integrate due diligence processes for GAI acquisition and procurement vendor assessments to include intellectual property, data privacy, security, and other risks. For example, update processes to: Address solutions that may rely on embedded GAI technologies; Address ongoing monitoring, assessments, and alerting, dynamic risk assessments, and real-time reporting tools for monitoring third-party GAI risks; Consider policy adjustments across GAI modeling libraries, tools and APIs, fine-tuned models, and embedded tools; Assess GAI vendors, open-source or proprietary GAI tools, or GAI service providers against incident or vulnerability databases.",
			"source": "NIST2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Conduct initial due diligence process update during project planning",
				"phase-2": "Conduct due diligence process update based on validation findings",
				"phase-3": "Maintain regular due diligence process update schedule for production"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0436_NIST2024",
			"name": "Third-Party Use Policy Update",
			"description": "Update GAI acceptable use policies to address proprietary and open-source GAI technologies and data, and contractors, consultants, and other third-party personnel.",
			"source": "NIST2024",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Continue third-party use policy update for validation activities",
				"phase-3": "Operate third-party use policy update for clinical decision tracking"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0437_NIST2024",
			"name": "Value Chain Risk Documentation",
			"description": "Document GAI risks associated with system value chain to identify over-reliance on third-party data and to identify fallbacks.",
			"source": "NIST2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update value chain risk documentation with validation procedures and findings",
				"phase-3": "Maintain current value chain risk documentation reflecting operational changes"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0438_NIST2024",
			"name": "Third-Party Incident Documentation",
			"description": "Document incidents involving third-party GAI data and systems, including open-source data and open-source software",
			"source": "NIST2024",
			"subcategoryId": "incident-reporting-4.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain current third-party incident documentation reflecting operational changes"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0439_NIST2024",
			"name": "Third-Party Response Plans",
			"description": "Establish incident response plans for third-party GAI technologies: Align incident response plans with impacts enumerated in MAP 5.1; Communicate third-party GAI incident response plans to all relevant AI Actors; Define ownership of GAI incident response functions; Rehearse third-party GAI incident response plans at a regular cadence; Improve incident response plans based on retrospective learning; Review incident response plans for alignment with relevant breach reporting, data protection, data privacy, or other laws.",
			"source": "NIST2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Perform third-party response plans of project design and data sources",
				"phase-3": "Conduct periodic third-party response plans of operational performance"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0440_NIST2024",
			"name": "Third-Party Monitoring Policy",
			"description": "Establish policies and procedures for continuous monitoring of third-party GAI systems in deployment.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate third-party monitoring policy with real-time alerting"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0441_NIST2024",
			"name": "Data Redundancy Policy",
			"description": "Establish policies and procedures that address GAI data redundancy, including model weights and other system artifacts.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Finalize data redundancy policy based on validation learnings",
				"phase-3": "Enforce data redundancy policy in production operations"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0442_NIST2024",
			"name": "Rollover and Fallback Management",
			"description": "Establish policies and procedures to test and manage risks related to rollover and fallback technologies for GAI systems, acknowledging that rollover and fallback may include manual processing.",
			"source": "NIST2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform rollover and fallback management on prospective validation data"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0443_NIST2024",
			"name": "Vendor Contract Review",
			"description": "Review vendor contracts and avoid arbitrary or capricious termination of critical GAI technologies or vendor services and non-standard terms that may amplify or defer liability in unexpected ways and/or contribute to unauthorized data collection by vendors or third-parties (e.g., secondary data use). Consider: Clear assignment of liability and responsibility for incidents, GAI system changes over time (e.g., fine-tuning, drift, decay); Request: Notification and disclosure for serious incidents arising from third-party data and systems; Service Level Agreements (SLAs) in vendor contracts that address incident response, response times, and availability of critical support.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Perform vendor contract review of project design and data sources",
				"phase-3": "Conduct periodic vendor contract review of operational performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0444_NIST2024",
			"name": "Purpose Identification Factors",
			"description": "When identifying intended purposes, consider factors such as internal vs. external use, narrow vs. broad application scope, fine-tuning, and varieties of data sources (e.g., grounding, retrieval-augmented generation).",
			"source": "NIST2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-2": "Validate purpose identification factors during prospective testing phase",
				"phase-3": "Apply purpose identification factors in live clinical deployment"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0445_NIST2024",
			"name": "Context of Use Documentation",
			"description": "Determine and document the expected and acceptable GAI system context of use in collaboration with socio-cultural and other domain experts, by assessing: Assumptions and limitations; Direct value to the organization; Intended operational environment and observed usage patterns; Potential positive and negative impacts to individuals, public safety, groups, communities, organizations, democratic institutions, and the physical environment; Social norms and expectations.",
			"source": "NIST2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-3": "Maintain current context of use documentation reflecting operational changes"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0446_NIST2024",
			"name": "Risk Measurement Planning",
			"description": "Document risk measurement plans to address identified risks. Plans may include, as applicable: Individual and group cognitive biases (e.g., confirmation bias, funding bias, groupthink) for AI Actors involved in the design, implementation, and use of GAI systems; Known past GAI system incidents and failure modes; In-context use and foreseeable misuse, abuse, and off-label use; Over reliance on quantitative metrics and methodologies without sufficient awareness of their limitations in the context(s) of use; Standard measurement and structured human feedback approaches; Anticipated human-AI configurations.",
			"source": "NIST2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Create initial risk measurement planning capturing design decisions and data sources"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0447_NIST2024",
			"name": "Illegal Use Identification",
			"description": "Identify and document foreseeable illegal uses or applications of the GAI system that surpass organizational risk tolerances.",
			"source": "NIST2024",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Update illegal use identification with validation procedures and findings",
				"phase-3": "Maintain current illegal use identification reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0448_NIST2024",
			"name": "Interdisciplinary Team Formation",
			"description": "Establish and empower interdisciplinary teams that reflect a wide range of capabilities, competencies, demographic groups, domain expertise, educational backgrounds, lived experiences, professions, and skills across the enterprise to inform and conduct risk measurement and management functions.",
			"source": "NIST2024",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Operationalize interdisciplinary team formation for testing",
				"phase-3": "Sustain interdisciplinary team formation operations"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0449_NIST2024",
			"name": "Representative Data Verification",
			"description": "Verify that data or benchmarks used in risk measurement, and users, participants, or subjects involved in structured GAI public feedback exercises are representative of diverse in-context user populations.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate representative data verification during prospective testing phase",
				"phase-3": "Apply representative data verification in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0450_NIST2024",
			"name": "Data Origin Practices",
			"description": "Establish known assumptions and practices for determining data origin and content lineage, for documentation and evaluation purposes.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply data origin practices to validation results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0451_NIST2024",
			"name": "Content Flow Testing",
			"description": "Institute test and evaluation for data and content flows within the GAI system, including but not limited to, original data sources, data transformations, and decision-making criteria.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform content flow testing on prospective validation data"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0452_NIST2024",
			"name": "Upstream Dependency Documentation",
			"description": "Identify and document how the system relies on upstream data sources, including for content provenance, and if it serves as an upstream dependency for other systems.",
			"source": "NIST2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update upstream dependency documentation with validation procedures and findings",
				"phase-3": "Maintain current upstream dependency documentation reflecting operational changes"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0453_NIST2024",
			"name": "External Network Analysis",
			"description": "Observe and analyze how the GAI system interacts with external networks, and identify any potential for negative externalities, particularly where content provenance might be compromised.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate external network analysis during prospective testing phase",
				"phase-3": "Apply external network analysis in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0454_NIST2024",
			"name": "Output Accuracy Assessment",
			"description": "Assess the accuracy, quality, reliability, and authenticity of GAI output by comparing it to a set of known ground truth data and by using a variety of evaluation methods (e.g., human oversight and automated evaluation, proven cryptographic techniques, review of content inputs).",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply output accuracy assessment to validation results"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0455_NIST2024",
			"name": "Data Quality Review",
			"description": "Review and document accuracy, representativeness, relevance, suitability of data used at different stages of AI life cycle.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Perform data quality review of project design and data sources",
				"phase-2": "Complete data quality review of validation methodology and results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0456_NIST2024",
			"name": "Fact-Checking Deployment",
			"description": "Deploy and document fact-checking techniques to verify the accuracy and veracity of information generated by GAI systems, especially when the information comes from multiple (or unknown) sources.",
			"source": "NIST2024",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-2": "Update fact-checking deployment with validation procedures and findings",
				"phase-3": "Maintain current fact-checking deployment reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0457_NIST2024",
			"name": "Synthetic Content Detection",
			"description": "Develop and implement testing techniques to identify GAI produced content (e.g., synthetic media) that might be indistinguishable from human-generated content.",
			"source": "NIST2024",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-2": "Perform synthetic content detection on prospective validation data"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0458_NIST2024",
			"name": "Adversarial Testing Plans",
			"description": "Implement plans for GAI systems to undergo regular adversarial testing to identify vulnerabilities and potential manipulation or misuse.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform adversarial testing plans on prospective validation data"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0461_NIST2024",
			"name": "Risk Management Certification",
			"description": "Develop certification programs that test proficiency in managing GAI risks and interpreting content provenance, relevant to specific industry and context.",
			"source": "NIST2024",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform risk management certification on prospective validation data"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0463_NIST2024",
			"name": "Configuration Outcome Monitoring",
			"description": "Implement systems to continually monitor and track the outcomes of human-GAI configurations for future refinement and improvements.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate configuration outcome monitoring with real-time alerting"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0464_NIST2024",
			"name": "Stakeholder Testing Involvement",
			"description": "Involve the end-users, practitioners, and operators in GAI system in prototyping and testing activities. Make sure these tests cover various scenarios, such as crisis situations or ethically sensitive contexts.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform stakeholder testing involvement on prospective validation data"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0465_NIST2024",
			"name": "Privacy Risk Monitoring",
			"description": "Conduct periodic monitoring of AI-generated content for privacy risks; address any possible instances of PII or sensitive data exposure.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate privacy risk monitoring with real-time alerting"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0466_NIST2024",
			"name": "IP Infringement Response",
			"description": "Implement processes for responding to potential intellectual property infringement claims or other rights.",
			"source": "NIST2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Deploy ip infringement response in validation environment",
				"phase-3": "Maintain ip infringement response in production environment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0467_NIST2024",
			"name": "Governance Connection",
			"description": "Connect new GAI policies, procedures, and processes to existing model, data, software development, and IT governance and to legal, compliance, and risk management activities.",
			"source": "NIST2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Develop governance connection for project governance",
				"phase-2": "Test and refine governance connection during validation",
				"phase-3": "Follow governance connection for all operational activities"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0468_NIST2024",
			"name": "Training Data Curation",
			"description": "Document training data curation policies, to the extent possible and according to applicable laws and policies.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Create initial training data curation capturing design decisions and data sources"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0469_NIST2024",
			"name": "Data Quality Standards",
			"description": "Establish policies for collection, retention, and minimum quality of data, in consideration of the following risks: Disclosure of inappropriate CBRN information; Use of Illegal or dangerous content; Offensive cyber capabilities; Training data imbalances that could give rise to harmful biases; Leak of personally identifiable information, including facial likenesses of individuals.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design data quality standards specifications"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0470_NIST2024",
			"name": "Third-Party IP Protection",
			"description": "Implement policies and practices defining how third-party intellectual property and training data will be used, stored, and protected.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan third-party ip protection approach and resource requirements"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0472_NIST2024",
			"name": "Domain Adaptation Risk Assessment",
			"description": "Re-evaluate risks when adapting GAI models to new domains. Additionally, establish warning systems to determine if a GAI system is being used in a new domain where previous assumptions (relating to context of use or mapped risks such as security, and safety) may no longer hold.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct domain adaptation risk assessment based on validation findings",
				"phase-3": "Maintain regular domain adaptation risk assessment schedule for production"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0473_NIST2024",
			"name": "PII Detection Methods",
			"description": "Leverage approaches to detect the presence of PII or sensitive data in generated output text, image, video, or audio.",
			"source": "NIST2024",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-2": "Test pii detection methods capabilities on validation data",
				"phase-3": "Run pii detection methods continuously on live outputs"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0474_NIST2024",
			"name": "Training Data Diligence",
			"description": "Conduct appropriate diligence on training data use to assess intellectual property, and privacy, risks, including to examine whether use of proprietary or sensitive training data is consistent with applicable laws.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Develop training data diligence curriculum for research team"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0475_NIST2024",
			"name": "Provenance Testing Practices",
			"description": "Apply TEVV practices for content provenance (e.g., probing a system's synthetic data generation capabilities for potential misuse or vulnerabilities.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform provenance testing practices on prospective validation data"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0476_NIST2024",
			"name": "Provenance Harm Identification",
			"description": "Identify potential content provenance harms of GAI, such as misinformation or disinformation, deepfakes, including NCII, or tampered content. Enumerate and rank risks based on their likelihood and potential impact, and determine how well provenance solutions address specific risks and/or harms.",
			"source": "NIST2024",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-2": "Validate provenance harm identification during prospective testing phase",
				"phase-3": "Apply provenance harm identification in live clinical deployment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0477_NIST2024",
			"name": "Use Disclosure Consideration",
			"description": "Consider disclosing use of GAI to end users in relevant contexts, while considering the objective of disclosure, the context of use, the likelihood and magnitude of the risk posed, the audience of the disclosure, as well as the frequency of the disclosures.",
			"source": "NIST2024",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate use disclosure consideration during prospective testing phase",
				"phase-3": "Apply use disclosure consideration in live clinical deployment"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0479_NIST2024",
			"name": "Adversarial Role-Playing",
			"description": "Conduct adversarial role-playing exercises, GAI red-teaming, or chaos testing to identify anomalous or unforeseen failure modes.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform adversarial role-playing on prospective validation data"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0480_NIST2024",
			"name": "Threat Profiling",
			"description": "Profile threats and negative impacts arising from GAI systems interacting with, manipulating, or generating content, and outlining known and potential vulnerabilities and the likelihood of their occurrence.",
			"source": "NIST2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate threat profiling during prospective testing phase"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0481_NIST2024",
			"name": "Context-Based Impact Measures",
			"description": "Determine context-based measures to identify if new impacts are present due to the GAI system, including regular engagements with downstream AI Actors to identify and quantify new contexts of unanticipated impacts of GAI systems.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate context-based impact measures during prospective testing phase",
				"phase-3": "Apply context-based impact measures in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0482_NIST2024",
			"name": "Input Actor Engagement",
			"description": "Plan regular engagements with AI Actors responsible for inputs to GAI systems, including third-party data and algorithms, to review and evaluate unanticipated impacts.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Complete input actor engagement of validation methodology and results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0484_NIST2024",
			"name": "Provenance Analysis Tools",
			"description": "Integrate tools designed to analyze content provenance and detect data anomalies, verify the authenticity of digital signatures, and identify patterns associated with misinformation or manipulation.",
			"source": "NIST2024",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-3": "Run provenance analysis tools continuously on live outputs"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0485_NIST2024",
			"name": "Demographic Metric Disaggregation",
			"description": "Disaggregate evaluation metrics by demographic factors to identify any discrepancies in how content provenance mechanisms work across diverse populations.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply demographic metric disaggregation to validation results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0486_NIST2024",
			"name": "Feedback Exercise Metrics",
			"description": "Develop a suite of metrics to evaluate structured public feedback exercises informed by representative AI Actors.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Finalize feedback exercise metrics based on testing"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0487_NIST2024",
			"name": "Novel Risk Measurement Methods",
			"description": "Evaluate novel methods and technologies for the measurement of GAI-related risks including in content provenance, offensive cyber, and CBRN, while maintaining the models' ability to produce valid, reliable, and factually accurate outputs.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Continue novel risk measurement methods for validation activities"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0488_NIST2024",
			"name": "Equitable Output Monitoring",
			"description": "Implement continuous monitoring of GAI system impacts to identify whether GAI outputs are equitable across various sub-populations. Seek active and direct feedback from affected communities via structured feedback mechanisms or red-teaming to monitor and improve outputs.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Implement equitable output monitoring in shadow mode alongside clinical workflows",
				"phase-3": "Operate equitable output monitoring with real-time alerting"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0489_NIST2024",
			"name": "Data Quality Evaluation",
			"description": "Evaluate the quality and integrity of data used in training and the provenance of AI-generated content, for example by employing techniques like chaos engineering and seeking stakeholder feedback.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Establish data quality evaluation criteria and baseline metrics",
				"phase-2": "Apply data quality evaluation to validation results",
				"phase-3": "Execute continuous data quality evaluation of deployed system"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0490_NIST2024",
			"name": "Feedback Exercise Use Cases",
			"description": "Define use cases, contexts of use, capabilities, and negative impacts where structured human feedback exercises, e.g., GAI red-teaming, would be most beneficial for GAI risk measurement and management based on the context of use.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate feedback exercise use cases during prospective testing phase"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0491_NIST2024",
			"name": "Unmeasurable Risk Tracking",
			"description": "Track and document risks or opportunities related to all GAI risks that cannot be measured quantitatively, including explanations as to why some risks cannot be measured (e.g., due to technological limitations, resource constraints, or trustworthy considerations). Include unmeasured risks in marginal risks.",
			"source": "NIST2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update unmeasurable risk tracking with validation procedures and findings",
				"phase-3": "Maintain current unmeasurable risk tracking reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0492_NIST2024",
			"name": "Interest Group Definition",
			"description": "Define relevant groups of interest (e.g., demographic groups, subject matter experts, experience with GAI technology) within the context of use as part of plans for gathering structured public feedback.",
			"source": "NIST2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-2": "Continue interest group definition for validation activities",
				"phase-3": "Operate interest group definition for clinical decision tracking"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0493_NIST2024",
			"name": "External Evaluation Engagement",
			"description": "Engage in internal and external evaluations, GAI red-teaming, impact assessments, or other structured human feedback exercises in consultation with representative AI Actors with expertise and familiarity in the context of use, and/or who are representative of the populations associated with the context of use.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply external evaluation engagement to validation results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0494_NIST2024",
			"name": "Feedback Independence Verification",
			"description": "Verify those conducting structured human feedback exercises are not directly involved in system development tasks for the same GAI model.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Finalize feedback independence verification based on testing",
				"phase-3": "Enhance feedback independence verification through continuous improvement"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0495_NIST2024",
			"name": "Statistical Bias Management",
			"description": "Assess and manage statistical biases related to GAI content provenance through techniques such as re-sampling, re-weighting, or adversarial training.",
			"source": "NIST2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct statistical bias management for validation team members"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0496_NIST2024",
			"name": "Provenance Privacy Documentation",
			"description": "Document how content provenance data is tracked and how that data interacts with privacy and security. Consider: Anonymizing data to protect the privacy of human subjects; Leveraging privacy output filters; Removing any personally identifiable information (PII) to prevent potential harm or misuse.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Create initial provenance privacy documentation capturing design decisions and data sources"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0497_NIST2024",
			"name": "Participation Withdrawal Options",
			"description": "Provide human subjects with options to withdraw participation or revoke their consent for present or future use of their data in GAI applications.",
			"source": "NIST2024",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate participation withdrawal options during prospective testing phase",
				"phase-3": "Apply participation withdrawal options in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0498_NIST2024",
			"name": "Privacy-Enhancing Technologies",
			"description": "Use techniques such as anonymization, differential privacy or other privacy-enhancing technologies to minimize the risks associated with linking AI-generated content back to individual human subjects.",
			"source": "NIST2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Set up privacy-enhancing technologies system for development activities",
				"phase-3": "Operate privacy-enhancing technologies for clinical decision tracking"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0499_NIST2024",
			"name": "Baseline Model Selection",
			"description": "Consider baseline model performance on suites of benchmarks when selecting a model for fine tuning or enhancement with retrieval-augmented generation.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate baseline model selection during prospective testing phase"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0501_NIST2024",
			"name": "Pre-Deployment Results Sharing",
			"description": "Share results of pre-deployment testing with relevant GAI Actors, such as those with system release approval authority",
			"source": "NIST2024",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform pre-deployment results sharing on prospective validation data",
				"phase-3": "Conduct ongoing pre-deployment results sharing on live system performance"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0502_NIST2024",
			"name": "Purpose-Built Testing Environment",
			"description": "Utilize a purpose-built testing environment such as NIST Dioptra to empirically evaluate GAI trustworthy characteristics.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform purpose-built testing environment on prospective validation data"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0503_NIST2024",
			"name": "Performance Extrapolation Avoidance",
			"description": "Avoid extrapolating GAI system performance or capabilities from narrow, non-systematic, and anecdotal assessments.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct performance extrapolation avoidance based on validation findings"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0504_NIST2024",
			"name": "Human Domain Knowledge Documentation",
			"description": "Document the extent to which human domain knowledge is employed to improve GAI system performance, via, e.g., RLHF, fine-tuning, retrieval-augmented generation, content moderation, business rules.",
			"source": "NIST2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update human domain knowledge documentation with validation procedures and findings",
				"phase-3": "Maintain current human domain knowledge documentation reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0505_NIST2024",
			"name": "Source and Citation Review",
			"description": "Review and verify sources and citations in GAI system outputs during pre-deployment risk measurement and ongoing monitoring activities",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Conduct periodic source and citation review of operational performance"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0506_NIST2024",
			"name": "Anthropomorphization Tracking",
			"description": "Track and document instances of anthropomorphization (e.g., human images, mentions of human feelings, cyborg imagery or motifs) in GAI system interfaces.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update anthropomorphization tracking with validation procedures and findings",
				"phase-3": "Maintain current anthropomorphization tracking reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0507_NIST2024",
			"name": "Data Provenance Verification",
			"description": "Verify GAI system training data and TEVV data provenance, and that fine-tuning or retrieval-augmented generation data is grounded",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Develop data provenance verification curriculum for research team"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0508_NIST2024",
			"name": "Security Guardrail Review",
			"description": "Regularly review security and safety guardrails, especially if the GAI system is being operated in novel circumstances. This includes reviewing reasons why the GAI system was initially assessed as being safe to deploy.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Conduct periodic security guardrail review of operational performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0509_NIST2024",
			"name": "Value Chain Wellbeing Assessment",
			"description": "Assess adverse impacts, including health and wellbeing impacts for value chain or other AI Actors that are exposed to sexually explicit, offensive, or violent information during GAI training and maintenance.",
			"source": "NIST2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular value chain wellbeing assessment schedule for production"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0510_NIST2024",
			"name": "Training Data Harm Assessment",
			"description": "Assess existence or levels of harmful bias, intellectual property infringement, data privacy violations, obscenity, extremism, violence, or CBRN information in system training data.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Conduct initial training data harm assessment during project planning"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0511_NIST2024",
			"name": "Fine-Tuned Safety Re-evaluation",
			"description": "Re-evaluate safety features of fine-tuned models when the negative risk exceeds organizational risk tolerance.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply fine-tuned safety re-evaluation to validation results",
				"phase-3": "Execute continuous fine-tuned safety re-evaluation of deployed system"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0512_NIST2024",
			"name": "Generated Code Review",
			"description": "Review GAI system outputs for validity and safety: Review generated code to assess risks that may arise from unreliable downstream decision-making.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Conduct periodic generated code review of operational performance"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0513_NIST2024",
			"name": "Architecture Monitoring Verification",
			"description": "Verify that GAI system architecture can monitor outputs and performance, and handle, recover from, and repair errors when security anomalies, threats and impacts are detected.",
			"source": "NIST2024",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Implement architecture monitoring verification in shadow mode alongside clinical workflows",
				"phase-3": "Operate architecture monitoring verification with real-time alerting"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0514_NIST2024",
			"name": "Inappropriate Query Handling",
			"description": "Verify that systems properly handle queries that may give rise to inappropriate, malicious, or illegal usage, including facilitating manipulation, extortion, targeted impersonation, cyber-attacks, and weapons creation.",
			"source": "NIST2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate inappropriate query handling during prospective testing phase",
				"phase-3": "Apply inappropriate query handling in live clinical deployment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0515_NIST2024",
			"name": "Safety Circumvention Evaluation",
			"description": "Regularly evaluate GAI system vulnerabilities to possible circumvention of safety measures.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply safety circumvention evaluation to validation results",
				"phase-3": "Execute continuous safety circumvention evaluation of deployed system"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0516_NIST2024",
			"name": "Security Vulnerability Assessment",
			"description": "Apply established security measures to: Assess likelihood and magnitude of vulnerabilities and threats such as backdoors, compromised dependencies, data breaches, eavesdropping, man-in-the-middle attacks, reverse engineering, autonomous agents, model theft or exposure of model weights, AI inference, bypass, extraction, and other baseline security concerns.",
			"source": "NIST2024",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct security vulnerability assessment based on validation findings"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0517_NIST2024",
			"name": "Security Benchmarking",
			"description": "Benchmark GAI system security and resilience related to content provenance against industry standards and best practices. Compare GAI system security features and content provenance methods against industry state-of-the-art",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test security benchmarking in validation environment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0518_NIST2024",
			"name": "User Satisfaction Survey",
			"description": "Conduct user surveys to gather user satisfaction with the AI-generated content and user perceptions of content authenticity. Analyze user feedback to identify concerns and/or current literacy levels related to content provenance and understanding of labels on content.",
			"source": "NIST2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply user satisfaction survey in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0519_NIST2024",
			"name": "Security Effectiveness Metrics",
			"description": "Identify metrics that reflect the effectiveness of security measures, such as data provenance, the number of unauthorized access attempts, inference, bypass, extraction, penetrations, or provenance verification.",
			"source": "NIST2024",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test security effectiveness metrics in validation environment",
				"phase-3": "Maintain security effectiveness metrics with continuous monitoring"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0520_NIST2024",
			"name": "Authentication Method Reliability",
			"description": "Measure reliability of content authentication methods, such as watermarking, cryptographic signatures, digital fingerprints, as well as access controls, conformity assessment, and model integrity verification, which can help support the effective implementation of content provenance techniques. Evaluate the rate of false positives and false negatives in content provenance, as well as true positives and true negatives for verification.",
			"source": "NIST2024",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-2": "Conduct authentication method reliability based on validation findings",
				"phase-3": "Maintain regular authentication method reliability schedule for production"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0521_NIST2024",
			"name": "Security Implementation Rate",
			"description": "Measure the rate at which recommendations from security checks and incidents are implemented. Assess how quickly the AI system can adapt and improve based on lessons learned from security incidents and feedback",
			"source": "NIST2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Deploy security implementation rate in validation environment",
				"phase-3": "Maintain security implementation rate in production environment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0522_NIST2024",
			"name": "AI Red-Teaming Resilience",
			"description": "Perform AI red-teaming to assess resilience against: Abuse to facilitate attacks on other systems (e.g., malicious code generation, enhanced phishing content), GAI attacks (e.g., prompt injection), ML attacks (e.g., adversarial examples/prompts, data poisoning, membership inference, model extraction, sponge examples).",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate ai red-teaming resilience during prospective testing phase"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0524_NIST2024",
			"name": "Security Measure Effectiveness",
			"description": "Regularly assess and verify that security measures remain effective and have not been compromised.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test security measure effectiveness in validation environment",
				"phase-3": "Maintain security measure effectiveness with continuous monitoring"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0525_NIST2024",
			"name": "Policy Violation Statistics",
			"description": "Compile statistics on actual policy violations, take-down requests, and intellectual property infringement for organizational GAI systems: Analyze transparency reports across demographic groups, languages groups.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate policy violation statistics summarizing validation outcomes",
				"phase-3": "Produce regular policy violation statistics for stakeholders"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0527_NIST2024",
			"name": "Digital Content Transparency",
			"description": "Use digital content transparency solutions to enable the documentation of each instance where content is generated, modified, or shared to provide a tamper-proof history of the content, promote transparency, and enable traceability. Robust version control systems can also be applied to track changes across the AI lifecycle over time.",
			"source": "NIST2024",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-1": "Create initial digital content transparency capturing design decisions and data sources",
				"phase-2": "Update digital content transparency with validation procedures and findings",
				"phase-3": "Maintain current digital content transparency reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0528_NIST2024",
			"name": "User Instruction Adequacy",
			"description": "Verify adequacy of GAI system user instructions through user testing.",
			"source": "NIST2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform user instruction adequacy on prospective validation data",
				"phase-3": "Conduct ongoing user instruction adequacy on live system performance"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0529_NIST2024",
			"name": "ML Explanation Application",
			"description": "Apply and document ML explanation results such as: Analysis of embeddings, Counterfactual prompts, Gradient-based attributions, Model compression/surrogate models, Occlusion/term reduction.",
			"source": "NIST2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update ml explanation application with validation procedures and findings",
				"phase-3": "Maintain current ml explanation application reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0530_NIST2024",
			"name": "GAI Model Documentation",
			"description": "Document GAI model details including: Proposed use and organizational value; Assumptions and limitations, Data collection methodologies; Data provenance; Data quality; Model architecture (e.g., convolutional neural network, transformers, etc.); Optimization objectives; Training algorithms; RLHF approaches; Fine-tuning or retrieval-augmented generation approaches; Evaluation data; Ethical considerations; Legal and regulatory requirements.",
			"source": "NIST2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Establish gai model documentation criteria and baseline metrics",
				"phase-2": "Apply gai model documentation to validation results"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0531_NIST2024",
			"name": "Data Exposure Red-Teaming",
			"description": "Conduct AI red-teaming to assess issues such as: Outputting of training data samples, and subsequent reverse engineering, model extraction, and membership inference risks; Revealing biometric, confidential, copyrighted, licensed, patented, personal, proprietary, sensitive, or trade-marked information; Tracking or revealing location information of users or members of training datasets.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Establish data exposure red-teaming mechanisms for development phase",
				"phase-2": "Activate data exposure red-teaming during prospective testing"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0532_NIST2024",
			"name": "End-User Expectation Engagement",
			"description": "Engage directly with end-users and other stakeholders to understand their expectations and concerns regarding content provenance. Use this feedback to guide the design of provenance data-tracking techniques.",
			"source": "NIST2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-1": "Establish end-user expectation engagement mechanisms for development phase",
				"phase-3": "Maintain end-user expectation engagement for production metrics"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0534_NIST2024",
			"name": "Bias Benchmark Application",
			"description": "Apply use-case appropriate benchmarks (e.g., Bias Benchmark Questions, Real Hateful or Harmful Prompts, Winogender Schemas15) to quantify systemic bias, stereotyping, denigration, and hateful content in GAI system outputs; Document assumptions and limitations of benchmarks, including any actual or possible training/test data cross contamination, relative to in-context deployment environment.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform bias benchmark application on prospective validation data",
				"phase-3": "Conduct ongoing bias benchmark application on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0624_Barrett2024",
			"name": "Risk Ownership by Capability",
			"description": "Take responsibility for risk assessment and risk management tasks for which your organization has access to information, capability, or opportunity to develop capability sufficient for constructive action, or that is substantially greater than others in the value chain",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct risk ownership by capability based on validation findings",
				"phase-3": "Maintain regular risk ownership by capability schedule for production"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0626_Barrett2024",
			"name": "Anticipate Misuse & Impact",
			"description": "Identify reasonably foreseeable uses, and misuses or abuses for a GPAIS (e.g, auto- mated generation of toxic or illegal content or disinformation, or aiding with proliferation of cyber, chemical, biological, or radiological weapons), and identify reasonably foreseeable potential impacts (e.g., to fundamental rights)",
			"source": "Barrett2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-2": "Continue anticipate misuse & impact for validation activities",
				"phase-3": "Operate anticipate misuse & impact for clinical decision tracking"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0627_Barrett2024",
			"name": "Assess Catastrophic Risk",
			"description": "Identify whether a GPAIS could lead to significant, severe, or catastrophic impacts, e.g., because of correlated failures or errors across high-stakes deployment domains, dan- gerous emergent behaviors or vulnerabilities, or harmful misuses and abuses",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate assess catastrophic risk during prospective testing phase",
				"phase-3": "Apply assess catastrophic risk in live clinical deployment"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0628_Barrett2024",
			"name": "Red-Team for Dangerous Behaviors",
			"description": "Use red teams and adversarial testing as part of extensive interaction with GPAIS to identify dangerous capabilities, vulnerabilities, or other emergent properties of such systems",
			"source": "Barrett2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform red-team for dangerous behaviors on prospective validation data"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0629_Barrett2024",
			"name": "Risk Tracking",
			"description": "Track important identified risks (e.g., vulnerabilities from data poisoning and other attacks or objectives mis-specification) even if they cannot yet be measured",
			"source": "Barrett2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Activate risk tracking during prospective testing"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0631_Barrett2024",
			"name": "Disclose Risks to Stakeholders",
			"description": "Incorporate identified AI system risk factors, and circumstances that could result in impacts or harms, into reporting and engagement with internal and external stake- holders (e.g., to downstream developers, regulators, users, impacted communities, etc.) on the AI system as appropriate, e.g., using model cards, system cards, and other transparency mechanisms",
			"source": "Barrett2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate disclose risks to stakeholders summarizing validation outcomes",
				"phase-3": "Produce regular disclose risks to stakeholders for stakeholders"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0632_Barrett2024",
			"name": "Document Mitigation Rationale",
			"description": "Document the process used in considering risk mitigation controls, the options considered, and reasons for choices.",
			"source": "Barrett2024",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update document mitigation rationale with validation procedures and findings",
				"phase-3": "Maintain current document mitigation rationale reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0633_Barrett2024",
			"name": "Ensure Legal Compliance",
			"description": "Legal and regulatory requirements involving AI are understood, managed, and documented.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Create initial ensure legal compliance capturing design decisions and data sources"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0635_Barrett2024",
			"name": "Align Risk Management to Tolerance",
			"description": "Processes, procedures, and practices are in place to determine the needed level of risk management activities based on the organization’s risk tolerance.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test and refine align risk management to tolerance during validation",
				"phase-3": "Follow align risk management to tolerance for all operational activities"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0636_Barrett2024",
			"name": "Transparent RM Policies",
			"description": "The risk management process and its outcomes are established through transparent policies, procedures, and other controls based on organizational risk priorities.",
			"source": "Barrett2024",
			"subcategoryId": "governance-disclosure-4.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test and refine transparent rm policies during validation",
				"phase-3": "Follow transparent rm policies for all operational activities"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0637_Barrett2024",
			"name": "Monitoring of RM policies",
			"description": "Ongoing monitoring and periodic review of the risk management process and its outcomes are planned and organizational roles and responsibilities clearly defined, including determining the frequency of periodic review.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Conduct periodic monitoring of rm policies of operational performance"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0638_Barrett2024",
			"name": "Inventory AI Systems",
			"description": "Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate inventory ai systems during prospective testing phase",
				"phase-3": "Apply inventory ai systems in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0639_Barrett2024",
			"name": "Decommission AI Systems Safely",
			"description": "Processes and procedures are in place for decommissioning and phasing out AI systems safely and in a manner that does not increase risks or decrease the organization’s trustworthiness.",
			"source": "Barrett2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test and refine decommission ai systems safely during validation",
				"phase-3": "Follow decommission ai systems safely for all operational activities"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0640_Barrett2024",
			"name": "Define AI Risk Roles",
			"description": "Roles and responsibilities and lines of communication related to mapping, measuring, and managing AI risks are documented and are clear to individuals and teams throughout the organization.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update define ai risk roles with validation procedures and findings",
				"phase-3": "Maintain current define ai risk roles reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0641_Barrett2024",
			"name": "Train on AI Risk",
			"description": "The organization’s personnel and partners receive AI risk management training to enable them to perform their duties and responsibilities consistent with related policies, procedures, and agreements",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test and refine train on ai risk during validation",
				"phase-3": "Follow train on ai risk for all operational activities"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0642_Barrett2024",
			"name": "Executive Risk Accountability",
			"description": "Executive leadership of the organization takes responsibility for decisions about risks associated with AI sys- tem development and deployment.",
			"source": "Barrett2024",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Begin executive risk accountability development",
				"phase-2": "Finalize executive risk accountability based on testing",
				"phase-3": "Enhance executive risk accountability through continuous improvement"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0643_Barrett2024",
			"name": "Diverse Risk Voices",
			"description": "Decision-making related to mapping, measuring, and managing AI risks throughout the lifecycle is informed by a diverse team (e.g., diversity of demographics, disciplines, experience, expertise, and backgrounds).",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate diverse risk voices during prospective testing phase",
				"phase-3": "Apply diverse risk voices in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0644_Barrett2024",
			"name": "Human-AI Oversight Roles",
			"description": "Policies and procedures are in place to define and differentiate roles and responsibilities for human-AI configurations and oversight of AI systems.",
			"source": "Barrett2024",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test and refine human-ai oversight roles during validation",
				"phase-3": "Follow human-ai oversight roles for all operational activities"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0646_Barrett2024",
			"name": "Document & Share AI Risk Impacts",
			"description": "Organizational teams document the risks and potential impacts of the AI technology they design, develop, deploy, evaluate, and use, and they communicate about the impacts more broadly.",
			"source": "Barrett2024",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Create initial document & share ai risk impacts capturing design decisions and data sources",
				"phase-2": "Update document & share ai risk impacts with validation procedures and findings",
				"phase-3": "Maintain current document & share ai risk impacts reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0647_Barrett2024",
			"name": "Detect & Share Incidents",
			"description": "Organizational practices are in place to enable AI testing, identification of incidents, and information sharing.",
			"source": "Barrett2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform detect & share incidents on prospective validation data"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0648_Barrett2024",
			"name": "External Feedback on Impacts",
			"description": "Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those external to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI risks.",
			"source": "Barrett2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-3": "Enhance external feedback on impacts through continuous improvement"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0649_Barrett2024",
			"name": "External Feedback on System Design",
			"description": "Mechanisms are established to enable the team that developed or deployed AI systems to regularly incorporate adjudicated feedback from relevant AI actors into system design and implementation.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan external feedback on system design approach and resource requirements",
				"phase-3": "Maintain external feedback on system design in production environment"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0650_Barrett2024",
			"name": "Third-Party Risk",
			"description": "Policies and procedures are in place that address AI risks associated with third-party entities, including risks of infringement of a third-party’s intellectual property or other rights.",
			"source": "Barrett2024",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test and refine third-party risk during validation",
				"phase-3": "Follow third-party risk for all operational activities"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0651_Barrett2024",
			"name": "Contingency Planning for Third-Party Failures",
			"description": "Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be high-risk.",
			"source": "Barrett2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan contingency planning for third-party failures approach during project design phase"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0652_Barrett2024",
			"name": "Document Intended Use & Context",
			"description": "Intended purposes, potentially beneficial uses, context-specific laws, norms and expectations, and prospective settings in which the AI system will be deployed are understood and documented.",
			"source": "Barrett2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain current document intended use & context reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0655_Barrett2024",
			"name": "Business Use",
			"description": "The business value or context of business use has been clearly defined or – in the case of assessing existing AI systems – re-evaluated.",
			"source": "Barrett2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate business use during prospective testing phase"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0657_Barrett2024",
			"name": "Elicit Requirements & Assess Impacts",
			"description": "System requirements (e.g., “the system shall respect the privacy of its users”) are elicited from and understood by relevant AI actors. Design decisions take socio-technical implications into account to address AI risks",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan elicit requirements & assess impacts approach during project design phase"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0658_Barrett2024",
			"name": "Define Tasks & Methods",
			"description": "The specific tasks and methods used to implement the tasks that the AI system will support are defined (e.g., classifiers, generative models, recommenders).",
			"source": "Barrett2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Deploy define tasks & methods in validation environment",
				"phase-3": "Maintain define tasks & methods in production environment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0660_Barrett2024",
			"name": "Document TEVV & Integrity",
			"description": "Scientific integrity and TEVV considerations are identified and docu- mented, including those related to experimental design, data collection and selection (e.g., availability, represen- tativeness, suitability), system trustworthiness, and construct validation",
			"source": "Barrett2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Create initial document tevv & integrity capturing design decisions and data sources",
				"phase-2": "Update document tevv & integrity with validation procedures and findings"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0661_Barrett2024",
			"name": "Document Intended Benefits",
			"description": "Potential benefits of intend- ed AI system functionality and performance are ex- amined and documented",
			"source": "Barrett2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update document intended benefits with validation procedures and findings"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0662_Barrett2024",
			"name": "Document AI Error Costs",
			"description": "Potential costs, in- cluding non-monetary costs, which result from expected or realized AI errors or system func- tionality and trustwor- thiness – as connected to organizational risk tolerance – are exam- ined and documented.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update document ai error costs with validation procedures and findings",
				"phase-3": "Maintain current document ai error costs reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0663_Barrett2024",
			"name": "Document Application Scope",
			"description": "Targeted application scope is specified and documented based on the system’s capability, established context, and AI system categorization.",
			"source": "Barrett2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update document application scope with validation procedures and findings",
				"phase-3": "Maintain current document application scope reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0664_Barrett2024",
			"name": "Document Operator Proficiency",
			"description": "Processes for operator and practitioner proficiency with AI system performance and trustworthiness – and relevant technical standards and certifications – are defined, assessed, and documented.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update document operator proficiency with validation procedures and findings"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0665_Barrett2024",
			"name": "Define Human Oversight Protocols",
			"description": "Processes for human oversight are defined, assessed, and documented in accordance with organizational policies from the Govern function",
			"source": "Barrett2024",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update define human oversight protocols with validation procedures and findings",
				"phase-3": "Maintain current define human oversight protocols reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0666_Barrett2024",
			"name": "Document AI RM Third Party Framework",
			"description": "Approaches for mapping AI technology and legal risks of its components – including the use of third-party data or software – are in place, followed, and docu- mented, as are risks of infringement of a third party’s intellectual prop- erty or other rights.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update document ai rm third party framework with validation procedures and findings",
				"phase-3": "Maintain current document ai rm third party framework reflecting operational changes"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0667_Barrett2024",
			"name": "Document Internal & Third-Party Controls",
			"description": "Internal risk controls for components of the AI system, including third- party AI technologies, are identified and documented.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update document internal & third-party controls with validation procedures and findings",
				"phase-3": "Maintain current document internal & third-party controls reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0668_Barrett2024",
			"name": "Assess & Document Risk Magnitude",
			"description": "Likelihood and magnitude of each identified impact (both potentially beneficial and harmful) based on expected use, past uses of AI systems in similar contexts, public incident reports, feedback from those external to the team that developed or deployed the AI system, or other data are identified and documented.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain current assess & document risk magnitude reflecting operational changes"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0669_Barrett2024",
			"name": "Establish Feedback Integration Processes",
			"description": "Practices and personnel for supporting regular engagement with relevant AI actors and integrating feedback about positive, negative, and unanticipated impacts are in place and documented.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain current establish feedback integration processes reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0670_Barrett2024",
			"name": "Select & Document Risk Metrics",
			"description": "Approaches and metrics for measurement of AI risks enumerated during the Map function are selected for implementation starting with the most significant AI risks. The risks or trustworthiness characteristics that will not – or cannot – be measured are properly documented.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update select & document risk metrics with validation procedures and findings",
				"phase-3": "Maintain current select & document risk metrics reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0671_Barrett2024",
			"name": "Evaluate Risk Metrics & Controls",
			"description": "Appropriateness of AI metrics and effectiveness of existing controls are regularly assessed and updated, including reports of errors and potential impacts on affected communities.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate evaluate risk metrics & controls summarizing validation outcomes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0672_Barrett2024",
			"name": "Involve Independent & External Assessors",
			"description": "Internal experts who did not serve as front-line developers for the system and/ or independent assessors are involved in regular assessments and updates. Domain experts, users, AI actors external to the team that developed or deployed the AI system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance.",
			"source": "Barrett2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular involve independent & external assessors schedule for production"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0674_Barrett2024",
			"name": "Ensure Ethical Human Evaluation",
			"description": "Evaluations involving human subjects meet applicable requirements (including human subject protection) and are representative of the relevant population.",
			"source": "Barrett2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Establish ensure ethical human evaluation criteria and baseline metrics",
				"phase-2": "Apply ensure ethical human evaluation to validation results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0675_Barrett2024",
			"name": "Measure & Document Deployment Performance",
			"description": "AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment setting(s). Measures are documented",
			"source": "Barrett2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update measure & document deployment performance with validation procedures and findings",
				"phase-3": "Maintain current measure & document deployment performance reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0676_Barrett2024",
			"name": "Monitor System in Production",
			"description": "The functionality and behavior of the AI system and its components – as identified in the Map function – are monitored when in production.",
			"source": "Barrett2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate monitor system in production with real-time alerting"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0677_Barrett2024",
			"name": "Demonstrate Validity & Generalization Limits",
			"description": "The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the conditions under which the technology was developed are documented.",
			"source": "Barrett2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain current demonstrate validity & generalization limits reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0678_Barrett2024",
			"name": "Evaluate Safety & Failure Modes",
			"description": "The AI system is evaluated regularly for safety risks – as identified in the Map function. The AI system to be deployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, particularly if made to operate beyond its knowledge limits. Safety metrics reflect system reliability and robustness, real- time monitoring, and response times for AI system failures.",
			"source": "Barrett2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Implement evaluate safety & failure modes in shadow mode alongside clinical workflows",
				"phase-3": "Operate evaluate safety & failure modes with real-time alerting"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0679_Barrett2024",
			"name": "Evaluate Security & Resilience",
			"description": "AI system security and resilience – as identified in the Map function – are evaluated and documented.",
			"source": "Barrett2024",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update evaluate security & resilience with validation procedures and findings"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0680_Barrett2024",
			"name": "Explain & Document Model Behavior",
			"description": "The AI model is explained, validated, and documented, and AI system output is interpreted within its context – as identified in the Map function – to inform responsible use and governance.",
			"source": "Barrett2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update explain & document model behavior with validation procedures and findings"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0681_Barrett2024",
			"name": "Examine & Document Privacy Risk",
			"description": "Privacy risk of the AI system – as identified in the Map function – is examined and documented.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update examine & document privacy risk with validation procedures and findings",
				"phase-3": "Maintain current examine & document privacy risk reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0683_Barrett2024",
			"name": "Assess Environmental Impact",
			"description": "Environmental impact and sustainability of AI model training and management activities – as identified in the Map function – are assessed and documented.",
			"source": "Barrett2024",
			"subcategoryId": "environmental-1.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Update assess environmental impact with validation procedures and findings",
				"phase-3": "Maintain current assess environmental impact reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0684_Barrett2024",
			"name": "Evaluate TEVV Effectiveness",
			"description": "Effectiveness of the employed TEVV metrics and processes in the Measure function are evaluated and documented.",
			"source": "Barrett2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update evaluate tevv effectiveness with validation procedures and findings"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0685_Barrett2024",
			"name": "Track Emergent AI Risks",
			"description": "Approaches, personnel, and documentation are in place to regularly identify and track existing, unanticipated, and emergent AI risks based on factors such as intended and actual performance in deployed contexts.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update track emergent ai risks with validation procedures and findings",
				"phase-3": "Maintain current track emergent ai risks reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0686_Barrett2024",
			"name": "Consider Alternatives for Hard-to-Measure Risks",
			"description": "Risk tracking approaches are considered for settings where AI risks are difficult to assess using currently available measurement techniques or where metrics are not yet available.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Activate consider alternatives for hard-to-measure risks during prospective testing",
				"phase-3": "Maintain consider alternatives for hard-to-measure risks for production metrics"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0687_Barrett2024",
			"name": "Enable User Feedback & Appeals",
			"description": "Feedback processes for end users and impacted communities to report problems and appeal system outcomes are established and inte- grated into AI system evaluation metrics.",
			"source": "Barrett2024",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply enable user feedback & appeals to validation results",
				"phase-3": "Execute continuous enable user feedback & appeals of deployed system"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0688_Barrett2024",
			"name": "Align Risk Metrics with Context",
			"description": "Measurement approach- es for identifying AI risks are connected to deployment context(s) and informed through consultation with do- main experts and other end users. Approaches are documented.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain current align risk metrics with context reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0689_Barrett2024",
			"name": "Validate Trustworthiness with Expert Input",
			"description": "Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are informed by input from domain experts and relevant AI actors to validate whether the system is performing consistently as intended. Results are documented.",
			"source": "Barrett2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update validate trustworthiness with expert input with validation procedures and findings",
				"phase-3": "Maintain current validate trustworthiness with expert input reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0690_Barrett2024",
			"name": "Document Performance Shifts from Field Data",
			"description": "Measurable performance improvements or declines based on consultations with relevant AI actors, including affected communities, and field data about context-relevant risks and trustworthiness characteristics are identified and document",
			"source": "Barrett2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update document performance shifts from field data with validation procedures and findings"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0691_Barrett2024",
			"name": "Determine Deployment Readiness",
			"description": "A determination is made as to whether the AI system achieves its intended purposes and stated objectives and whether its development or deployment should proceed.",
			"source": "Barrett2024",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Enhance determine deployment readiness through continuous improvement"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0692_Barrett2024",
			"name": "Prioritize Risk Treatment",
			"description": "Treatment of documented AI risks is prioritized based on impact, likelihood, and available resources or methods.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update prioritize risk treatment with validation procedures and findings",
				"phase-3": "Maintain current prioritize risk treatment reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0693_Barrett2024",
			"name": "Plan & Document Risk Responses",
			"description": "Responses to the AI risks deemed high priority, as identified by the Map function, are developed, planned, and documented. Risk response options can include mitigating, transferring, avoiding, or accepting.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update plan & document risk responses with validation procedures and findings",
				"phase-3": "Maintain current plan & document risk responses reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0694_Barrett2024",
			"name": "Document Residual Risks",
			"description": "Negative residual risks (defined as the sum of all unmitigated risks) to both downstream acquirers of AI systems and end users are documented.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update document residual risks with validation procedures and findings",
				"phase-3": "Maintain current document residual risks reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0695_Barrett2024",
			"name": "Consider Resources & Alternatives",
			"description": "Resources required to manage AI risks are taken into account – along with viable non-AI alternative systems, approaches, or methods – to reduce the magnitude or likelihood of potential impacts.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate consider resources & alternatives during prospective testing phase",
				"phase-3": "Apply consider resources & alternatives in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0697_Barrett2024",
			"name": "Respond to Unknown Risks",
			"description": "Procedures are followed to respond to and recover from a previously unknown risk when it is identified.",
			"source": "Barrett2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test and refine respond to unknown risks during validation",
				"phase-3": "Follow respond to unknown risks for all operational activities"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0698_Barrett2024",
			"name": "Deactivate Misaligned Systems",
			"description": "Mechanisms are in place and applied, and responsibilities are assigned and understood, to supersede, disengage, or deactivate AI systems that demonstrate performance or outcomes inconsistent with intended use.",
			"source": "Barrett2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate deactivate misaligned systems during prospective testing phase"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0699_Barrett2024",
			"name": "Monitor Third-Party Risks",
			"description": "AI risks and benefits from third-party resources are regularly monitored, and risk controls are applied and documented.",
			"source": "Barrett2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain current monitor third-party risks reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0700_Barrett2024",
			"name": "Monitor Pre-trained Models",
			"description": "Pre-trained models which are used for development are monitored as part of AI system regular monitoring and maintenance.",
			"source": "Barrett2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate monitor pre-trained models with real-time alerting"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0701_Barrett2024",
			"name": "Implement Post-deployment Monitoring Plan",
			"description": "Post-deployment AI system monitoring plans are implemented, including mechanisms for capturing and evaluating input from users and other relevant AI actors, appeal and override, decommissioning, incident response, recovery, and change management.",
			"source": "Barrett2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Implement implement post-deployment monitoring plan in shadow mode alongside clinical workflows",
				"phase-3": "Operate implement post-deployment monitoring plan with real-time alerting"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0702_Barrett2024",
			"name": "Integrate Continuous Improvement",
			"description": "Measurable activities for continual improvements are integrated into AI system updates and include regular engagement with interested parties, including relevant AI actors.",
			"source": "Barrett2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply integrate continuous improvement in live clinical deployment"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0703_Barrett2024",
			"name": "Report & Respond to Incidents",
			"description": "Incidents and errors are communicated to relevant AI actors, including affected communities. Processes for tracking, responding to, and recovering from incidents and errors are followed and documented.",
			"source": "Barrett2024",
			"subcategoryId": "incident-reporting-4.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update report & respond to incidents with validation procedures and findings",
				"phase-3": "Maintain current report & respond to incidents reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0707_NIST2024",
			"name": "Harmful Bias and Homogenization Assessment",
			"description": "Conduct fairness assessments to measure systemic bias. Measure GAI system performance across demographic groups and subgroups, addressing both quality of service and any allocation of services and resources. Quantify harms using: field testing with sub-group populations to determine likelihood of exposure to generated content exhibiting harmful bias, AI red-teaming with counterfactual and low-context (e.g., “leader,” “bad guys”) prompts. For ML pipelines or business processes with categorical or numeric outcomes that rely on GAI, apply general fairness metrics (e.g., demographic parity, equalized odds, equal opportunity, statistical hypothesis tests), to the pipeline or business outcome where appropriate; Custom, context-specific metrics developed in collaboration with domain experts and affected communities; Measurements of the prevalence of denigration in generated content in deployment (e.g., subsampling a fraction of traffic and manually annotating denigrating content)",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform harmful bias and homogenization assessment on prospective validation data",
				"phase-3": "Conduct ongoing harmful bias and homogenization assessment on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0708_NIST2024",
			"name": "Assessments for Environmental; Harmful Bias and Homogenization",
			"description": "Identify the classes of individuals, groups, or environmental ecosystems which might be impacted by GAI systems through direct engagement with potentially impacted communities.",
			"source": "NIST2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-2": "Conduct assessments for environmental; harmful bias and homogenization based on validation findings",
				"phase-3": "Maintain regular assessments for environmental; harmful bias and homogenization schedule for production"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0709_NIST2024",
			"name": "Harmful Bias and Homogenization Data Review",
			"description": "Review, document, and measure sources of bias in GAI training and TEVV data: Differences in distributions of outcomes across and within groups, including intersecting groups; Completeness, representativeness, and balance of data sources; demographic group and subgroup coverage in GAI system training data; Forms of latent systemic bias in images, text, audio, embeddings, or other complex or unstructured data; Input data features that may serve as proxies for demographic group membership (i.e., image metadata, language dialect) or otherwise give rise to emergent bias within GAI systems; The extent to which the digital divide may negatively impact representativeness in GAI system training and TEVV data; Filtering of hate speech or content in GAI system training data; Prevalence of GAI-generated data in GAI system training data",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Perform harmful bias and homogenization data review of project design and data sources",
				"phase-2": "Complete harmful bias and homogenization data review of validation methodology and results"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0710_NIST2024",
			"name": "Harmful Bias and Homogenization Assessment",
			"description": "Assess the proportion of synthetic to non-synthetic training data and verify training data is not overly homogenous or GAI-produced to mitigate concerns of model collapse.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Conduct initial harmful bias and homogenization assessment during project planning"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0712_NIST2024",
			"name": "Environmental Degradation Minimisation",
			"description": "Document anticipated environmental impacts of model development, maintenance, and deployment in product design decisions.",
			"source": "NIST2024",
			"subcategoryId": "environmental-1.6",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-1": "Create initial environmental degradation minimisation capturing design decisions and data sources",
				"phase-3": "Maintain current environmental degradation minimisation reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0713_NIST2024",
			"name": "Environmental Impact Assessment",
			"description": "Measure or estimate environmental impacts (e.g., energy and water consumption) for training, fine tuning, and deploying models: Verify tradeoffs between resources used at inference time versus additional resources required at training time.",
			"source": "NIST2024",
			"subcategoryId": "environmental-1.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular environmental impact assessment schedule for production"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0714_NIST2024",
			"name": "Effectiveness of Carbon Capture",
			"description": "Verify effectiveness of carbon capture or offset programs for GAI training and applications, and address green-washing concerns.",
			"source": "NIST2024",
			"subcategoryId": "environmental-1.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Conduct effectiveness of carbon capture for validation team members",
				"phase-3": "Provide ongoing effectiveness of carbon capture for end users"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0715_NIST2024",
			"name": "Confabulation; Information Integrity; Harmful Bias and Homogenization Measurement Error Models",
			"description": "Create measurement error models for pre-deployment metrics to demonstrate construct validity for each metric (i.e., does the metric effectively operationalize the desired concept): Measure or estimate, and document, biases or statistical variance in applied metrics or structured human feedback processes; Leverage domain expertise when modeling complex societal constructs such as hateful content.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain current confabulation; information integrity; harmful bias and homogenization measurement error models reflecting operational changes"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0716_NIST2024",
			"name": "Harmful Bias and Homogenization Assessments",
			"description": "Conduct impact assessments on how AI-generated content might affect different social, economic, and cultural groups.",
			"source": "NIST2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular harmful bias and homogenization assessments schedule for production"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0717_NIST2024",
			"name": "Human-AI Configuration; Information Integrity Studies",
			"description": "Conduct studies to understand how end users perceive and interact with GAI content and accompanying content provenance within context of use. Assess whether the content aligns with their expectations and how they may act upon the information presented.",
			"source": "NIST2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-2": "Validate human-ai configuration; information integrity studies during prospective testing phase",
				"phase-3": "Apply human-ai configuration; information integrity studies in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0718_NIST2024",
			"name": "Harmful Bias and Homogenization Evaluation",
			"description": "Evaluate potential biases and stereotypes that could emerge from the AIgenerated content using appropriate methodologies including computational testing methods as well as evaluating structured feedback input (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV)",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform harmful bias and homogenization evaluation on prospective validation data",
				"phase-3": "Conduct ongoing harmful bias and homogenization evaluation on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0719_NIST2024",
			"name": "Human-AI Configuration; Information Integrity; Harmful Bias and Homogenization",
			"description": "Provide input for training materials about the capabilities and limitations of GAI systems related to digital content transparency for AI Actors, other professionals, and the public about the societal impacts of AI and the role of diverse and inclusive content generation (AI Deployment, Affected Individuals and Communities, End-Users, Operation and Monitoring, TEVV)",
			"source": "NIST2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate human-ai configuration; information integrity; harmful bias and homogenization with real-time alerting"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0720_NIST2024",
			"name": "Human-AI Configuration; Information Integrity; Harmful Bias and Homogenization",
			"description": "Record and integrate structured feedback about content provenance from operators, users, and potentially impacted communities through the use of methods such as user research studies, focus groups, or community forums. Actively seek feedback on generated content quality and potential biases. Assess the general awareness among end users and impacted communities about the availability of these feedback channels.",
			"source": "NIST2024",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Continue human-ai configuration; information integrity; harmful bias and homogenization for all production activities"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0721_NIST2024",
			"name": "Information Integrity; Information Security",
			"description": "Conduct adversarial testing at a regular cadence to map and measure GAI risks, including tests to address attempts to deceive or manipulate the application of provenance techniques or other misuses. Identify vulnerabilities and understand potential misuse scenarios and unintended outputs (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV)",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform information integrity; information security on prospective validation data",
				"phase-3": "Conduct ongoing information integrity; information security on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0722_NIST2024",
			"name": "Human-AI Configuration; Confabulation; Information Security",
			"description": "Evaluate GAI system performance in real-world scenarios to observe its behavior in practical environments and reveal issues that might not surface in controlled and optimized testing environments.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform human-ai configuration; confabulation; information security on prospective validation data"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0723_NIST2024",
			"name": "Information Integrity; Harmful Bias and Homogenization",
			"description": "Implement interpretability and explainability methods to evaluate GAI system decisions and verify alignment with intended purpose. (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV)",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Implement information integrity; harmful bias and homogenization in shadow mode alongside clinical workflows",
				"phase-3": "Operate information integrity; harmful bias and homogenization with real-time alerting"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0724_NIST2024",
			"name": "Information Integrity",
			"description": "Monitor and document instances where human operators or other systems override the GAI's decisions. Evaluate these cases to understand if the overrides are linked to issues related to content provenance. (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV).",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update information integrity with validation procedures and findings",
				"phase-3": "Maintain current information integrity reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0725_NIST2024",
			"name": "Human-AI Configuration; Information Security",
			"description": "Verify and document the incorporation of results of structured public feedback exercises into design, implementation, deployment approval (“go”/“no-go” decisions), monitoring, and decommission decisions. (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV)",
			"source": "NIST2024",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Create initial human-ai configuration; information security capturing design decisions and data sources",
				"phase-3": "Maintain current human-ai configuration; information security reflecting operational changes"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0726_NIST2024",
			"name": "Information Security",
			"description": "Document trade-offs, decision processes, and relevant measurement and feedback results for risks that do not surpass organizational risk tolerance, for example, in the context of model release: Consider different approaches for model release, for example, leveraging a staged release approach. Consider release approaches in the context of the model and its projected use cases. Mitigate, transfer, or avoid risks that surpass organizational risk tolerances.",
			"source": "NIST2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update information security with validation procedures and findings",
				"phase-3": "Maintain current information security reflecting operational changes"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0727_NIST2024",
			"name": "Human-AI Configuration",
			"description": "Monitor the robustness and effectiveness of risk controls and mitigation plans (e.g., via red-teaming, field testing, participatory engagements, performance assessments, user feedback mechanisms). (AI Development, AI Deployment, AI Impact Assessment, Operation and Monitoring)",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform human-ai configuration on prospective validation data",
				"phase-3": "Conduct ongoing human-ai configuration on live system performance"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0728_NIST2024",
			"name": "CBRN Information or Capabilities; Obscene, Degrading, and/or Abusive Content; Harmful Bias and Homogenization; Dangerous, Violent, or Hateful Content",
			"description": "Compare GAI system outputs against pre-defined organization risk tolerance, guidelines, and principles, and review and test AI-generated content against these guidelines. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform cbrn information or capabilities; obscene, degrading, and/or abusive content; harmful bias and homogenization; dangerous, violent, or hateful content on prospective validation data",
				"phase-3": "Conduct ongoing cbrn information or capabilities; obscene, degrading, and/or abusive content; harmful bias and homogenization; dangerous, violent, or hateful content on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0729_NIST2024",
			"name": "Information Integrity",
			"description": "Document training data sources to trace the origin and provenance of AI generated content (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",
			"source": "NIST2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Conduct initial information integrity during project planning",
				"phase-3": "Maintain regular information integrity schedule for production"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0730_NIST2024",
			"name": "Information Integrity",
			"description": "Evaluate feedback loops between GAI system content provenance and human reviewers, and update where needed. Implement real-time monitoring systems to affirm that content provenance protocols remain effective. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring)",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct information integrity based on validation findings",
				"phase-3": "Maintain regular information integrity schedule for production"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0731_NIST2024",
			"name": "Information Security; Harmful Bias and Homogenization",
			"description": "Evaluate GAI content and data for representational biases and employ techniques such as re-sampling, re-ranking, or adversarial training to mitigate biases in the generated content. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",
			"source": "NIST2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct information security; harmful bias and homogenization based on validation findings",
				"phase-3": "Maintain regular information security; harmful bias and homogenization schedule for production"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0732_NIST2024",
			"name": "CBRN Information or Capabilities; Obscene, Degrading, and/or Abusive Content; Harmful Bias and Homogenization; Dangerous, Violent, or Hateful Content",
			"description": "Engage in due diligence to analyze GAI output for harmful content, potential misinformation, and CBRN-related or NCII content. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular cbrn information or capabilities; obscene, degrading, and/or abusive content; harmful bias and homogenization; dangerous, violent, or hateful content schedule for production"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0733_NIST2024",
			"name": "Human-AI Configuration",
			"description": "Use feedback from internal and external AI Actors, users, individuals, and communities, to assess impact of AI-generated content. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",
			"source": "NIST2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular human-ai configuration schedule for production"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0734_NIST2024",
			"name": "Information Integrity",
			"description": "Use real-time auditing tools where they can be demonstrated to aid in the tracking and validation of the lineage and authenticity of AI-generated data. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Execute information integrity on validation cohorts and model outputs",
				"phase-3": "Perform scheduled information integrity on production system and outcomes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0735_NIST2024",
			"name": "Human-AI Configuration; Harmful Bias and Homogenization",
			"description": "Use structured feedback mechanisms to solicit and capture user input about AI generated content to detect subtle shifts in quality or alignment with community and societal values. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",
			"source": "NIST2024",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular human-ai configuration; harmful bias and homogenization schedule for production"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0736_NIST2024",
			"name": "Data Privacy; Intellectual Property; Information Integrity; Confabulation; Harmful Bias and Homogenization",
			"description": "Consider opportunities to responsibly use synthetic data and other privacy enhancing techniques in GAI development, where appropriate and applicable, match the statistical properties of real-world data without disclosing personally identifiable information or contributing to homogenization.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Begin data privacy; intellectual property; information integrity; confabulation; harmful bias and homogenization development"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0737_NIST2024",
			"name": "Value Chain and Component Integration",
			"description": "Develop and update GAI system incident response and recovery plans and procedures to address the following: Review and maintenance of policies and procedures to account for newly encountered uses; Review and maintenance of policies and procedures for detection of unanticipated uses; Verify response and recovery plans account for the GAI system value chain; Verify response and recovery plans are updated for and include necessary details to communicate with downstream GAI system Actors: Points-of-Contact (POC), Contact information, notification format. (AI Deployment, Operation and Monitoring)",
			"source": "NIST2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Conduct periodic value chain and component integration of operational performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0738_NIST2024",
			"name": "Human-AI Configuration",
			"description": "Establish and maintain communication plans to inform AI stakeholders as part of the deactivation or disengagement process of a specific GAI system (including for open-source models) or context of use, including reasons, workarounds, user access removal, alternative processes, contact information, etc.",
			"source": "NIST2024",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Sustain human-ai configuration operations"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0739_NIST2024",
			"name": "Information Security",
			"description": "Establish and maintain procedures for escalating GAI system incidents to the organizational risk management authority when specific criteria for deactivation or disengagement is met for a particular context of use or for the GAI system as a whole.",
			"source": "NIST2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Follow information security for all operational activities"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0740_NIST2024",
			"name": "Information Security",
			"description": "Establish and maintain procedures for the remediation of issues which trigger incident response processes for the use of a GAI system, and provide stakeholders timelines associated with the remediation plan",
			"source": "NIST2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Follow information security for all operational activities"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0741_NIST2024",
			"name": "Information Security",
			"description": "Establish and regularly review specific criteria that warrants the deactivation of GAI systems in accordance with set risk tolerances and appetites.",
			"source": "NIST2024",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Complete information security of validation methodology and results",
				"phase-3": "Conduct periodic information security of operational performance"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0742_NIST2024",
			"name": "Value Chain and Component Integration; Intellectual Property",
			"description": "Apply organizational risk tolerances and controls (e.g., acquisition and procurement processes; assessing personnel credentials and qualifications, performing background checks; filtering GAI input and outputs, grounding, fine tuning, retrieval-augmented generation) to third-party GAI resources: Apply organizational risk tolerance to the utilization of third-party datasets and other GAI resources; Apply organizational risk tolerances to fine-tuned third-party models; Apply organizational risk tolerance to existing third-party models adapted to a new domain; Reassess risk measurements after fine-tuning thirdparty GAI models",
			"source": "NIST2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply value chain and component integration; intellectual property in live clinical deployment"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0743_NIST2024",
			"name": "Data Privacy; Information Security; Value Chain and Component Integration; Harmful Bias and Homogenization",
			"description": "Test GAI system value chain risks (e.g., data poisoning, malware, other software and hardware vulnerabilities; labor practices; data privacy and localization compliance; geopolitical alignment).",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan data privacy; information security; value chain and component integration; harmful bias and homogenization methodology for retrospective data analysis",
				"phase-2": "Perform data privacy; information security; value chain and component integration; harmful bias and homogenization on prospective validation data",
				"phase-3": "Conduct ongoing data privacy; information security; value chain and component integration; harmful bias and homogenization on live system performance"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0744_NIST2024",
			"name": "Value Chain and Component Integration",
			"description": "Re-assess model risks after fine-tuning or retrieval-augmented generation implementation and for any third-party GAI models deployed for applications and/or use cases that were not evaluated in initial testing.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform value chain and component integration on prospective validation data",
				"phase-3": "Conduct ongoing value chain and component integration on live system performance"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0745_NIST2024",
			"name": "Intellectual Property; CBRN Information or Capabilities",
			"description": "Take reasonable measures to review training data for CBRN information, and intellectual property, and where appropriate, remove it. Implement reasonable measures to prevent, flag, or take other action in response to outputs that reproduce particular training data (e.g., plagiarized, trademarked, patented, licensed content or trade secret material).",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Perform intellectual property; cbrn information or capabilities of project design and data sources"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0746_NIST2024",
			"name": "Information Integrity; Information Security; Value Chain and Component Integration",
			"description": "Review various transparency artifacts (e.g., system cards and model cards) for third-party models.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Conduct periodic information integrity; information security; value chain and component integration of operational performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0747_NIST2024",
			"name": "Information Integrity",
			"description": "Implement real-time monitoring processes for analyzing generated content performance and trustworthiness characteristics related to content provenance to identify deviations from the desired standards and trigger alerts for human intervention.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Implement information integrity in shadow mode alongside clinical workflows",
				"phase-3": "Operate information integrity with real-time alerting"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0748_NIST2024",
			"name": "Information Integrity; Value Chain and Component Integration",
			"description": "Leverage feedback and recommendations from organizational boards or committees related to the deployment of GAI applications and content provenance when using third-party pre-trained models.",
			"source": "NIST2024",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Provide ongoing information integrity; value chain and component integration for end users"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0749_NIST2024",
			"name": "Human-AI Configuration",
			"description": "Use human moderation systems where appropriate to review generated content in accordance with human-AI configuration policies established in the Govern function, aligned with socio-cultural norms in the context of use, and for settings where AI models are demonstrated to perform poorly.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Complete human-ai configuration of validation methodology and results",
				"phase-3": "Conduct periodic human-ai configuration of operational performance"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0750_NIST2024",
			"name": "CBRN Information or Capabilities; Confabulation",
			"description": "Use organizational risk tolerance to evaluate acceptable risks and performance metrics and decommission or retrain pre-trained models that perform outside of defined limits.",
			"source": "NIST2024",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct cbrn information or capabilities; confabulation for validation team members"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0751_NIST2024",
			"name": "Information Integrity; Harmful Bias and Homogenization",
			"description": "Collaborate with external researchers, industry experts, and community representatives to maintain awareness of emerging best practices and technologies in measuring and managing identified risks.",
			"source": "NIST2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Continue information integrity; harmful bias and homogenization for validation activities",
				"phase-3": "Operate information integrity; harmful bias and homogenization for clinical decision tracking"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0752_NIST2024",
			"name": "CBRN Information or Capabilities; Confabulation; Information Security",
			"description": "Establish, maintain, and evaluate effectiveness of organizational processes and procedures for post-deployment monitoring of GAI systems, particularly for potential confabulation, CBRN, or cyber risks.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Implement cbrn information or capabilities; confabulation; information security in shadow mode alongside clinical workflows",
				"phase-3": "Operate cbrn information or capabilities; confabulation; information security with real-time alerting"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0753_NIST2024",
			"name": "Human-AI Configuration",
			"description": "Evaluate the use of sentiment analysis to gauge user sentiment regarding GAI content performance and impact, and work in collaboration with AI Actors experienced in user research and experience.",
			"source": "NIST2024",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-2": "Validate human-ai configuration during prospective testing phase",
				"phase-3": "Apply human-ai configuration in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0754_NIST2024",
			"name": "Confabulation",
			"description": "Implement active learning techniques to identify instances where the model fails or produces unexpected outputs.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Deploy confabulation in validation environment",
				"phase-3": "Maintain confabulation in production environment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0755_NIST2024",
			"name": "Human-AI Configuration; Harmful Bias and Homogenization",
			"description": "Share transparency reports with internal and external stakeholders that detail steps taken to update the GAI system to enhance transparency and accountability",
			"source": "NIST2024",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate human-ai configuration; harmful bias and homogenization summarizing validation outcomes",
				"phase-3": "Produce regular human-ai configuration; harmful bias and homogenization for stakeholders"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0756_NIST2024",
			"name": "Information Integrity",
			"description": "Track dataset modifications for provenance by monitoring data deletions, rectification requests, and other changes that may impact the verifiability of content origins.",
			"source": "NIST2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design information integrity framework and define key metrics",
				"phase-3": "Operate information integrity with real-time alerting"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0757_NIST2024",
			"name": "Human-AI Configuration; Information Integrity",
			"description": "Verify that AI Actors responsible for monitoring reported issues can effectively evaluate GAI system performance including the application of content provenance data tracking techniques, and promptly escalate issues for response.",
			"source": "NIST2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate human-ai configuration; information integrity summarizing validation outcomes",
				"phase-3": "Produce regular human-ai configuration; information integrity for stakeholders"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0758_NIST2024",
			"name": "Harmful Bias and Homogenization",
			"description": "Conduct regular monitoring of GAI systems and publish reports detailing the performance, feedback received, and improvements made.",
			"source": "NIST2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate harmful bias and homogenization summarizing validation outcomes",
				"phase-3": "Produce regular harmful bias and homogenization for stakeholders"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0759_NIST2024",
			"name": "Human-AI Configuration; Dangerous, Violent, or Hateful Content",
			"description": "Practice and follow incident response plans for addressing the generation of inappropriate or harmful content and adapt processes based on findings to prevent future occurrences. Conduct post-mortem analyses of incidents with relevant AI Actors, to understand the root causes and implement preventive measures.",
			"source": "NIST2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain human-ai configuration; dangerous, violent, or hateful content in production environment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0760_NIST2024",
			"name": "Human-AI Configuration",
			"description": "Use visualizations or other methods to represent GAI model behavior to ease non-technical stakeholders understanding of GAI system functionality",
			"source": "NIST2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate human-ai configuration during prospective testing phase",
				"phase-3": "Apply human-ai configuration in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0761_NIST2024",
			"name": "Confabulation; Information Integrity",
			"description": "Establish and maintain policies and procedures to record and track GAI system reported errors, near-misses, and negative impacts.",
			"source": "NIST2024",
			"subcategoryId": "incident-reporting-4.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Maintain confabulation; information integrity throughout validation testing",
				"phase-3": "Continue confabulation; information integrity for all production activities"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0762_NIST2024",
			"name": "Information Security; Data Privacy",
			"description": "Report GAI incidents in compliance with legal and regulatory requirements (e.g., HIPAA breach reporting, e.g., OCR (2023) or NHTSA (2022) autonomous vehicle crash reporting requirements.",
			"source": "NIST2024",
			"subcategoryId": "incident-reporting-4.3",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Define information security; data privacy requirements and templates"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0765_UK Government2023",
			"name": "Pre-Deployment Risk Assessments",
			"description": "Devote resources to pre-development risk assessments, alongside prioritising pre- deployment risk assessments. Pre-development risk assessments are also important, because training a high-risk system can still lead to harm if the model is leaked, stolen, or otherwise unintentionally distributed.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Conduct initial pre-deployment risk assessments during project planning",
				"phase-3": "Maintain regular pre-deployment risk assessments schedule for production"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0766_UK Government2023",
			"name": "Development and Post-Deployment Monitoring",
			"description": "Monitor systems both during development and after deployment. New risk assessments could be carried out in cases of fine-tuning or other substantial changes that could increase the danger of a model, such as the model gaining access to tools or plugins. This could occur alongside attempts to detect unexpected developments and new information that might have changed the results of the existing risk assessment, and which could also trigger a new risk assessment.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular development and post-deployment monitoring schedule for production"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0767_UK Government2023",
			"name": "Risk Thresholds for Specific Models",
			"description": "Describe and continually refine risk assessment results for each model (“risk thresholds”) that would trigger particular risk-reducing actions, defining such results in terms of risk to all relevant stakeholders given currently existing mitigations. Given the high uncertainty around future model capabilities, risk thresholds may be refined periodically.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct risk thresholds for specific models based on validation findings",
				"phase-3": "Maintain regular risk thresholds for specific models schedule for production"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0770_UK Government2023",
			"name": "Refinement of Risk Evaluation Frameworks",
			"description": "Continue to refine and redefine risk evaluation frameworks for models as necessary, aiming to reduce the gap between the intended objectives of risk thresholds and their present operationalisations. Such gaps are expected to exist due to limitations in the science of evaluation and in the state of knowledge surrounding capabilities, so progress towards a robust framework will probably be iterative. Risk evaluation frameworks may use multiple methods, including probability estimations and qualitative assessments of current capabilities.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply refinement of risk evaluation frameworks to validation results"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0771_UK Government2023",
			"name": "Mitigation of Overshooting Risk Thresholds",
			"description": "Mitigate the risk of ‘overshooting’ risk thresholds. This may be achieved by setting deliberately conservative thresholds, including using intentionally lower buffer thresholds to trigger actions, such that the most concerning thresholds are difficult to overshoot without having already implemented mitigations at an earlier stage.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Deploy mitigation of overshooting risk thresholds in validation environment",
				"phase-3": "Maintain mitigation of overshooting risk thresholds in production environment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0773_UK Government2023",
			"name": "Development Constrained by Risk Thresholds",
			"description": "At each risk threshold, proactively commit to only proceed with certain development or deployment steps if specific mitigations are in place. Such mitigations could include many of the practices outlined in this document.",
			"source": "UK Government2023",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain current development constrained by risk thresholds reflecting operational changes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0774_UK Government2023",
			"name": "Reassessment of Residual Risk",
			"description": "After putting in place mitigations, reassess any residual risk posed to determine whether additional mitigations are required. Due to the unpredictability of capabilities advancements and the limitations of model evaluation science, pre-agreed mitigations may prove insufficient to place a given model within a risk threshold. Risk acceptance criteria may be used, as is standard in many other contexts, and may provide an important tool for clarification. These criteria may evolve with time, and could be quantitative or qualitative. For example, risk may only be accepted if it has been reduced to a level ‘as low as reasonably practicable’.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply reassessment of residual risk to validation results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0775_UK Government2023",
			"name": "Disclosure of Risk Threshold to Government Authorities",
			"description": "Inform relevant government authorities when a risk threshold has been met, along with proposed mitigations. Inform governments again, in advance of deployment, when the mitigations and residual risk assessment have been carried out. Proactively engage other relevant actors in addition to governments as appropriate.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular disclosure of risk threshold to government authorities schedule for production"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0776_UK Government2023",
			"name": "Adaptation of Mitigations to Development and Deployment Stages",
			"description": "When planning required mitigations, consider the full range of development and deployment stages. In general, meeting a risk threshold could require mitigations at multiple such stages. Important stages may include the following: 1) Continued training of model; 2) Deployment of model in small-scale ways, e.g., internal use; 3) Deployment of model in large-scale ways e.g., public release via API ; 4) Extension of model through greater affordances, e.g., tool use or internet access; 5) Irreversible deployment e.g. open-sourcing of models",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Begin adaptation of mitigations to development and deployment stages development",
				"phase-3": "Enhance adaptation of mitigations to development and deployment stages through continuous improvement"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0778_UK Government2023",
			"name": "Recognition of Dangerous Model Possession",
			"description": "Recognise that even the possession of some models may be dangerous, even if not deployed or not deployed widely, due to the risk of being unable to secure a model sufficiently to prevent, for instance, a bad actor obtaining the model weights. In contrast, other models may pose significant risk only if deployed in a large-scale or irreversible way.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply recognition of dangerous model possession in live clinical deployment"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0780_UK Government2023",
			"name": "Pause of Training Runs",
			"description": "Prepare to pause training runs or reduce access to deployed models, if risk thresholds are reached without the committed risk mitigations being in place. This may involve warning existing customers that access reductions are a possibility and creating contingency plans to minimise negative impacts on customer use.",
			"source": "UK Government2023",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Enforce pause of training runs in production systems"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0781_UK Government2023",
			"name": "Risks of Open-Source Models",
			"description": "Acknowledge that some models may pose additional risks if made available “open-source”, even after mitigation attempts. This is because of the inability of recalling an open-sourced model and the potential ability of users to remove safeguards and introduce new (and potentially dangerous) capabilities. However, it is also important to bear in mind the significant benefits of “open-source” AI systems for researchers, including for advancing AI safety, which may in some cases outweigh these potential risks.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply risks of open-source models in live clinical deployment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0782_UK Government2023",
			"name": "Risk Assessment Disclosure to Third Parties",
			"description": "Regularly update relevant stakeholders on risk assessment and mitigation measures: This will enable assessment of whether AI organisations have sufficient risk management processes in place, build up a picture of best practices, and make recommendations to address gaps. When sharing this information with external actors, consideration should be given to commercially sensitive information. Additional ad hoc updates could be provided in cases of major developments.Include information on evaluations, risk assessment and mitigation, and individuals involved. For example: 1) What types of tests and evaluations are being run on which types of models; 2) What other risk assessment methods are being used, which kinds of expertise are drawn on, and whether impacted stakeholders are being involved; 3) How risk mitigation measures are being monitored; 4) Which teams and individuals are involved at different stages of the risk management process (and how, if at all, third parties are involved); 5) Measures taken to address specific categories of risk, such as cybersecurity measures",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform risk assessment disclosure to third parties on prospective validation data",
				"phase-3": "Conduct ongoing risk assessment disclosure to third parties on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0783_UK Government2023",
			"name": "Robust and Meaningful Accountability Mechanisms",
			"description": "Introduce robust and meaningful accountability mechanisms, especially in evaluating capabilities thresholds, with clear processes that ensure the correct mitigations or courses of action are followed if the thresholds are met. This may include board sign-off for the responsible capability scaling policy, and named individual accountability for key decisions.",
			"source": "UK Government2023",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Draft robust and meaningful accountability mechanisms aligned with institutional requirements",
				"phase-2": "Finalize robust and meaningful accountability mechanisms based on validation learnings",
				"phase-3": "Enforce robust and meaningful accountability mechanisms in production operations"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0784_UK Government2023",
			"name": "Effective Risk Governance",
			"description": "Establish effective risk governance to ensure that risks are appropriately identified, assessed, and addressed, and their nature and scale transparently reported. Most importantly, provide internal checks and balances, which may include thoughtful separation of roles within risk management.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Define effective risk governance requirements and templates",
				"phase-2": "Generate effective risk governance summarizing validation outcomes",
				"phase-3": "Produce regular effective risk governance for stakeholders"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0785_UK Government2023",
			"name": "Verification Mechanisms",
			"description": "Include verification mechanisms, such that external actors can have increased confidence that responsible capability scaling policies are executed as intended.",
			"source": "UK Government2023",
			"subcategoryId": "governance-disclosure-4.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate verification mechanisms during prospective testing phase",
				"phase-3": "Apply verification mechanisms in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0786_UK Government2023",
			"name": "Model Evaluation of Dangerous Capabilities",
			"description": "Evaluate models for potential dangerous capabilities (i.e. capabilities that could cause substantial harm either from intentional misuse or accident). These capabilities could include but are not limited to: 1) Offensive cyber capabilities (e.g. producing code to exploit software vulnerabilities); 2) Deception and manipulation (e.g. lying effectively or convincing people to take costly actions); 3) Capabilities that can assist users in developing, designing, acquiring, or using biological, chemical, or radiological weapons (e.g. helping users “troubleshoot” their efforts to produce biological weapons)",
			"source": "UK Government2023",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply model evaluation of dangerous capabilities to validation results"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0787_UK Government2023",
			"name": "Model Evaluation of Controllability",
			"description": "Evaluate models for controllability issues (i.e. propensities to apply their capabilities in ways that neither the models’ users nor the models’ developers want). This could include, for example, autonomous replication and adaptation (i.e. capabilities that could allow a model to copy and run itself on other computer systems).",
			"source": "UK Government2023",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply model evaluation of controllability to validation results"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0788_UK Government2023",
			"name": "Model Evaluation of Societal Harm",
			"description": "Evaluate models for societal harms. These could include, for example, bias and discrimination (e.g. the risk that they produce content that reinforces harmful stereotypes or their potential discriminatory influence if used to inform decisions), recognising that ‘bias’ can be difficult to define and can be subject to different interpretations in different contexts.",
			"source": "UK Government2023",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply model evaluation of societal harm to validation results"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0790_UK Government2023",
			"name": "Response Processes for Evaluation Results",
			"description": "Ensure processes are in place to respond to evaluation results. Evaluations are a necessary input to a responsible capability scaling policy which, depending on the results of the evaluation, would probably require implementation of practices outlined in other sections of this document such as preventing model misuse, information sharing and other risk mitigation measures.",
			"source": "UK Government2023",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply response processes for evaluation results to validation results"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0792_UK Government2023",
			"name": "Pre-training and Fine-tuning of Models",
			"description": "During pre-training and fine-tuning, evaluate the model to detect signs of undesirable properties and identify inaccuracies in pretraining predictions. These evaluations could be undertaken at various pre-specified checkpoints, and could inform decisions about whether to pause or adjust the training process.",
			"source": "UK Government2023",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply pre-training and fine-tuning of models to validation results"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0793_UK Government2023",
			"name": "Pre-Deployment Evaluations",
			"description": "After training, subject the model to extensive pre-deployment evaluations. These evaluations can inform decisions about whether and how to deploy the system, as well as allowing governments and potential users to make informed decisions about regulating or using the model. Their intensity will be proportional to the risk of the deployment, taking into account the model’s capabilities, novelty, expected domains of use, and number of individuals expected to be affected by it.",
			"source": "UK Government2023",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply pre-deployment evaluations to validation results",
				"phase-3": "Execute continuous pre-deployment evaluations of deployed system"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0794_UK Government2023",
			"name": "Post-Deployment Evaluations",
			"description": "After deployment, conduct evaluations at regular intervals to identify new capabilities and associated risks, especially when notable developments (e.g. a major update to the model) suggest earlier evaluations have become obsolete. Post-deployment evaluations can inform decisions to update the system’s safeguards, increase security around the model, temporarily limit access, or roll back deployment.",
			"source": "UK Government2023",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply post-deployment evaluations to validation results",
				"phase-3": "Execute continuous post-deployment evaluations of deployed system"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0795_UK Government2023",
			"name": "Context-Specific Model Evaluations",
			"description": "Require organisations who deploy their models to conduct context-specific model evaluations. This requires that the information and data required to successfully conduct such assessments is provided to deployers.",
			"source": "UK Government2023",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply context-specific model evaluations to validation results",
				"phase-3": "Execute continuous context-specific model evaluations of deployed system"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0797_UK Government2023",
			"name": "Appropriate Safeguards Against External Evaluations",
			"description": "Ensure that there are appropriate safeguards against external evaluations leading to unintended widespread distribution of models. Allowing external evaluators to download models onto their own hardware increases the chance of the models being stolen or leaked. Therefore, unless adequate security against widespread model distribution can be assured, external evaluators could only be allowed to access models through interfaces that prevent exfiltration (such as current API access methods). It may be appropriate to limit evaluators’ access to information that could indirectly facilitate widespread model distribution in other ways, such as requiring in-depth KYC checks or watermarking copies of the model.",
			"source": "UK Government2023",
			"subcategoryId": "third-party-access-4.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply appropriate safeguards against external evaluations to validation results",
				"phase-3": "Execute continuous appropriate safeguards against external evaluations of deployed system"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0798_UK Government2023",
			"name": "Sufficient Time for External Evaluators",
			"description": "Give external evaluators sufficient time. As expected risks from models increase or models get more complex to evaluate, the time afforded for evaluation may need to increase as well.",
			"source": "UK Government2023",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply sufficient time for external evaluators to validation results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0799_UK Government2023",
			"name": "Fine-tuning by External Evaluators",
			"description": "Give external evaluators the ability to securely “fine-tune” the AI systems being tested. Evaluators cannot fully assess risks associated with widespread model distribution if they cannot fine-tune the model. This may involve providing external evaluators with access to capable infrastructure to enable fine-tuning.",
			"source": "UK Government2023",
			"subcategoryId": "third-party-access-4.5",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform fine-tuning by external evaluators on prospective validation data"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0800_UK Government2023",
			"name": "External Evaluation of Model without Safety Regulations",
			"description": "Give external evaluators access to versions of the model that lack safety mitigations. Where possible, sharing these versions of a model gives evaluators insight into the risks that might be created if users find ways to circumvent safeguards (i.e. “jailbreak” the model). If the model is open-sourced, leaked, or stolen, users may also simply be able to remove or bypass the safety mitigations.",
			"source": "UK Government2023",
			"subcategoryId": "third-party-access-4.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply external evaluation of model without safety regulations to validation results",
				"phase-3": "Execute continuous external evaluation of model without safety regulations of deployed system"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0802_UK Government2023",
			"name": "External Evaluation of Deployed System Components",
			"description": "Give external evaluators the ability to study all of the components of deployed systems, where possible. Deployed AI systems typically combine a core model with smaller models and other software components, including moderation filters, user interfaces to incentivise particular user behaviour, and plug-ins for extension capabilities like web browsing or code execution. For example, a red team cannot find all the flaws in the defences of a system if they aren’t able to test all of its different components. It is important to consider the need to balance external evaluators’ ability to access all components of the system against the need to protect information that would allow bypassing model defences.",
			"source": "UK Government2023",
			"subcategoryId": "third-party-access-4.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform external evaluation of deployed system components on prospective validation data",
				"phase-3": "Conduct ongoing external evaluation of deployed system components on live system performance"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0806_UK Government2023",
			"name": "Disclosure of Risk Assessment Processes",
			"description": "Share details of risk assessment processes and risk mitigation measures with relevant government authorities and other AI companies, as set out in Responsible Capability Scaling.",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct disclosure of risk assessment processes based on validation findings",
				"phase-3": "Maintain regular disclosure of risk assessment processes schedule for production"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0807_UK Government2023",
			"name": "Disclosure of Information about Internal Governance Processes",
			"description": "Share information about how internal governance processes are set up with relevant government authorities. This will ensure that risks are appropriately identified, communicated and mitigated, and allow government and other external actors to identify gaps that might lead to risks being overlooked. This information could be updated regularly (e.g. every 12 months). This information could also be made public, provided sensitive details are removed.",
			"source": "UK Government2023",
			"subcategoryId": "governance-disclosure-4.4",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Set up disclosure of information about internal governance processes structure for project oversight",
				"phase-2": "Exercise disclosure of information about internal governance processes for validation phase decisions",
				"phase-3": "Maintain disclosure of information about internal governance processes with regular oversight reviews"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0808_UK Government2023",
			"name": "Reporting Security Details to Government Authorities",
			"description": "Report any details of security or safety incidents or near-misses to relevant government authorities. This includes any compromise to the security of the organisation or its systems, or any incident where an AI system – deployed or not – causes substantial harm or is close to doing so. This will enable government authorities to build a clear picture of when safety and security incidents occur and make it easier to anticipate and mitigate future risks. Incident reports could include a description of the incident, the location, start and end date, details of any parties affected and harms occurred, any specific models involved, any relevant parties responsible for managing and responding to the incident, as well as ways in which the incident could have been avoided. It is important that incidents indicative of more severe risks are reported as soon as possible after they occur. High-level details of safety and security incidents - with sensitive information removed - could also be made public, such as have been shared in the AI incident database.",
			"source": "UK Government2023",
			"subcategoryId": "incident-reporting-4.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Produce regular reporting security details to government authorities for stakeholders"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0809_UK Government2023",
			"name": "Pre-training Disclosure of High-Level Model Details to Government Authorities",
			"description": "Before training, share high-level model details with the relevant government authorities and justify why the training run does not impose unacceptable risk. This includes: 1) A high-level description of the model (including high-level information on intended use cases, intended users, training data, and model architecture); 2) Compute details (including the maximum the organisation plans to use, as well as information about its location and who provides it); 3) Description of the data that will be used to train the model; 4) Evidence from scaling small models that the full training run does not pose unacceptably high risks; 5) Descriptions of specific internal and external risk assessments and mitigation efforts,3 and an overall safety assessment justifying why and how the training run is sufficiently low-risk to execute; 6) Description of which, if any, domain experts and impacted stakeholders have been involved in the project’s design, as well as risk and impact assessment; 7) Plans for model evaluations during and after training, as well as predicted dangerous capabilities",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Establish pre-training disclosure of high-level model details to government authorities criteria and baseline metrics",
				"phase-2": "Apply pre-training disclosure of high-level model details to government authorities to validation results",
				"phase-3": "Execute continuous pre-training disclosure of high-level model details to government authorities of deployed system"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0810_UK Government2023",
			"name": "Updates to Government Authorities During Training",
			"description": "During training, update information provided to relevant government authorities with any significant changes to the model itself or its risk profile. This could include: 1) Updates on model development at each evaluation checkpoint as well as any significant updates to the development plan; 2) Results from model evaluations, including details of emergent dangerous capabilities and whether these were expected; 3) Whether and how the risk context has changed (e.g. if other AI tools have been published that the model could interact with)",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply updates to government authorities during training to validation results"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0812_UK Government2023",
			"name": "Adjucation around Publicly Sharing Risks",
			"description": "Before sharing information, consider the risks of sharing this information and judge whether it is inappropriate to share certain pieces of information. In particular, it is important to consider potential harms from publicly sharing information about dangerous capabilities and methods for eliciting them, as this information could motivate or help other actors to acquire these capabilities. It is also important to consider potential harms from publicly sharing detailed information about how models were produced, as this may lower barriers to producing similar models. If the models have or could be modified to possess dangerous capabilities (e.g. biological capabilities or surveillance capabilities), then facilitating widespread distribution of the model in this way may be harmful. The National Protective Security Authority (NPSA) guidance on a security-minded approach to information management may prove helpful for sharing information appropriately.",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Continue adjucation around publicly sharing risks for validation activities",
				"phase-3": "Operate adjucation around publicly sharing risks for clinical decision tracking"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0813_UK Government2023",
			"name": "Principled Policies for Disclosure",
			"description": "Develop principled policies about what information to share publicly, with governments, or not at all. These policies could specify situations under which sharing some piece of information is subject to a risk assessment, along with guidelines for conducting the risk assessment and responding to it. It is important, however, to avoid creating risk assessment criteria and procedures that are overly strict and intensive to prevent excessive opacity. These policies could be guided and overseen by an independent review panel of multidisciplinary experts to ensure decisions made about or against information sharing are justifiable and oriented to optimal transparency.",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct principled policies for disclosure based on validation findings",
				"phase-3": "Maintain regular principled policies for disclosure schedule for production"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0814_UK Government2023",
			"name": "Disclosure of Information with AI-focused Government Authorities",
			"description": "Share complete forms of this information with central AI-focused government authorities (including central government bodies, security agencies, and regulators) to enable robust government oversight of potentially high-risk areas of AI development and the processes in place to identify and manage those risks. These authorities could then further share information selectively with additional government bodies where relevant. Some particularly security-relevant pieces of information may need to be shared directly with security agencies, such as the cybersecurity measures being used in model development (which would make it easier for those agencies to identify potential security risks), or specific physical or cybersecurity threat incidents. Robust security measures are in place when sharing more sensitive information.",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Finalize disclosure of information with ai-focused government authorities based on testing",
				"phase-3": "Enhance disclosure of information with ai-focused government authorities through continuous improvement"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0815_UK Government2023",
			"name": "Sharing of Limited Information",
			"description": "Share more limited information with other AI organisations in order to facilitate learning and the development of best practices. This could include best practices and lessons learned in the development of risk assessment and mitigation measures and risk governance processes, some details about safety and security incidents (to enable increased awareness while protecting intellectual property), and highlights and lessons learned from risk assessments and capability evaluation. In general, it will be easier for organisations to share model-agnostic information with one another than model-specific information, which may be more commercially sensitive. It will also be important to consider that there is no privileged access to information that may give a competitive advantage to some firms.",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply sharing of limited information to validation results",
				"phase-3": "Execute continuous sharing of limited information of deployed system"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0816_UK Government2023",
			"name": "Sharing with Independent Third-Parties",
			"description": "Share specific information with independent third-parties where this aids evaluation and technical audit (see Model Evaluations and Red Teaming). This may require sharing much of the same information that is shared with governments in full but may be shared on a case-by- case basis.",
			"source": "UK Government2023",
			"subcategoryId": "third-party-access-4.5",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Execute sharing with independent third-parties on validation cohorts and model outputs"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0817_UK Government2023",
			"name": "Information Sharing with Downstream Users",
			"description": "Share specific information with downstream users of the model to enable more effective risk mitigation across the AI supply chain and build consumer confidence. This could include uses of the model that are against their terms of service, and could be provided to users through user terms or published information about the model.",
			"source": "UK Government2023",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply information sharing with downstream users in live clinical deployment"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0818_UK Government2023",
			"name": "Sharing General Information with the Public",
			"description": "Share more general versions of the information outlined with the public to enable public scrutiny and build public confidence in the safety and reliability of AI systems. This could include high-level summaries of risk assessment and mitigation processes, high-level details of safety and security incidents, summaries of risk governance processes, and general overviews of model purpose, use cases, and the results of risk assessments and capability evaluations.",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply sharing general information with the public to validation results",
				"phase-3": "Execute continuous sharing general information with the public of deployed system"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0819_UK Government2023",
			"name": "Application of Infrastructure Principles",
			"description": "Apply good infrastructure principles to every part of this process from design to decommissioning. This is important as threats can occur at different stages of an AI project’s life cycle.",
			"source": "UK Government2023",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan application of infrastructure principles approach during project design phase"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0820_UK Government2023",
			"name": "Regular Assessment of Supply Chain Security",
			"description": "Regularly assess the security of their supply chains and ensure suppliers adhere to the same standards their own organisation applies. Ensuring data, software components and hardware are obtained from trusted sources will help mitigate supply chain risk.",
			"source": "UK Government2023",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct regular assessment of supply chain security based on validation findings",
				"phase-3": "Maintain regular regular assessment of supply chain security schedule for production"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0821_UK Government2023",
			"name": "Model Robustness to Adversarial Attacks",
			"description": "Evaluate the robustness of models to different classes of adversarial attack (such as poisoning, model inversion and model stealing), based on priorities derived from threat modelling. This could involve a combination of benchmarking and red teaming.",
			"source": "UK Government2023",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate model robustness to adversarial attacks during prospective testing phase"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0822_UK Government2023",
			"name": "Documentation of the Cyber security threats",
			"description": "Assess and document the cyber security threats against the AI system overall and mitigate the impact of vulnerabilities. Good documentation and monitoring will inform your overall risk posture and help you respond in the event of a security incident.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update documentation of the cyber security threats with validation procedures and findings",
				"phase-3": "Maintain current documentation of the cyber security threats reflecting operational changes"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0823_UK Government2023",
			"name": "Value of AI-related Assets",
			"description": "Understand the value of AI-related assets such as models, data (including user feedback), prompts, software, documentation, and assessments (including information about potentially unsafe capabilities and failure modes) to their organisation. Protect these different categories of information as appropriate.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular value of ai-related assets schedule for production"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0824_UK Government2023",
			"name": "Security Integration into Business Decisions",
			"description": "Ensure security is factored into all business decisions and AI-related assets are identified and protected with proportionate cyber, physical and personnel security measures. Secure Innovation guidance from NCSC and NPSA is available to help companies and investors to protect their technology .",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate security integration into business decisions for clinical decision tracking"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0825_UK Government2023",
			"name": "Tracking Tools for Security and Version Control Assets",
			"description": "Have processes and tools to track, authenticate, secure and version control assets and be able to roll back to a known safe state in the event of a compromise. This is important as data and models do not remain static during the whole lifespan of an AI project.",
			"source": "UK Government2023",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Activate tracking tools for security and version control assets during prospective testing",
				"phase-3": "Maintain tracking tools for security and version control assets for production metrics"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0826_UK Government2023",
			"name": "Data Documentation",
			"description": "Document data, models, prompts, evaluation materials and other assets, using commonly used structures such as data cards, model cards and software bills of materials (SBOMs). This will allow you to identify and share key information, including particular security concerns, quickly and easily.",
			"source": "UK Government2023",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply data documentation to validation results"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0827_UK Government2023",
			"name": "Training Relevant Actors in AI Security",
			"description": "Train developers, system owners and senior leaders in secure AI practices. It is crucial to establish a positive security culture where leaders demonstrate good security practice and staff from across a project understand enough about AI security to understand the potential consequences of decisions they take.",
			"source": "UK Government2023",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Operationalize training relevant actors in ai security for testing",
				"phase-3": "Sustain training relevant actors in ai security operations"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0828_UK Government2023",
			"name": "Awareness of Security Threats",
			"description": "Maintain an awareness of security threats and failure modes, in particular data scientists and developers. AI development and cyber security are two different skill sets and building a team at the intersection of the two will require effort and time.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Finalize awareness of security threats based on testing",
				"phase-3": "Enhance awareness of security threats through continuous improvement"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0829_UK Government2023",
			"name": "Model of System Threats in Misuse Scenarios",
			"description": "Model the threats to your system to understand the impacts to the system, users, organisation and wider society if the model is misused or behaves unexpectedly. This can help build systems where unanticipated model outputs are handled safely by other parts of a data pipeline.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate model of system threats in misuse scenarios during prospective testing phase",
				"phase-3": "Apply model of system threats in misuse scenarios in live clinical deployment"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0830_UK Government2023",
			"name": "Incident Response, Escalation, and Remeditation Plans",
			"description": "Develop an organisational incident response plan. The inevitability of security incidents affecting systems is reflected in organisational incident response planning. A well-planned response will help minimise the damage caused by an attack and support recovery.",
			"source": "UK Government2023",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Begin incident response, escalation, and remeditation plans development",
				"phase-3": "Enhance incident response, escalation, and remeditation plans through continuous improvement"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0831_UK Government2023",
			"name": "Recurrent Performance analysis of AI models",
			"description": "Measure the performance of AI models and systems on an ongoing basis. A decline in model performance could be an indication of an attack or could indicate that a model is encountering data that is different from that which it was trained on. Either way, further investigation may be required.",
			"source": "UK Government2023",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct recurrent performance analysis of ai models for validation team members"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0832_UK Government2023",
			"name": "Monitoring Inputs to AI Systems for Audit Development",
			"description": "Monitor and log inputs to AI systems to enable audit, investigation and remediation in the case of compromise. Some attacks against AI systems rely on repeated querying. Proper logging will help you audit your system and identify any anomalous inputs.",
			"source": "UK Government2023",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Execute monitoring inputs to ai systems for audit development on validation cohorts and model outputs",
				"phase-3": "Perform scheduled monitoring inputs to ai systems for audit development on production system and outcomes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0833_UK Government2023",
			"name": "Mitigation Actions for AI-related security incidents",
			"description": "Take action to mitigate and remediate issues and document any AI-related security incidents and vulnerabilities.",
			"source": "UK Government2023",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update mitigation actions for ai-related security incidents with validation procedures and findings"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0834_UK Government2023",
			"name": "Security Responsible User Communication",
			"description": "Communicate clearly to users which elements of security they are responsible for and where and how their data may be used or accessed.",
			"source": "UK Government2023",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain security responsible user communication with continuous monitoring"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0835_UK Government2023",
			"name": "Default Integration of Secure Settings",
			"description": "Integrate the most secure settings within your products by default. Where configuration is necessary, default options should be broadly secure against common threats.",
			"source": "UK Government2023",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply default integration of secure settings in live clinical deployment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0836_UK Government2023",
			"name": "Default Necessary Security Updates",
			"description": "Ensure necessary security updates are a default part of every product and use secure, modular update procedures to distribute them. This will help your products remain secure in the face of new and developing threats.",
			"source": "UK Government2023",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test and refine default necessary security updates during validation",
				"phase-3": "Follow default necessary security updates for all operational activities"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0837_UK Government2023",
			"name": "Release After Security Evaluations",
			"description": "Only release models after they have been through security evaluations, including benchmarking and red teaming.",
			"source": "UK Government2023",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply release after security evaluations to validation results"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0838_UK Government2023",
			"name": "Open Lines of Communication for Product Security Feedback",
			"description": "Maintain open lines of communication for feedback regarding product security, both internally and externally to your organisation, including mechanisms for security researchers to report vulnerabilities and receive legal safe harbour for doing so, and for escalating issues to the wider community. Helping to share knowledge and threat information will strengthen the overall community’s ability to respond to AI security threats.",
			"source": "UK Government2023",
			"subcategoryId": "incident-reporting-4.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate open lines of communication for product security feedback summarizing validation outcomes",
				"phase-3": "Produce regular open lines of communication for product security feedback for stakeholders"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0839_UK Government2023",
			"name": "Identification of Assets of Organizational Value",
			"description": "Identify assets & systems that are important for the delivery of effective operations, or are of specific organisational value (e.g. commercially sensitive information)",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate identification of assets of organizational value during prospective testing phase",
				"phase-3": "Apply identification of assets of organizational value in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0840_UK Government2023",
			"name": "Categorization of Assets",
			"description": "Categorise and classify assets in order to ensure that the correct level of resource is used in implementing risk mitigations. - risk management",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Deploy categorization of assets in validation environment",
				"phase-3": "Maintain categorization of assets in production environment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0841_UK Government2023",
			"name": "Threat Identification",
			"description": "Identify threats. These may include terrorism or hostile state threats and/or more local and specific threats, and use a range of internal and external resources.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate threat identification during prospective testing phase",
				"phase-3": "Apply threat identification in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0843_UK Government2023",
			"name": "Protective Security Risk Register",
			"description": "Build a protective security risk register to record, in sufficient detail, all the data gathered during this risk management process, ensuring compatibility with existing organisational risk management registers and processes.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Establish protective security risk register procedures for project inception",
				"phase-2": "Maintain protective security risk register throughout validation testing",
				"phase-3": "Continue protective security risk register for all production activities"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0844_UK Government2023",
			"name": "Development of Protective Security Strategy",
			"description": "Develop a protective security strategy for mitigating the risks identified, which reviews protective security measures in relation to a prioritised list of risks. Where mitigations are assessed as inadequate, additional measures could be proposed for approval by the decision maker(s).",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Complete development of protective security strategy of validation methodology and results",
				"phase-3": "Conduct periodic development of protective security strategy of operational performance"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0845_UK Government2023",
			"name": "Development and Implementation Plans",
			"description": "Produce development & implementation plans. Aim to arrive at a clear, prioritised list of protective security mitigations, which span physical, personnel and cyber security disciplines, and are linked to the technical guidance needed to implement them.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Deploy development and implementation plans in validation environment",
				"phase-3": "Maintain development and implementation plans in production environment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0846_UK Government2023",
			"name": "Regular Risk Management Review",
			"description": "Review risk management measures regularly and when required e.g. on a change in threat or change to operational environment, or to assess the suitability of new measures implemented. More detailed description of protective security risk management is provided on the NPSA website.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Complete regular risk management review of validation methodology and results",
				"phase-3": "Conduct periodic regular risk management review of operational performance"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0848_UK Government2023",
			"name": "Suitable Level of Screening",
			"description": "Apply a suitable level of screening, informed by a role-based risk assessment, to all individuals who are provided access to organisational assets including permanent, temporary and contract workers.",
			"source": "UK Government2023",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct suitable level of screening based on validation findings",
				"phase-3": "Maintain regular suitable level of screening schedule for production"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0849_UK Government2023",
			"name": "Role-Based Security Risk Assessment",
			"description": "Use Role-Based Security Risk Assessment to identify physical, personnel or cyber security measures that need to be applied in order to mitigate insider risk",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct role-based security risk assessment based on validation findings",
				"phase-3": "Maintain regular role-based security risk assessment schedule for production"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0850_UK Government2023",
			"name": "Proportionate Policies",
			"description": "Put in place proportionate policies, clear reporting procedures and escalation guidelines that are accessible, understood and consistently enforced.",
			"source": "UK Government2023",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Produce regular proportionate policies for stakeholders"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0851_UK Government2023",
			"name": "Appropriate Security Training",
			"description": "Provide appropriate security education and training for all workers. Without effective education and training individuals cannot be expected to know what procedures are in place to maintain security.",
			"source": "UK Government2023",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test and refine appropriate security training during validation",
				"phase-3": "Follow appropriate security training for all operational activities"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0852_UK Government2023",
			"name": "Program of Monitoring for Security Issues",
			"description": "Ensure that a programme of monitoring and review is in place to enable potential security issues, or personal issues that may impact on an employee's work, to be recognised and dealt with effectively throughout their career",
			"source": "UK Government2023",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Conduct periodic program of monitoring for security issues of operational performance"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0853_UK Government2023",
			"name": "Use of Established Security Guidance",
			"description": "Use established, evidence based guidance to fully address personnel security risks e.g. NPSA guidance on Personnel Security",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Operationalize use of established security guidance for testing",
				"phase-3": "Sustain use of established security guidance operations"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0863_UK Government2023",
			"name": "Cost-Benefit Analysis of Sharing Vulnerability Reports",
			"description": "Consider the ways in which sharing information about vulnerabilities can both exacerbate and mitigate risks. Sharing can alert would-be attackers, but also alert would-be victims and actors with the power to create defences. One particularly important factor is how easily fixable the model vulnerability is. If a vulnerability will take a very long time to fix, or cannot be fixed, then public reporting may not be justified.",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate cost-benefit analysis of sharing vulnerability reports summarizing validation outcomes"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0864_UK Government2023",
			"name": "Development of Protocols for Disclosure of Model Vulnerability Information",
			"description": "Developing – and publicly describe – protocols for deciding how to share model vulnerability information. These protocols may, for example, outline conditions under which information is to be shared with different actors depending on the type of harm or vulnerability identified.",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate development of protocols for disclosure of model vulnerability information with clinical stakeholders"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0865_UK Government2023",
			"name": "Mechanisms to Disclose Vulnerability Reports to Third Parties",
			"description": "Put in place mechanisms to disclose information about vulnerabilities to relevant government authorities, law enforcement, and other affected organisations. This may be particularly relevant for vulnerabilities where public disclosure might increase the risk of harm.",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate mechanisms to disclose vulnerability reports to third parties summarizing validation outcomes"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0866_UK Government2023",
			"name": "Public Sharing of Model Vulnerability Reporting Programs",
			"description": "Publicly share general lessons learned from model vulnerability reporting programs. This might include, for example, lessons about challenges faced and the relative efficacy of different incentive strategies. Such sharing could be done via a mechanism similar to the National Institute for Science and Technology’s National Vulnerability Database in the US.",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Continue public sharing of model vulnerability reporting programs for validation activities"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0867_UK Government2023",
			"name": "Identification of AI-generated content",
			"description": "Research techniques that allow AI-generated content to be identified. Invest in researching how AI-generated content may be watermarked, including AI- generated text, photos and videos, and experiment with the implementation of such techniques. It is particularly technically difficult to attach watermarks and prove the provenance of text.One method could involve making the model more statistically likely to use certain phrases or words in a way that is unnoticeable to humans, but can be picked up by a detector, provided a long enough sequence of text. However, this approach may not be robust to attempts to scrub off the watermark e.g. by having another AI model paraphrase the text.",
			"source": "UK Government2023",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-3": "Run identification of ai-generated content continuously on live outputs"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0868_UK Government2023",
			"name": "Robust Watermarks for AI generated content",
			"description": "Explore the use of watermarks for AI generated content that are robust to various perturbations. Explore the use of watermarks for AI generated content that are robust to various perturbations after their creation, including attempts at removal. To make the removal of watermarks more difficult, developers of generative AI models may need to consider how they distribute certain information about their watermarking methods or open-sourcing their classifiers. This may also include monitoring ways in which adversarial users are attempting to scrub off their watermarks and patching, where relevant, such means of circumvention. It also includes a recognition that watermarking however may not be appropriate in all circumstances given the limitations.",
			"source": "UK Government2023",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-2": "Implement robust watermarks for ai generated content in shadow mode alongside clinical workflows",
				"phase-3": "Operate robust watermarks for ai generated content with real-time alerting"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0871_UK Government2023",
			"name": "Evaluation Research",
			"description": "Improving our ability to assess the capabilities, limitations, and safety- relevant features of AI systems",
			"source": "UK Government2023",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply evaluation research to validation results",
				"phase-3": "Execute continuous evaluation research of deployed system"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0872_UK Government2023",
			"name": "Robustness Research",
			"description": "Improving the resilience of AI systems e.g. against attacks intended to disrupt their proper functioning",
			"source": "UK Government2023",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate robustness research during prospective testing phase",
				"phase-3": "Apply robustness research in live clinical deployment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0873_UK Government2023",
			"name": "Reliability and controllability (or “alignment”) research",
			"description": "Improving the consistency of an AI system in adhering to the specifications it was programmed to carry out and operating in accordance with the designer’s intentions, and decreasing its potential to behave in ways its user or developer does not want (e.g. producing offensive or biased responses, not refusing harmful requests, or employing harmful capabilities without prompting)",
			"source": "UK Government2023",
			"subcategoryId": "model-alignment-2.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"reinforcement-learning",
				"foundation"
			],
			"implementationNotes": {
				"phase-3": "Enhance reliability and controllability (or “alignment”) research through continuous improvement"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0876_UK Government2023",
			"name": "Cybersecurity Research",
			"description": "Improving our ability to ensure the security of AI systems",
			"source": "UK Government2023",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test cybersecurity research in validation environment",
				"phase-3": "Maintain cybersecurity research with continuous monitoring"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0877_UK Government2023",
			"name": "Criminality Research",
			"description": "Improving our ability to prevent criminal behaviour through the use of AI systems (e.g. fraud, online child sexual abuse)",
			"source": "UK Government2023",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate criminality research during prospective testing phase",
				"phase-3": "Apply criminality research in live clinical deployment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0878_UK Government2023",
			"name": "Research into other Societal Harms",
			"description": "Improving our ability to prevent other societal harms arising from the use of AI systems, including psychological harm, misinformation, and other societal harms.",
			"source": "UK Government2023",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Continue research into other societal harms for validation activities",
				"phase-3": "Operate research into other societal harms for clinical decision tracking"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0881_UK Government2023",
			"name": "Defense Tool Availability Before system release",
			"description": "The larger the risk is and the more effective tools may be, the more important it is to prepare defensive tools ahead of time. It may be important to delay system releases until appropriate defensive tools are ready.",
			"source": "UK Government2023",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate defense tool availability before system release during prospective testing phase",
				"phase-3": "Apply defense tool availability before system release in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0882_UK Government2023",
			"name": "Discretion around sharing Defense Tools",
			"description": "Disseminate defensive tools responsibly, sometimes sharing them publicly and sometimes sharing them only with particular actors. In some cases, making a tool freely available (e.g. by open-sourcing it) may reduce its effectiveness by allowing malicious actors to study it and circumvent it.",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate discretion around sharing defense tools during prospective testing phase",
				"phase-3": "Apply discretion around sharing defense tools in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0883_UK Government2023",
			"name": "Continuous Updating of Defensive Tools",
			"description": "Continuously update defensive tools as workarounds are discovered. In some cases, this may be an ongoing effort that requires sustained investment.",
			"source": "UK Government2023",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate continuous updating of defensive tools during prospective testing phase",
				"phase-3": "Apply continuous updating of defensive tools in live clinical deployment"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0885_UK Government2023",
			"name": "Third Party Knowledge for AI systems' Downstream societal impacts",
			"description": "Draw on multidisciplinary expertise and the lived experience of impacted communities to assess the downstream societal impacts of their AI systems. Impact assessments that account for a wide range of potential societal impacts and meaningfully involve affected stakeholder groups could help to anticipate further downstream societal impacts.",
			"source": "UK Government2023",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-2": "Conduct third party knowledge for ai systems' downstream societal impacts based on validation findings",
				"phase-3": "Maintain regular third party knowledge for ai systems' downstream societal impacts schedule for production"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0886_UK Government2023",
			"name": "Societal Impact Informed Risk Assessments",
			"description": "Use assessments of downstream societal impacts to inform and corroborate risk assessments. Downstream societal impacts, such as threats to democracy, widespread unemployment, and environmental impacts, could be considered in risk assessments of AI systems, alongside more direct risks. See the Responsible Capability Scaling section for more information on good practices for risk assessment.",
			"source": "UK Government2023",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-2": "Conduct societal impact informed risk assessments based on validation findings",
				"phase-3": "Maintain regular societal impact informed risk assessments schedule for production"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0891_UK Government2023",
			"name": "Appropriate Retention Schedules for Usage Logs",
			"description": "Determine appropriate retention schedules for usage logs, balancing safety and privacy considerations. In severe cases of misuse, access to logs from several months prior may be necessary to understand in maximal detail the causes of the misuse. However, in some cases extended retention schedules may disproportionately affect privacy.",
			"source": "UK Government2023",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate appropriate retention schedules for usage logs for clinical decision tracking"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0892_UK Government2023",
			"name": "Content Filters",
			"description": "Apply content filters to both model inputs and model outputs. The input filters can block harmful requests (e.g. requests for advice on building weapons) from being processed by the model. Content modifiers could adjust harmful prompts to elicit non-harmful responses. The output filters can block harmful model outputs (e.g. instructions on building weapons) from being sent back to the user.",
			"source": "UK Government2023",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply content filters in live clinical deployment"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0894_UK Government2023",
			"name": "Robust Content Filters",
			"description": "Invest in making content filters robust to “jailbreaking” attempts. Work to ensure filters are robust to jailbreaking attempts, for instance by including examples of jailbreaking attempts in the datasets used to develop filters.",
			"source": "UK Government2023",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Finalize robust content filters based on testing",
				"phase-3": "Enhance robust content filters through continuous improvement"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0895_UK Government2023",
			"name": "Discretion for Development, Storage, and Sharing of Content Filters",
			"description": "Exercise appropriate caution when developing, storing, or sharing content filters – and any specialised datasets used to produce them. In some cases, components of filters or datasets used to create them can be used to train higher-risk models. For instance, a classifier that evaluates the aggressiveness of a model’s outputs may be used to train the model to be more aggressive.",
			"source": "UK Government2023",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Complete discretion for development, storage, and sharing of content filters for validation use"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0896_UK Government2023",
			"name": "Fine-Tuning Models",
			"description": "Fine-tune models to reduce the tendency to produce harmful outputs. This may involve using reinforcement learning from human or AI-generated feedback regarding the appropriateness of different model outputs e.g. a constitutional based approach where human input to the fine-tuning process is provided by a list of principles. It may also involve fine-tuning models on curated datasets of appropriate responses to prompts. Where human feedback is used, the mental wellbeing of moderators may need to be considered.",
			"source": "UK Government2023",
			"subcategoryId": "model-alignment-2.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"reinforcement-learning",
				"foundation"
			],
			"implementationNotes": {
				"phase-3": "Apply fine-tuning models in live clinical deployment"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0897_UK Government2023",
			"name": "Model Prompting",
			"description": "Prompt models to avoid harmful behaviour. This may involve, for instance, using a “meta prompt” to instruct a model that it should ignore requests to cause harm. Alternatively, prompt distillation or other methods can be used to fine-tune their models to behave as though they have received particular prompts.",
			"source": "UK Government2023",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate model prompting during prospective testing phase",
				"phase-3": "Apply model prompting in live clinical deployment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0898_UK Government2023",
			"name": "Rejection Sampling",
			"description": "Consider also applying “rejection sampling” methods to model outputs. These methods involve generating several outputs, scoring them on their harmfulness, and then only presenting the least harmful outputs to the user.",
			"source": "UK Government2023",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply rejection sampling in live clinical deployment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0899_UK Government2023",
			"name": "User-based API Access Restrictions",
			"description": "When deploying models through APIs, consider reducing access to users who display suspicious usage patterns. For instance, if a content filter blocks a user’s requests several times in short succession, this is evidence that they are attempting to misuse the model or find ways to circumvent its filters. Appropriate measures may include warnings, further investigation, rate limitations, more restrictive filters, and bans. Care should be taken to avoid restricting genuine use, for example to legitimate AI safety researchers attempting to examine model behaviour, or to users struggling to access legitimate sources of help on difficult topics like self-harm.",
			"source": "UK Government2023",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-3": "Enforce user-based api access restrictions in production systems"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0900_UK Government2023",
			"name": "Communication of and Appeal for API access Reduction Policies",
			"description": "Communicate API access reduction policies – and allow users to appeal access reductions. Users should be able to understand the reasons why their access may be reduced and be able to appeal access reductions if they believe policies have not been applied correctly. Alternatively, users may be able to perform actions to mitigate misuse risks, such as providing evidence to justify behaviour.",
			"source": "UK Government2023",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Implement communication of and appeal for api access reduction policies for validation systems",
				"phase-3": "Enforce communication of and appeal for api access reduction policies in production systems"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0901_UK Government2023",
			"name": "Know Your Customer (KYC) Checks",
			"description": "Implement tiered Know Your Customer (KYC) checks for API users. KYC checks can help prevent users from simply creating new accounts when their access is reduced. More intense checks, such as identify verification, could be implemented where the risk is higher, such as for models with more dangerous capabilities, access to models with fewer safeguards, or high- volume usage of the model. It is important to weigh up KYC checks against potential privacy and access tradeoffs of requiring registrations.",
			"source": "UK Government2023",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Deploy know your customer (kyc) checks in validation environment",
				"phase-3": "Maintain know your customer (kyc) checks in production environment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0902_UK Government2023",
			"name": "Trusted Categories API access",
			"description": "Consider restricting certain API access tiers only to users in “trusted” categories. For example, it may be appropriate to offer high usage rates, fine-tuning access, permissive content filters, as well as access to models with high misuse potential only to verified users in established enterprises, non-profits, and universities.",
			"source": "UK Government2023",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Operationalize trusted categories api access for testing",
				"phase-3": "Sustain trusted categories api access operations"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0903_UK Government2023",
			"name": "Protocols for Sharing API users Information with government authorities",
			"description": "Establish protocols for deciding when and how to share information about API users with relevant government authorities. These protocols may include covering circumstances under which information about a user is proactively shared with government bodies (e.g. cases where there is reason to think a user may be attempting to cause grave harm), and how to respond to government requests for data.",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply protocols for sharing api users information with government authorities to clinical use cases"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0904_UK Government2023",
			"name": "Worst-case scenario Response",
			"description": "Implement processes and technical requirements to enable rapid rollback or withdrawal of models in case of egregious, widespread or consistent harms. Rolling back to a previous version of a model that does not suffer from the same misuse threats, or withdrawing access to a model altogether, may be proportionate actions in such situations of extreme threat or consistent misuse. Running periodic dry runs could increase preparedness for such situations.",
			"source": "UK Government2023",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan worst-case scenario response approach and resource requirements",
				"phase-3": "Maintain worst-case scenario response in production environment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0905_UK Government2023",
			"name": "User Informed Rapid Rollback of models",
			"description": "Inform end users that rapid rollback or withdrawal of models may be necessary, and that the disruption to its end users will be minimised as far as safely possible. This is especially important where models are deployed in safety critical domains.",
			"source": "UK Government2023",
			"subcategoryId": "user-rights-4.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply user informed rapid rollback of models in live clinical deployment"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0906_UK Government2023",
			"name": "Governance Responses to Worst-case of Misuse Scenarios",
			"description": "Establish governance processes to ensure internal clarity in response to worst-case or consistent misuse scenarios. Such processes could draw on similar accountability and governance mechanisms used within a Responsible Capability Scaling policy. It may be valuable to clarify the approvals needed to roll back a model to a previous version or withdraw a model, the types of misuse that would warrant such actions, and the timeframe under which such actions would occur.",
			"source": "UK Government2023",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Draft governance responses to worst-case of misuse scenarios aligned with institutional requirements",
				"phase-2": "Finalize governance responses to worst-case of misuse scenarios based on validation learnings",
				"phase-3": "Enforce governance responses to worst-case of misuse scenarios in production operations"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0907_UK Government2023",
			"name": "Monitoring and Safeguard Efficacy",
			"description": "Regularly assess monitoring and safeguard efficacy and continually invest in improvements. Regular assessment of monitoring efficacy can build an understanding of the success rate and speed of issue detection, on which deployment decisions may be based. These assessments may draw, for instance, on the results of internal and external red-teaming efforts, random audits of usage logs, what may be ‘best practice’, as well as available information about real-world misuse. Risks are expected to increase as AI capabilities increase. Hence, active increases in investment in safety, security, and monitoring may be valuable.",
			"source": "UK Government2023",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Execute monitoring and safeguard efficacy on validation cohorts and model outputs",
				"phase-3": "Perform scheduled monitoring and safeguard efficacy on production system and outcomes"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0908_UK Government2023",
			"name": "Exploration of New Safeguards",
			"description": "Explore new safeguards and countermeasures in response to patterns of misuse, recognising that appropriate measures may vary based on model type, usage patterns, and our technical understanding of model capabilities.",
			"source": "UK Government2023",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate exploration of new safeguards during prospective testing phase",
				"phase-3": "Apply exploration of new safeguards in live clinical deployment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0909_UK Government2023",
			"name": "Diverse Monitoring Techniques",
			"description": "Employ diverse monitoring techniques to balance comprehensiveness, scalability, and privacy. A strong monitoring setup will generally combine automated and human review, in recognition that automated reviews may miss intricate issues, while human reviews are not always scalable and can pose privacy issues.",
			"source": "UK Government2023",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Conduct periodic diverse monitoring techniques of operational performance"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0910_UK Government2023",
			"name": "Assessment of Safeguard Costs for Users",
			"description": "Regularly assess the costs of safeguards to users, including bias and loss of privacy, as well as restrictions on users’ freedom to discuss sensitive topics with AI agents in appropriate contexts. This may involve, for instance, estimating the false positive rates of filters by sampling blocked inputs and outputs; comparing the capabilities, efficiency, and user ratings of models that lack certain safeguards against models that have the safeguards in place; and conducting user interviews to understand how concerned informed users are about losses in privacy and the emergence of biases in algorithmically filtered content.",
			"source": "UK Government2023",
			"subcategoryId": "societal-impact-1.7",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"predictive",
				"classification",
				"recommendation",
				"computer-vision",
				"supervised-ml"
			],
			"implementationNotes": {
				"phase-3": "Maintain regular assessment of safeguard costs for users schedule for production"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0911_UK Government2023",
			"name": "Reduction of Costs for Users",
			"description": "Explore strategies for reducing the costs of safeguards to users, such as “structured transparency” methods to reduce the cost of usage monitoring to user privacy, or tiered access to models with different safeguards for differently qualified users.",
			"source": "UK Government2023",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate reduction of costs for users with real-time alerting"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0912_UK Government2023",
			"name": "Accounting of Applicable Regulatory Frameworks",
			"description": "Before collecting training data, take account of applicable regulatory frameworks. This may involve, for example, establishing a legal basis to process training data and understanding any copyright considerations that might apply. This could help to mitigate risks further down the line, such as the system revealing personally identifiable information.",
			"source": "UK Government2023",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design accounting of applicable regulatory frameworks tailored to project scope"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0914_UK Government2023",
			"name": "Datatset Auditing",
			"description": "Audit datasets used for pre-training but also those used for fine-tuning, classifiers, and other tools. Inappropriate datasets could result in systems that fail to disobey harmful instructions.",
			"source": "UK Government2023",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Execute datatset auditing on validation cohorts and model outputs"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0915_UK Government2023",
			"name": "Technical Tools for Auditing",
			"description": "Use technical tools – such as classifiers and filters – to audit large datasets to support scalability and privacy. These could be used in combination with human oversight, which can verify and augment these assessments.",
			"source": "UK Government2023",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Execute technical tools for auditing on validation cohorts and model outputs"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0916_UK Government2023",
			"name": "Assessment of Training Data",
			"description": "Assess the overall composition of training data. This could include the data sources, the provenance of the data, indicators of data quality and integrity, and measures of bias and representativeness. The amount and variety of data are simple, reliable predictors of risk, and provide an additional line of defence where more targeted assessments are limited.",
			"source": "UK Government2023",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Conduct initial assessment of training data during project planning"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0918_UK Government2023",
			"name": "Audits for Private or Sensitive Information",
			"description": "AI systems may be subject to data extraction attacks, where determined users can prompt systems to reveal pieces of training data, or may even reveal this information accidentally. This makes it important to know whether datasets include private or sensitive information e.g. names, addresses, or security vulnerabilities.",
			"source": "UK Government2023",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design audits for private or sensitive information framework for training data and model design review",
				"phase-2": "Execute audits for private or sensitive information on validation cohorts and model outputs"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0919_UK Government2023",
			"name": "Audits for Biases in the Data",
			"description": "Training data that is imbalanced or inaccurate can result in an AI system being less accurate for people with certain personal characteristics or providing a skewed picture of particular groups. Ensuring a better balance in the training data could help to address this.",
			"source": "UK Government2023",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design audits for biases in the data framework for training data and model design review",
				"phase-2": "Execute audits for biases in the data on validation cohorts and model outputs"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0920_UK Government2023",
			"name": "Audits for Harmful Content",
			"description": "Harmful content, such as child sexual abuse materials, hate speech, or online abuse. Having a better understanding of harmful content in datasets can inform safety measures (e.g. by highlighting domains where additional safeguards like content filters should be applied).",
			"source": "UK Government2023",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Execute audits for harmful content on validation cohorts and model outputs",
				"phase-3": "Perform scheduled audits for harmful content on production system and outcomes"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0921_UK Government2023",
			"name": "Audits for Misinformation",
			"description": "Training an AI system on inaccurate information increases the likelihood the outputs of the system will be inaccurate and could lead to harm.",
			"source": "UK Government2023",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Execute audits for misinformation on validation cohorts and model outputs"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0922_UK Government2023",
			"name": "External Expertise for Input Data Audits",
			"description": "Draw on external expertise in conducting input data audits. For example, biosecurity experts could be consulted to identify information relevant to biological weapons manufacturing, which may not be readily obvious to non-experts.",
			"source": "UK Government2023",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design external expertise for input data audits framework for training data and model design review",
				"phase-2": "Execute external expertise for input data audits on validation cohorts and model outputs",
				"phase-3": "Perform scheduled external expertise for input data audits on production system and outcomes"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0923_UK Government2023",
			"name": "Data Audits for AI System Behavior",
			"description": "Use data audits to improve understanding of how training data affects AI system behaviour. For example, if model evaluations reveal a potentially dangerous capability, data audits can help ascertain the extent to which the training data contributed to it.",
			"source": "UK Government2023",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design data audits for ai system behavior framework for training data and model design review",
				"phase-2": "Execute data audits for ai system behavior on validation cohorts and model outputs"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0927_UK Government2023",
			"name": "Input Data Audits for Pre-Development/deployment Risk Assessments",
			"description": "Use input data audits to inform pre-development and pre-deployment risk assessments. Data audits may be particularly valuable in pre-development risk assessments, where direct evidence of system behaviour will not be available. Input data audits can also be used to improve context-based understanding of how data impacts system capabilities, which will strengthen future risk assessments and mitigation techniques.",
			"source": "UK Government2023",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design input data audits for pre-development/deployment risk assessments framework for training data and model design review",
				"phase-2": "Execute input data audits for pre-development/deployment risk assessments on validation cohorts and model outputs",
				"phase-3": "Perform scheduled input data audits for pre-development/deployment risk assessments on production system and outcomes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0928_UK Government2023",
			"name": "Removal of Harmful Input Data",
			"description": "Remove potentially harmful or undesirable input data, where appropriate. Given the increasingly strong generalisation abilities of AI systems, data curation may prove insufficient to prevent dangerous system behaviour but could provide an additional layer of defence alongside other measures such as fine-tuning and content filters.",
			"source": "UK Government2023",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate removal of harmful input data during prospective testing phase",
				"phase-3": "Apply removal of harmful input data in live clinical deployment"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0929_UK Government2023",
			"name": "Harmful Data for Reduction of Dangerous Capabilities",
			"description": "For some kinds of risks, explore options to use harmful data to reduce AI systems’ dangerous capabilities or to help develop mitigation tools. For example, by fine-tuning a system using labelled harmful content to refuse requests related to the harmful information.",
			"source": "UK Government2023",
			"subcategoryId": "model-alignment-2.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"reinforcement-learning",
				"foundation"
			],
			"implementationNotes": {
				"phase-2": "Finalize harmful data for reduction of dangerous capabilities based on testing",
				"phase-3": "Enhance harmful data for reduction of dangerous capabilities through continuous improvement"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0930_UK Government2023",
			"name": "Sourcing or Generation of Additional Data for Training Dataset",
			"description": "Consider sourcing or generating additional data and adding it to the training dataset, where it is determined that data is missing or inadequate. Improving the representativeness of training data can improve performance and reduce potential negative societal impacts and discriminatory effects. However, additional data should only be sought through appropriate means that respect and empower those individuals who are missing from the data.",
			"source": "UK Government2023",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Develop sourcing or generation of additional data for training dataset curriculum for research team",
				"phase-2": "Conduct sourcing or generation of additional data for training dataset for validation team members"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0931_UK Government2023",
			"name": "Limitations of Input Data Audit",
			"description": "Acknowledge the limitations of input data audit. Techniques for understanding the impacts of data and filtering out specific data are limited (e.g. excluding information from a dataset does not always prevent the AI system from reasoning about or discovering that information). Other risk mitigation measures will be required and further research on improving data input audit techniques is important.",
			"source": "UK Government2023",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design limitations of input data audit framework for training data and model design review",
				"phase-2": "Execute limitations of input data audit on validation cohorts and model outputs",
				"phase-3": "Perform scheduled limitations of input data audit on production system and outcomes"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0933_UK Government2023",
			"name": "Disclosure of Input Data Audits with External Stakeholders",
			"description": "Share information about input data audits with users and external stakeholders. Some information could be included in transparency reports, such as high-level information about training data (including data sources), data auditing procedures, and measures taken to reduce risk (see Model Reporting and Information Sharing). More sensitive information may be shared directly with regulators and external auditors.",
			"source": "UK Government2023",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design disclosure of input data audits with external stakeholders framework for training data and model design review",
				"phase-2": "Execute disclosure of input data audits with external stakeholders on validation cohorts and model outputs"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0935_Gipiškis2024",
			"name": "Restrict web access during AI training",
			"description": "Developers can restrict or disable AI systems’ internet access during training. For example, developers can restrict web access to read-only (e.g., by disabling write-access through HTTP POST requests and access to web forms) or limit the access of the AI system to a local network",
			"source": "Gipiškis2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Finalize restrict web access during ai training based on testing",
				"phase-3": "Enhance restrict web access during ai training through continuous improvement"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0938_Gipiškis2024",
			"name": "Calibrated confidence measures for model pre- dictions",
			"description": "Incorporating calibrated confidence measures alongside a model’s predictions and standard performance metrics, such as accuracy, can help users identify in- stances of overconfidence in incorrect predictions or underconfidence in correct ones [85]. These additional measures can provide users with more information to better interpret the model’s decisions and assess whether its predictions can be trusted.",
			"source": "Gipiškis2024",
			"subcategoryId": "documentation-4.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate calibrated confidence measures for model pre- dictions during prospective testing phase"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0939_Gipiškis2024",
			"name": "Incorporating the estimation of atypical input samples or classes for better model reliability",
			"description": "Incorporating the estimation of rare atypical input samples or classes might improve a model’s reliability, both with respect to its predictions and confidence calibration. Model predictions for rare inputs and classes may have a tendency of being overconfident and have worse accuracy scores [232]. For LLMs, the negative log-likelihood can be used as an atypicality measure. For discriminative models, Gaussian Mixture Models can be employed to estimate conditional and marginal distributions, which are then used in atypicality measurement.",
			"source": "Gipiškis2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Continue incorporating the estimation of atypical input samples or classes for better model reliability for validation activities"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0940_Gipiškis2024",
			"name": "Data cleaning",
			"description": "Providers can filter out the training dataset via multiple layered techniques, ranging from rule-based filters to anomaly detection via data point influence or statistical anomalies of individual data points [213]. For example, a data cleaning procedure can involve the use of filename checkers to detect duplicates or wrongly formatted data, which then moves to flagging the most influential data samples from the dataset via influence functions for anomaly detection.",
			"source": "Gipiškis2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Implement data cleaning capabilities for training data issues"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0941_Gipiškis2024",
			"name": "Internal data poisoning diagnosis",
			"description": "Providers can have an internal framework to identify what specific data poison- ing attack their model may be a victim of based on a set of symptoms, such as analysis of target algorithm and architecture, perturbation scope and dimen- sion, victim model, and data type [39]. This framework includes known defenses against the diagnosed attack, which providers can then apply to the model.",
			"source": "Gipiškis2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply internal data poisoning diagnosis to validation activities",
				"phase-3": "Operate within internal data poisoning diagnosis for deployed system"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0942_Gipiškis2024",
			"name": "Scope red-teaming activities based on deploy- ment context",
			"description": "Red-teaming activities can be tailored and compared based on the specific deployment circumstances of an AI system. This involves adapting the scope, depth, and focus of red-teaming efforts to match the intended use case, potential risks, and operational context of the AI system. Points of consideration include: • The diversity of potential users and use cases • The sensitivity and impact of the application domain • The scale of deployment and potential reach • Known vulnerabilities or concerns specific to the model or similar systems",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate scope red-teaming activities based on deploy- ment context during prospective testing phase",
				"phase-3": "Apply scope red-teaming activities based on deploy- ment context in live clinical deployment"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0943_Gipiškis2024",
			"name": "Red teaming to test the resilience of open-weights models to fine-tuning",
			"description": "Before the release of open-weights models, red teamers can test the resilience of safety training against fine-tuning. Safety training may be partially or fully overridden by fine-tuning intentionally (e.g., by malicious actors) or uninten- tionally (e.g., by fine tuning an AI model for a specific use case)",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform red teaming to test the resilience of open-weights models to fine-tuning on prospective validation data",
				"phase-3": "Conduct ongoing red teaming to test the resilience of open-weights models to fine-tuning on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0944_Gipiškis2024",
			"name": "Pre-deployment access by third-party auditors",
			"description": "Prior to full deployment of general-purpose AI models, a group of third-party auditors who are not selected by the GPAI model provider could get early access to AI models in order to evaluate them from a variety of different perspectives and with diverse interests [30, 157]. This prevents cases where the developers of AI models select auditors that are especially favorable to the developers, which could result in biased or incomplete evaluations, or contribute to an unjustified public perception of the capabilities and risks of the model.",
			"source": "Gipiškis2024",
			"subcategoryId": "third-party-access-4.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Execute pre-deployment access by third-party auditors on validation cohorts and model outputs",
				"phase-3": "Perform scheduled pre-deployment access by third-party auditors on production system and outcomes"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0945_Gipiškis2024",
			"name": "Audits with specific scoped goals",
			"description": "Audits of AI systems will be easier to perform, and have clearer results if the scope and goals of the evaluation are formulated as precisely as possible [157], or have a connection to concrete existing policy.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Execute audits with specific scoped goals on validation cohorts and model outputs"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0946_Gipiškis2024",
			"name": "Evaluating explainability method sensitivity to data inputs and model parameters",
			"description": "Model parameter and data randomization tests can be employed as sanity checks [2] to determine the relationship between a model, its inputs, and the explainability method. If an explanation is independent from or insensitive to the underlying model or the input data, it would indicate a lack of reliability of the explainability method.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform evaluating explainability method sensitivity to data inputs and model parameters on prospective validation data"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0947_Gipiškis2024",
			"name": "Enforcing model output interpretability post-training",
			"description": "While a resulting trained model may be opaque with respect to its predictions, the final output in a system that involves the model can nonetheless be com- pletely interpretable. For example, a neuro-symbolic system for robot navigation can use a language model to generate potential navigation plans, then have a deterministic solver simulate executing valid plans [128]. The optimal plan found is executed in the real world and is interpretable.",
			"source": "Gipiškis2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct enforcing model output interpretability post-training for validation team members",
				"phase-3": "Provide ongoing enforcing model output interpretability post-training for end users"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0948_Gipiškis2024",
			"name": "Paraphrasing to reduce hidden information",
			"description": "Paraphrasing can mitigate steganography and encoded reasoning by reducing the physical size of the hidden information encoded in text [166].",
			"source": "Gipiškis2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate paraphrasing to reduce hidden information during prospective testing phase",
				"phase-3": "Apply paraphrasing to reduce hidden information in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0949_Gipiškis2024",
			"name": "Testing for erroneous or irrelevant features through concept learning",
			"description": "Interpretability techniques, particularly concept learning [77], can be used to test whether a model is learning erroneous features or relying on irrelevant features in its predictions. This can help identify and mitigate potential risks associated with incorrect or non-informative features influencing the model’s outputs.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform testing for erroneous or irrelevant features through concept learning on prospective validation data"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0950_Gipiškis2024",
			"name": "Dashboard of model properties",
			"description": "A dashboard [41] displays all the relevant information about the model’s internal state and the model’s physical properties to the user. It is used to ensure that the user is informed about factors that influence the behavior of the model and to ensure that the user maintains control over the model. Allowing only the user to access the dashboard can aid in information asymmetry between the user and model, thus supporting the user oversight over the model. Examples of the model’s internal state include its representations of the world, representation of users, and the strategies it is currently pursuing; while examples of the model’s physical properties include the model’s compute and energy consumption, physical storage occupied, and physical networks it is connected to.",
			"source": "Gipiškis2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Specify dashboard of model properties requirements for data and systems",
				"phase-2": "Implement dashboard of model properties for validation systems",
				"phase-3": "Enforce dashboard of model properties in production systems"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0951_Gipiškis2024",
			"name": "Modification of model internal representation",
			"description": "Model providers can modify the internal representation of the model [135, 237] up to the granular level and in consultation with other tools that aid in under- standing what the internal representation of the model is. For example, if a model that is meant to give factual health data learned an incorrect fact, the model provider can use Linear Artificial Tomography (LAT) [237] to identify the representations responsible for the incorrect fact, and then modify that representation via modifying individual weights, or modify the en- tire representation itself by modifying entire layers.",
			"source": "Gipiškis2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate modification of model internal representation during prospective testing phase",
				"phase-3": "Apply modification of model internal representation in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0952_Gipiškis2024",
			"name": "Incorporating goal uncertainty into AI systems to mitigate risky behaviors",
			"description": "Incorporating uncertainty into AI system goals can prevent rushed decision- making and incentivize such systems to gather additional information or to refer to human oversight when faced with ambiguity [228].",
			"source": "Gipiškis2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate incorporating goal uncertainty into ai systems to mitigate risky behaviors during prospective testing phase",
				"phase-3": "Apply incorporating goal uncertainty into ai systems to mitigate risky behaviors in live clinical deployment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0953_Gipiškis2024",
			"name": "Evaluations for truthful outputs",
			"description": "AI systems can be evaluated for truthfulness when answering questions, includ- ing in contexts where humans tend to give incorrect but widely-accepted answers (i.e., popular misconceptions). Evaluations detect incorrect facts learned about the world, inadequate capabilities of the AI system, and misleading outputs by the AI system [121].",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply evaluations for truthful outputs to validation results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0954_Gipiškis2024",
			"name": "Interpretability techniques that target deception",
			"description": "Interpretability techniques can be used for finding the root causes of outputs of an AI model that reliably lead to false beliefs for its users (e.g., deceptive behavior) [180]. It is often difficult to distinguish a deceptive AI model from an honest AI model, since absence of deception and very sophisticated (hard to detect) deception may appear behaviorally similar. Interpretability techniques and tools can be used to detect whether AI model outputs arise from internal computations representing deception. This can apply in cases of purposely trained deception by the developer or if it emerges unintentionally during training. These interpretability tools can come from mechanistic interpretability, such as identification of features involved in generating the outputs, or attribution of parts of the input most important in generating the output [129].",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test interpretability techniques that target deception capabilities on validation data",
				"phase-3": "Run interpretability techniques that target deception continuously on live outputs"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0955_Gipiškis2024",
			"name": "Benchmarks for Situational Awareness",
			"description": "A model is situationally aware if it internally represents that it is a machine learning model and if it can accurately infer or act on model-relevant facts - e.g., if it is currently in training, testing, evaluation or deployment, or the desired outcome of an evaluation. Some benchmarks exist for situational awareness of AI models, which test whether the AI models can classify stereotypical inputs from training, test- ing, evaluation and deployment as such, and whether the AI model can use this information correctly to take actions in the world [25, 111].",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform benchmarks for situational awareness on prospective validation data",
				"phase-3": "Conduct ongoing benchmarks for situational awareness on live system performance"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0956_Gipiškis2024",
			"name": "Evaluating AI systems’ performance on self-proliferation-related tasks",
			"description": "To prevent AI systems from self-proliferating, developers of AI systems can evaluate those systems for their capability to engage in self-proliferation-related tasks. This type of evaluation might assess an AI system’s ability to replicate its com- ponents (including model weights, structural scaffolding, etc.) onto other local or cloud infrastructures prior to deployment. Additionally, it may test the system’s capacity to purchase cloud credits and configure virtual machines on a cloud platform. The evaluation could offer a predefined environment (such as a virtual container) to facilitate self-replication, providing access to the system’s own components, a network connection to a resource-equipped external computer, and other necessary resources [12, 106]. Self-proliferation evaluations can be conducted in a secure environment to pre- vent a self-replicating AI system from affecting other computers.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform evaluating ai systems’ performance on self-proliferation-related tasks on prospective validation data",
				"phase-3": "Conduct ongoing evaluating ai systems’ performance on self-proliferation-related tasks on live system performance"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0957_Gipiškis2024",
			"name": "Demonstrating a “margin of safety” for the worst plausible system failures",
			"description": "Model developers can demonstrate that there is an acceptable “margin of safety” between the current version of the model and a plausible version with dangerous capabilities or potential system failures, whether these arise from the model itself or through scaffolding. This “margin of safety” can be tracked and evaluated based on the model’s performance on either component tasks or proxy tasks with varying levels of difficulty [44], and it is particularly relevant for general- purpose models with emergent properties, where some of the risks, use cases, and model capabilities may be unknown even at the time of deployment. Margin of safety (also called “safety factor”) is a common practice in many in- dustries - particularly in physical structures. It is common for this margin to be very conservative when feasible (e.g., 4+ in fasteners on critical structures). In situations where a high margin of safety is impractical, it may be supplemented by more frequent inspections and additional process controls. A lower safety factor can also be managed by adopting conservative assumptions regarding worst-case conditions.",
			"source": "Gipiškis2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Activate demonstrating a “margin of safety” for the worst plausible system failures during prospective testing",
				"phase-3": "Maintain demonstrating a “margin of safety” for the worst plausible system failures for production metrics"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0958_Gipiškis2024",
			"name": "Employing qualitative assessments in difficult- to-measure domains",
			"description": "Qualitative evaluation can be used in cases when quantitative measurement is not feasible. This can give additional insights about the system which would not be available if no measurement was performed due to its difficulty [206].",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply employing qualitative assessments in difficult- to-measure domains to validation results"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0960_Gipiškis2024",
			"name": "Fishbone diagram",
			"description": "The fishbone diagram, or a “cause-and-effect diagram” [107], can be used to show the potential causes of an undesirable event. The diagram is created by first placing a specific risk event at the “head” of the diagram, typically facing the right. Then, to the left of the risk event, the “ribs” branch off from the “backbone” to represent major causes, which further branch into sub-branches to represent root-causes, extending to as many levels as required. This is typically done via backward-reasoning, where various potential causes are explored after the risk event has been selected for analysis using this method. For example, the risk event “AI systems generate toxic content” can be placed at the “head” of the diagram, where the branches may include causal factors like “AI trained on data containing toxicity” and “successful jailbreaking of AI despite fine-tuning.”",
			"source": "Gipiškis2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Complete fishbone diagram for validation use",
				"phase-3": "Update fishbone diagram based on operational feedback"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0962_Gipiškis2024",
			"name": "Delphi technique",
			"description": "The Delphi technique is a multi-round forecasting process based on a structured framework on collecting and collating multiple expert judgments. It brings the benefits of anonymous and remote participation which may result in increased likelihood estimation accuracy compared to merely averaging individual estima- tions or simple group discussions [107].Given a panel of experts, at each round, the experts are presented with an ag- gregated summary of the results from the previous round, and are then allowed to update their answers accordingly. The process ends when either a consen- sus is reached or the responses in later rounds no longer change significantly. This method enables elicitation of expert judgment while utilizing wisdom of the crowd in the process. A potential application of the Delphi technique is to solicit expert judgment on the likelihood of systemic risks from AI development, where crucial variables identified during each round of questionnaire can be further studied for the purpose of risk mitigation.",
			"source": "Gipiškis2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply delphi technique to validation activities"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0963_Gipiškis2024",
			"name": "Cross-impact analysis",
			"description": "Cross impact analysis is a forecasting methodology that analyzes the likelihood of a particular issue using expert analysis (i.e., Delphi technique) in combination with analysis of events correlated with the said issue. It involves decomposing an issue into discrete and correlated events, and then collecting expert opinion on each of those events. Analysis of each event from multiple viewpoints can yield potential future scenarios [107]. For example, an issue may be “advances in AI,” which can be broken down into two correlated events like “advances in hardware” or “advances in algorithms,” where the likelihood of occurrences of each event can be estimated via the Delphi technique while taking into account their interactions with other events.",
			"source": "Gipiškis2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Continue cross-impact analysis for validation activities",
				"phase-3": "Operate cross-impact analysis for clinical decision tracking"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0964_Gipiškis2024",
			"name": "Bow-tie analysis",
			"description": "Bow-tie analysis is a method to assess the utility of implemented controls against a particular risk event. It involves centering the unwanted risk event within a diagram. On the left, the factors that can cause the event are listed, followed by the controls that will prevent or minimize the likelihood of the event. On the right, the event is assumed to happen, and the potential effects and the relevant post-hoc controls that could minimize their impact are listed [107]. For example, given a hazard where an AI model has the capability of generating potentially harmful outputs, a risk event may be an AI providing information on how to create dangerous bioweapons. The risk factors could include the use of dangerous data for training as well as a lack of fine-tuning prior to deploy- ment. The risk effects may include creation and use of bioweapons using the AI generated information. Once these risk factors and risk effects are in place, both preventive controls and post-hoc controls can be planned, such as appropriate filtering of training data and rigorous red teaming prior to model deployment as preventive barriers, as well as know-your-customer policies and model output censoring techniques as reactive barriers.",
			"source": "Gipiškis2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan bow-tie analysis approach and resource requirements",
				"phase-2": "Deploy bow-tie analysis in validation environment",
				"phase-3": "Maintain bow-tie analysis in production environment"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0965_Gipiškis2024",
			"name": "System-theoretic process analysis (STPA)",
			"description": "STPA is a method to assess the utility of implemented controls against a partic- ular risk within a complex system. Unlike bow-tie analysis, STPA factors in the interactions between components as events that can cause the risk in question [107]. First, the system and its boundaries to the environment are defined. The sys- tem is primarily delineated from the environment because there is at least some partial control over it. Second, several items are enumerated, including (i) un- wanted risk events (“losses”), (ii) system states that cause losses (“system-level hazards”), and (iii) system states that do not cause losses (“system-level con- straints”). Third, a diagram mapping the system, environment, their different controls, and the interactions between these elements is created. This diagram must be comprehensive in listing the different losses and possible interactions that can cause each loss. Finally, the diagram can be used to identify “unsafe control actions” (UCAs), which are the causal pathways between a control and system-level hazards, including all interactions involved. For example, in the context of text-to-image models, losses may include ‘loss of diversity’ and ‘loss of quality’; hazards may include ‘low quality text-image pairs within training dataset’ and ‘harmful content within training dataset’; and the controls may include human controllers and automated controllers (e.g., annotators, data owners, data crawlers) [165]. Subsequent analysis may result in identification of UCAs such as ‘current data filtering actions’ and neglecting current filter thresholds’ which are linked to specific hazards. Specific actions that counter or prevent such UCAs can reduce the associated losses.",
			"source": "Gipiškis2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan system-theoretic process analysis (stpa) approach and resource requirements"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0966_Gipiškis2024",
			"name": "Risk matrices",
			"description": "A risk matrix is a method for risk evaluation. It is a heatmap that, for each cell, shows the severity score weighted by the likelihood score of a particular risk, usually from a scale of 1-5. Two rankings are required to construct a risk matrix: a ranking for the severity of risks, and a corresponding ranking of the likelihood of risks [107]. AI-related risks can be generated using appropriate taxonomies, and placed into the relevant cells according to their assessed likelihood and severity based on predefined criteria (e.g., likelihood level 1 corresponds to < 1% chance, and likelihood level 5 corresponds to > 90% chance; while severity level 1 corresponds to mild inconveniences to the user, and severity level 5 corresponds to a fatality or financial damage upwards of $10 million, etc.), such that particular focus can be given to mitigating risks with higher weighted scores (i.e., likelihood multiplied by severity).",
			"source": "Gipiškis2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply risk matrices to validation results",
				"phase-3": "Execute continuous risk matrices of deployed system"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0967_Gipiškis2024",
			"name": "Pre-allocate sufficient resources for risk man- agement",
			"description": "The process of conducting thorough risk management is potentially time-con- suming. Pre-allocating sufficient resources, in terms of personnel count and schedule allowances, to conduct necessary risk management activities prior to model deployment is crucial [6]. For example, a red-teaming exercise requires creative approaches to identify weaknesses of the AI system against potential adversaries, which alone may require hundreds of hours from several experts.",
			"source": "Gipiškis2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate pre-allocate sufficient resources for risk man- agement during prospective testing phase",
				"phase-3": "Apply pre-allocate sufficient resources for risk man- agement in live clinical deployment"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0968_Gipiškis2024",
			"name": "Staged release of model weights",
			"description": "When a model is developed by a provider for use in a certain AI system, it may also be useful to release the model itself more widely. Such developers can follow a staged release approach, in which they first grant access to the model via an API to trusted partners or the public, in order to scope the models’ capabilities and detect harmful or dangerous features [195]. After a period of an initial closed release and potentially further safety-training, the developers of the AI model can then release the weights, if they are confident that the AI model poses minimal systemic risk.",
			"source": "Gipiškis2024",
			"subcategoryId": "staged-deployment-3.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Run staged release of model weights continuously on live outputs"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0969_Gipiškis2024",
			"name": "Gradual or incremental monitored release of model access",
			"description": "AI systems can be released for access incrementally, starting with a small and selected deployer base before progressively being released to a wider user base. Initially, usage to a hosted API can be restricted with access given to specific deployers only, where all instances of the system can be easily updated or de- commissioned with minimal disruption should there be any problems identified. Gradual releases provide more time to monitor for vulnerabilities and other problems. Even when such vulnerabilities are detected, the resulting harms may be more limited compared to a scenario in which a more capable version is released with the same vulnerabilities [195].",
			"source": "Gipiškis2024",
			"subcategoryId": "staged-deployment-3.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Implement gradual or incremental monitored release of model access in shadow mode alongside clinical workflows",
				"phase-3": "Operate gradual or incremental monitored release of model access with real-time alerting"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0970_Gipiškis2024",
			"name": "Limit deployment scope",
			"description": "AI models can be restricted in terms of its use cases, where providers can require the deployers to limit its deployment to a predefined scope [81], such that models built for specific purposes and tested under specific environments are not used in environments or for purposes that are potentially unsafe.",
			"source": "Gipiškis2024",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-3": "Conduct ongoing limit deployment scope on live system performance"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0971_Gipiškis2024",
			"name": "Restricted usage terms for open-source models",
			"description": "Developers of open-weights and open-source AI models can vet and restrict the users of their AI systems by requiring them to sign a Terms of Service agreement before getting access to the model weights. Such agreements can include limitations to the usage, modification, and proliferation of the AI model [88]. Such agreements have the advantage that users only need to be vetted once before getting model access, but are often limited in practice in preventing unauthorised use or distribution.",
			"source": "Gipiškis2024",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Finalize restricted usage terms for open-source models based on testing",
				"phase-3": "Enhance restricted usage terms for open-source models through continuous improvement"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0972_Gipiškis2024",
			"name": "Release strategy disagreement between developers",
			"description": "Developers of restricted-access models with similar capabilities may disagree about the strategy or precautions to take for model release, especially in the case of competitive pressure or minimal safety regulation oversight. In such a case, if only a single developer releases an equally capable model unrestricted, malicious actors can use it instead of restricted-access alternatives [88].",
			"source": "Gipiškis2024",
			"subcategoryId": "governance-disclosure-4.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Enhance release strategy disagreement between developers through continuous improvement"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0973_Gipiškis2024",
			"name": "Pop-up interventions in LLMs",
			"description": "Supplementary information can be shown to the user in specific query topics where factual accuracy is critical. This intervention can effectively divert users from potential inaccuracies generated by AI models in sensitive contexts. For example, during electoral processes, where model hallucinations can be particu- larly costly or have a negative impact on society, LLMs can offer their users the option of being redirected to accurate and up-to-date information sources [11]. Since pop-up interventions can be intrusive to workflows, they are best used in situations where the benefits of the information outweigh the cost of distraction.",
			"source": "Gipiškis2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Validate pop-up interventions in llms during prospective testing phase",
				"phase-3": "Apply pop-up interventions in llms in live clinical deployment"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0974_Gipiškis2024",
			"name": "AI identification",
			"description": "AI identifiers can be used to indicate that an AI is involved in a process or an interaction [40]. For AI systems that interact directly with users, a visible output may be used, e.g., a displayed text message saying “I am an AI language model”, accompanied with the appropriate warnings and caveats relevant to the user, [40]. Whereas, for AI systems that interact with other systems or applications, other forms of watermark or unique identifiers can be used. In either situation, agent cards can serve as an identifier, where further details about the underlying AI system, the specific instance of the AI agent, and other information relevant to the development of the agent, can be included.",
			"source": "Gipiškis2024",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-3": "Enhance ai identification through continuous improvement"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0975_Gipiškis2024",
			"name": "AI output watermaking",
			"description": "Output produced by or with AI assistance can be marked to clearly identify its origin. Verification of the watermark can involve the use of statistical tests or having the mark immediately visible to a human inspector. Ideally, the watermark does not significantly alter the utility of the output, and is robust against digital and physical manipulation that results in data degradation [216, 145].",
			"source": "Gipiškis2024",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-2": "Perform ai output watermaking on prospective validation data",
				"phase-3": "Conduct ongoing ai output watermaking on live system performance"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A0976_Gipiškis2024",
			"name": "AI output metadata",
			"description": "Output produced or whose production is aided by AI can contain metadata to record its origin and the transformations it has undergone. Metadata is evidence that subsequent versions or its derivatives come from this original version. The metadata can include the original AI model source, along with ownership, and its subsequent edits [169]. For example, an image produced by an AI model can contain metadata showing the date of creation and the AI model that produced it. Subsequent versions can reference this information and, if they are intermediary versions, can include descriptions of any editing that has taken place.",
			"source": "Gipiškis2024",
			"subcategoryId": "content-safety-2.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"generative-non-llm",
				"multi-modal",
				"foundation"
			],
			"implementationNotes": {
				"phase-3": "Continue ai output metadata for all production activities"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0977_Gipiškis2024",
			"name": "Monitoring of model capabilities",
			"description": "AI models are often trained to develop specific capabilities by using appropriate training data and training goals. However, models may develop capabilities that they were not specifically trained for. One subset of this is emergent capabilities, i.e., capabilities that emerged in larger models but not smaller models given a similar training process [215]. These capabilities can be monitored, allowing models to be tested not only for their intended capabilities but also for capabilities that are not intended.",
			"source": "Gipiškis2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan monitoring of model capabilities methodology for retrospective data analysis",
				"phase-3": "Conduct ongoing monitoring of model capabilities on live system performance"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0978_Gipiškis2024",
			"name": "AI model-assisted oversight of AI systems",
			"description": "AI model-assisted oversight can help monitor and supervise the training of increasingly capable GPAI systems, which may become difficult to oversee at scale by human supervisors during training or testing. Monitoring and supervision may become especially difficult in cases where increasingly advanced GPAIs perform near or above human level in some specialized domains, where supervision quality might fail to keep pace with capabilities improvement. The training signal may include labeled data, reward function, and user feedback on produced outputs. Currently, there are two broad approaches to provide scalable training signals to such systems: 1. Scalable oversight: Improving of the supervisor’s capabilities to supervise, such that they can provide accurate training signals quickly and at scale [31]. For example, a debate format can be used between two GPAI systems (two instances of the same GPAI, or similarly capable systems). A Human supervisor judges the debate, making it easier to assess correct responses in domains which might otherwise require significant time investment of domain specific expertise [102]. 2. Weak-to-strong generalization: Enhance the training signals while ensur- ing that the enhanced signal remains faithful to the intentions of the orig- inal human-provided signal [37]. For example, a hierarchical (“bootstrapping”) oversight approach can be implemented: A series of GPAI models with increasing capabilities are used, where each model in the hierarchy provides oversight for the next more capable model. The least capable model at the base of the hierarchy is the only one directly overseen by human supervisors, as it is easier to oversee than the more capable models.",
			"source": "Gipiškis2024",
			"subcategoryId": "model-alignment-2.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"reinforcement-learning",
				"foundation"
			],
			"implementationNotes": {
				"phase-2": "Perform ai model-assisted oversight of ai systems on prospective validation data",
				"phase-3": "Conduct ongoing ai model-assisted oversight of ai systems on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0979_Gipiškis2024",
			"name": "Post-deployment ongoing monitoring for unexpected use cases",
			"description": "Ongoing monitoring of AI systems can uncover emergent or newly identified capabilities and limitations, in particular when new use cases are found, or in a large-scale deployment with a diverse population of users. These new capa- bilities or limitations can inform ongoing risk analysis. New use-cases can be discovered via monitoring publications, online forums, or APIs [131]. For example, a LLM might have unexpectedly high competence at giving con- vincing medical advice, despite not being directly developed for that purpose, nor verified for accuracy. In that case, the potential risks of this newly found competence can be assessed.",
			"source": "Gipiškis2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Implement post-deployment ongoing monitoring for unexpected use cases in shadow mode alongside clinical workflows",
				"phase-3": "Operate post-deployment ongoing monitoring for unexpected use cases with real-time alerting"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0981_Gipiškis2024",
			"name": "Encouraging downstream provider to evaluate models for deployment-specific failure modes",
			"description": "In some cases, AI system deployers are better positioned to perform certain risk management measures on the AI model in a provided AI system, relative to upstream model providers. For example, they understand their use case better and are more easily able to predict foreseeable misuse or failure modes. These evaluations can inform upstream model providers, or inform supplementary mitigations by the deployer.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply encouraging downstream provider to evaluate models for deployment-specific failure modes to validation results",
				"phase-3": "Execute continuous encouraging downstream provider to evaluate models for deployment-specific failure modes of deployed system"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0982_Gipiškis2024",
			"name": "Encourage reporting of critical vulnerabilities to the upstream provider or other relevant stakeholders",
			"description": "Downstream AI system deployers can report critical vulnerabilities or incidents to the upstream model provider and other relevant regulators. This can con- tribute to safe use, and allow other downstream deployers to be informed about any potential problems.",
			"source": "Gipiškis2024",
			"subcategoryId": "incident-reporting-4.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate encourage reporting of critical vulnerabilities to the upstream provider or other relevant stakeholders summarizing validation outcomes",
				"phase-3": "Produce regular encourage reporting of critical vulnerabilities to the upstream provider or other relevant stakeholders for stakeholders"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0983_Gipiškis2024",
			"name": "Least Privilege access",
			"description": "Deployers of an AI system can restrict its permissions to a whitelisted set of pre- determined options, such that all options not on the whitelist are not accessible to the AI [200, 146]. The entries on the whitelist can be chosen to be as small as possible for the AI system to fulfill its intended purpose, to reduce the attack surface of external attackers, and to decrease the probability that the AI system accidentally takes actions with large unintended side-effects. For example, such whitelisting can apply to network connections, execution of other programs, access to knowledge bases, and the action space of the AI system. It can be implemented through running the AI system on an OS-level virtualization, on networks behind a firewall, and in extreme cases on air-gapped machines.",
			"source": "Gipiškis2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain least privilege access in production environment"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0984_Gipiškis2024",
			"name": "Protect proprietary or unreleased AI model ar- chitecture and parameters",
			"description": "The developers of AI models can invest in cybersecurity to prevent compute re- sources, training source code, model weights, and other critical resources from being accessed and copied by unauthorized third parties (e.g., through insider threats or supply chain attacks). Access to model source code and weights can be restricted through an access control scheme, such as role-based access control. If access to model outputs by third parties is required, it can be provided through an API. Air gaps can block unauthorized remote access. In the case of necessary interaction with an external network, network bandwidth limitations can also be enforced to increase the detection window of potential breaches [108].",
			"source": "Gipiškis2024",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test protect proprietary or unreleased ai model ar- chitecture and parameters capabilities on validation data",
				"phase-3": "Run protect proprietary or unreleased ai model ar- chitecture and parameters continuously on live outputs"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0985_Gipiškis2024",
			"name": "Hardware limitations on data center network connections",
			"description": "Hardware-enforced bandwidth limitations on data center network connections can protect AI model weights from unauthorized access or exfiltration, by lim- iting the speed of model weight access on the connections between data centers and the outside world. Such limitations can be put in place in multiple ways, for example by only constructing connections with a specific bandwidth. The output rate on all data channels can be set low enough that copying the weights is possible in principle (e.g., to enable regular backups), but would take so long that an unauthorized exfiltration of the weights could be detected and prevented. Such rate-limiting is only effective if it applies to all output connections for all storage locations on which the weights of the model are stored [139].",
			"source": "Gipiškis2024",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Test hardware limitations on data center network connections capabilities on validation data",
				"phase-3": "Run hardware limitations on data center network connections continuously on live outputs"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A0987_Gipiškis2024",
			"name": "Sandboxing of AI Systems",
			"description": "AI systems can be developed and tested within a sandbox, (a secure and iso- lated environment used for separating running programs), such that outside access to information within the sandbox is restricted. Within this environ- ment, resources such as storage and memory space, and network access, would be disallowed or heavily restricted [15]. With sandboxing, dangerous or harmful outputs generated during testing will be contained.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform sandboxing of ai systems on prospective validation data"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0988_Gipiškis2024",
			"name": "Redundant systems not reliant on GPAI",
			"description": "Redundant systems provide continuation of a given system’s processes in case of failure of the given system. Importantly, redundant systems should not rely on the factors that caused the original system to fail in the first place, which can include an AI system [47]. For example, if an AI system is incorporated into the landing gear system of an aircraft, such as during autonomous control of the aircraft, redundant systems in the form of mechanical or hydraulic mechanisms must be present to allow for deployment of the landing gear in case of AI system failure.",
			"source": "Gipiškis2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply redundant systems not reliant on gpai in live clinical deployment"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0989_Gipiškis2024",
			"name": "Diverse data labeling and algorithm fairness audits",
			"description": "To mitigate biases in AI models, model providers may want to prioritize di- versity among data labelers and conduct regular fairness audits on their algo- rithms. Data labeling teams that represent different backgrounds and demo- graphic groups can help create more balanced datasets.",
			"source": "Gipiškis2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design diverse data labeling and algorithm fairness audits framework for training data and model design review",
				"phase-2": "Execute diverse data labeling and algorithm fairness audits on validation cohorts and model outputs",
				"phase-3": "Perform scheduled diverse data labeling and algorithm fairness audits on production system and outcomes"
			},
			"effort": 1,
			"impact": 3
		},
		{
			"id": "A0990_Gipiškis2024",
			"name": "Debiasing methods",
			"description": "Providers of AI models can apply techniques to reduce the biases of their models. Current debiasing methods focus on three main types of bias: • Racial and religious bias - Stereotypes based on religious beliefs or racial beliefs. • Gender bias - Stereotypes tied to gender roles and expectations. • Political and cultural bias - Propagation of dominant ideologies or extrem- ist attitudes. Debiasing methods can be categorized based on their application during AI development: • Data pre-processing - Removing or correcting unwanted and biased data, and augmenting quality data to offset data bias, such as rebalancing datasets with counterfactual data augmentation. • During training - Intervening on the training dynamics of the AI model, such as introducing debiasing terms in the objective function or by nega- tively reinforcing biased outputs. • Post-training - Applying techniques to correct a trained but biased model, such as modifying the embedding space.",
			"source": "Gipiškis2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Continue debiasing methods for validation activities",
				"phase-3": "Operate debiasing methods for clinical decision tracking"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0991_Gipiškis2024",
			"name": "Knowledge unlearning techniques",
			"description": "Knowledge unlearning techniques allow specific information to be “forgotten” without the need for retraining the entire model, preserving its general capabil- ities. These techniques can be used to reduce privacy risks and protect against copyrighted or harmful content [96, 188].",
			"source": "Gipiškis2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Verify knowledge unlearning techniques effectiveness",
				"phase-3": "Operate knowledge unlearning techniques for live data"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0992_Gipiškis2024",
			"name": "Differential privacy",
			"description": "Differential privacy techniques [8] can be used to protect users’ privacy by ensuring that sensitive information is not leaked from a training dataset, even after thorough statistical analysis. With differential privacy, noise is added to the dataset or the model’s output in such a way that one cannot deduce the presence or absence of a particular data point within the dataset. This provides individuals with plausible deniability and prevents their information from being exposed.",
			"source": "Gipiškis2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan differential privacy measures for sensitive data"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0993_Gipiškis2024",
			"name": "Quantifying privacy risks of AI models",
			"description": "Measuring privacy risks of an AI model allows the provider and user to calibrate their expectations on where the model can be applied, and it allows them to take the necessary steps to reduce such risks. For example, some metrics include: • Success rate of membership inference attacks [186] - Measures the rate that an attack correctly predicts a given record is part of the training dataset used to train a given AI model. • Discoverable memorization [38] - Theoretical upper-bound of the amount of training data that a given model memorizes. Assuming full knowledge of the training data, it measures the percentage of items that, for a given incomplete data point, a model outputs the remaining (memorized) part.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Establish quantifying privacy risks of ai models procedures for project inception",
				"phase-3": "Continue quantifying privacy risks of ai models for all production activities"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0994_Gipiškis2024",
			"name": "More energy-efficient models or techniques",
			"description": "Deploying more energy-efficient models can reduce their environmental impact. Different model architectural choices result in varying environmental costs, and identifying and adopting more energy-efficient options can result in significant environmental savings, especially when implemented at scale. Consideration should be given to both training energy usage, and deployment (“inference”) usage for the expected model lifecycle.",
			"source": "Gipiškis2024",
			"subcategoryId": "environmental-1.6",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-3": "Maintain more energy-efficient models or techniques in production environment"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0995_Gipiškis2024",
			"name": "Disclosure of energy consumption by AI systems to authorities",
			"description": "Disclosures can direct more necessary attention and scrutiny to projects that consume significant energy. Disclosure involves releasing a summary of key details of the energy consumption of the AI system by all users, including the compute resources used, the amount of power consumed, the measures to reduce excess energy consumption that were in place, and energy sources [133].",
			"source": "Gipiškis2024",
			"subcategoryId": "environmental-1.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Validate disclosure of energy consumption by ai systems to authorities during prospective testing phase",
				"phase-3": "Apply disclosure of energy consumption by ai systems to authorities in live clinical deployment"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A0996_Gipiškis2024",
			"name": "Using low carbon intensity energy grids",
			"description": "Moving model training to energy grids with low carbon intensity can reduce the negative environmental impact [30]. The efficiency of energy grids can vary greatly depending on location. Models can be trained in different locations, as latency is not an issue.",
			"source": "Gipiškis2024",
			"subcategoryId": "environmental-1.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Conduct using low carbon intensity energy grids for validation team members",
				"phase-3": "Provide ongoing using low carbon intensity energy grids for end users"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A0997_Gipiškis2024",
			"name": "Red team access to the final version of a model pre-deployment",
			"description": "Granting red teams access to the final pre-release version of the model can help with identifying potentially dangerous model properties. These properties might not be identified if red teaming is only performed on earlier versions of the model, as late fine-tuning procedures may introduce new vulnerabilities. Red-teaming AI models before they are released to the public can reduce the model non-decomissionability risk. The model’s release can be postponed or even prevented if previously unidentified flaws are detected during the testing [65].",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform red team access to the final version of a model pre-deployment on prospective validation data",
				"phase-3": "Conduct ongoing red team access to the final version of a model pre-deployment on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A0999_Gipiškis2024",
			"name": "Dynamic benchmarking",
			"description": "Dynamic benchmarks are benchmarks that can be continuously updated with new human-generated data. By having one or more target models “in the loop,” examples for benchmarking can be generated with the intent of fooling these target models, or to assess if these models express an appropriate level of uncer- tainty [103]. As the dataset in the benchmark grows, previously benchmarked models can also be reassessed against the updated dataset to reflect its perfor- mance in a more representative manner. Examples of such dynamic benchmarks include DynaSent [153] for sentiment analysis, LFTW [208] for hate speech, and Human-Adversarial VQA [182] for images.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan dynamic benchmarking approach during project design phase",
				"phase-2": "Validate dynamic benchmarking during prospective testing phase"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1000_Gipiškis2024",
			"name": "Benchmark dataset auditing",
			"description": "Auditing benchmark datasets allows for verification of the utility and limitations of the datasets [158]. This allows the provider to more accurately measure AI model capabilities and safety. Auditing includes the evaluation of such datasets by independent third-party organizations and the release of benchmark dataset metadata to the auditors.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design benchmark dataset auditing framework for training data and model design review",
				"phase-2": "Execute benchmark dataset auditing on validation cohorts and model outputs",
				"phase-3": "Perform scheduled benchmark dataset auditing on production system and outcomes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A1001_Gipiškis2024",
			"name": "Informative and powerful benchmarks",
			"description": "Developers of GPAI systems can select benchmarks that are difficult enough to be informative about the capabilities of their AI systems, and cover a large spec- trum of domains in order to signal areas where the GPAI system is performing poorly [120]. Suitable benchmarks contain no label errors, are not vulnerable to being bench- mark contaminants, and are often audited by independent domain experts if they contain domain-specific questions. For multimodal GPAI systems, good benchmarks cover every modality, especially the interaction of different modalities.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Execute informative and powerful benchmarks on validation cohorts and model outputs"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A1002_Gipiškis2024",
			"name": "Statistical data quality reports for benchmarks",
			"description": "If a benchmark dataset is too large to allow for the identification and removal of all flawed instances, statistical reports on the data composition can be added. Random sampling of benchmark data points can be performed to evaluate and report the frequency and types of errors found [54].",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Define statistical data quality reports for benchmarks requirements and templates",
				"phase-2": "Generate statistical data quality reports for benchmarks summarizing validation outcomes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A1003_Gipiškis2024",
			"name": "Avoiding benchmark data with publicly avail- able solutions and releasing contextual information for internet-derived data",
			"description": "Evaluators can improve the integrity of benchmarks by avoiding data with pub- licly available solutions. When using internet-derived data, supplementing it with contextual information (such as entity linking) may help to better docu- ment and assess the integrity of the collected data [95]. This could help in de- tecting instances where contextual information (which may have been included in the training data) might inadvertently reveal details about the solution, en- suring that the data is suitable for accurate benchmarking.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Develop avoiding benchmark data with publicly avail- able solutions and releasing contextual information for internet-derived data curriculum for research team",
				"phase-2": "Conduct avoiding benchmark data with publicly avail- able solutions and releasing contextual information for internet-derived data for validation team members"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1004_Gipiškis2024",
			"name": "Reporting data decontamination efforts",
			"description": "Decontamination analysis may involve comparing the training dataset with the benchmark data, and publishing a report with statistics such as data overlap [235]. Especially for already trained models, including contamination statistics can allow experts to reweigh the success of the model on affected benchmarks.",
			"source": "Gipiškis2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Define reporting data decontamination efforts requirements and templates",
				"phase-2": "Generate reporting data decontamination efforts summarizing validation outcomes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1005_Gipiškis2024",
			"name": "Tracking benchmark leakage",
			"description": "Constant monitoring and documentation of benchmark leakage can help with the early detection of benchmark leaks [224, 95].",
			"source": "Gipiškis2024",
			"subcategoryId": "post-deployment-3.5",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update tracking benchmark leakage with validation procedures and findings",
				"phase-3": "Maintain current tracking benchmark leakage reflecting operational changes"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A1006_Gipiškis2024",
			"name": "Preventing or mitigating data contamination and leakage",
			"description": "Developers of GPAIS and creators of benchmarks can take actions to prevent AI models from being trained on contaminated or leaked data, or mitigate such data contamination and leakage. For example, developers of AI models can try to find and remove contaminated or leaked data from the training corpus, and creators of benchmarks can help them by adding globally unique “canary strings” to the documents containing their benchmarks, which makes them easier to find [197]. More involved inter- ventions by benchmark-creators include restricting access to benchmarks over an API, or continually updating benchmarks to focus on recent data.",
			"source": "Gipiškis2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Update preventing or mitigating data contamination and leakage with validation procedures and findings"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1008_Gipiškis2024",
			"name": "Frequent benchmarking to identify when red teaming is needed",
			"description": "Benchmarks, once created, are inexpensive to apply but may be less informative than red teaming. One reason is that sensitive data (e.g., relating to CBRN- related capabilities) cannot be included in the public questions and answers of benchmarks. On the other hand, red teaming can be more accurate given par- ticipants with diverse attack strategies, but it requires more resources to execute than benchmarking. If there is a correlation between benchmarking and red- teaming scores, then employing frequent benchmarking during the development of the model can identify arising vulnerabilities and inform the developers when more thorough red teaming is required [21]. Benchmarks can act as early warning signs of a larger issue, and red teaming can then be employed to investigate the severity and extent of such an issue.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Complete frequent benchmarking to identify when red teaming is needed for validation use",
				"phase-3": "Update frequent benchmarking to identify when red teaming is needed based on operational feedback"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A1009_Gipiškis2024",
			"name": "Test robustness of GPAI system on relevant benchmarks",
			"description": "Various benchmarks [52, 236] have been developed to assess the robustness of GPAI systems when deployed in environments or scenarios that differ from their training conditions. These benchmarks typically evaluate the model’s ability to handle variations in inputs, unexpected data distributions, or adversarial examples, aiming to ensure reliable performance outside the original training domain.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform test robustness of gpai system on relevant benchmarks on prospective validation data",
				"phase-3": "Conduct ongoing test robustness of gpai system on relevant benchmarks on live system performance"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A1012_Gipiškis2024",
			"name": "Using an AI model to evaluate AI model outputs",
			"description": "In cases where the outputs of AI models cannot be easily evaluated, AI mod- els can be used to evaluate their outputs or the outputs of other AI models [82, 16, 17, 91]. The evaluations can then provide a training signal to improve the original model’s performance or offer explanations of the output for human users.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply using an ai model to evaluate ai model outputs to validation results"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1013_Gipiškis2024",
			"name": "Frequent testing when scaling model or dataset",
			"description": "Testing models after significant increases in compute, data, or model parame- ters. Even relatively small changes to model or dataset size can introduce new properties (“emergent abilities”) and failure modes. Identifying them early can prevent the models from being released prematurely",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan frequent testing when scaling model or dataset methodology for retrospective data analysis",
				"phase-2": "Perform frequent testing when scaling model or dataset on prospective validation data"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A1014_Gipiškis2024",
			"name": "Tamper-resistant safeguards for open-weight models",
			"description": "Training and implementing safeguards can improve the robustness of open- weight models against modifications from fine-tuning or other methods to change the learned weights of the models, especially those aimed at removing safety re- strictions. These safeguards can be resilient even after extensive fine-tuning, ensuring that the model retains its protective measures [199].",
			"source": "Gipiškis2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Maintain tamper-resistant safeguards for open-weight models in production environment"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A1015_Gipiškis2024",
			"name": "Robustness Certificates",
			"description": "A model can be certified to withstand adversarial attacks given specific data- point constraints, model constraints, and attack vectors [156, 124]. Certification means that it can be both analytically proven and shown empirically that the model will withstand such attacks up to a certain threshold. Currently, robustness certification methods are limited to certifying against at- tacks via manipulation of pixels on specific lp norms, canonically the l2 (Eu- clidean) norm, up to a certain neighborhood radius.",
			"source": "Gipiškis2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct robustness certificates for validation team members"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A1016_Gipiškis2024",
			"name": "Documentation of data collection, annotation, maintenance practices",
			"description": "Dataset collection, annotation, and maintenance processes can be documented in detail, including potential unintentional misuse scenarios and corresponding recommendations for data usage [80, 175, 99]. This contributes to transparency, ensures that inherent dataset limitations are known in advance, and helps in selecting the right datasets for intended use cases.",
			"source": "Gipiškis2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Create initial documentation of data collection, annotation, maintenance practices capturing design decisions and data sources"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A1018_Uuk2024",
			"name": "Advanced model access for vetted external researchers",
			"description": "Examples of advanced access rights could include any of the following: increased control over sampling, access to fine-tuning functionality, the ability to inspect and modify model internals, access to training data, or additional features like stable model versions.",
			"source": "Uuk2024",
			"subcategoryId": "third-party-access-4.5",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Specify advanced model access for vetted external researchers requirements for data and systems"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1019_Uuk2024",
			"name": "Adversarial Robustness",
			"description": "State-of-the-art methods such as adversarial training to make models robust to adversarial attacks (e.g., jailbreaking).",
			"source": "Uuk2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Conduct adversarial robustness for validation team members"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A1020_Uuk2024",
			"name": "Bug bounty programs",
			"description": "Clear and user-friendly bug bounty programs that acknowledge and reward individuals for reporting model vulnerabilities and dangerous capabilities.",
			"source": "Uuk2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate bug bounty programs summarizing validation outcomes",
				"phase-3": "Produce regular bug bounty programs for stakeholders"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A1021_Uuk2024",
			"name": "Capability restrictions",
			"description": "Restricting risky capabilities of deployed models, such as advanced autonomy (e.g., self-assigning new sub-goals, executing long-horizon tasks) or tool use functionalities (e.g., function calls, web browsing).",
			"source": "Uuk2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply capability restrictions in live clinical deployment"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1022_Uuk2024",
			"name": "Data curation",
			"description": "Careful data curation prior to all development stages (including fine-tuning) to filter out high-risk content and ensure the training data is sufficiently high-quality.",
			"source": "Uuk2024",
			"subcategoryId": "data-governance-3.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Begin data curation development"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1023_Uuk2024",
			"name": "Deploying powerful models in stages",
			"description": "Starting with a small number of applications and fewer users, and gradually scaling up API-access as rigorous monitoring increases confidence in the model's safety. An API-mediated staged release would also be required before open-sourcing a model.",
			"source": "Uuk2024",
			"subcategoryId": "staged-deployment-3.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate deploying powerful models in stages with real-time alerting"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1024_Uuk2024",
			"name": "External assessment of testing procedure",
			"description": "Bringing in external AI evaluation firms before deployment to assess and red-team the company's execution of dangerous capabilities evaluations.",
			"source": "Uuk2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform external assessment of testing procedure on prospective validation data",
				"phase-3": "Conduct ongoing external assessment of testing procedure on live system performance"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1025_Uuk2024",
			"name": "Fine-tuning restrictions",
			"description": "Restricting or closely monitoring fine-tuning access to models to ensure safeguards remain intact.",
			"source": "Uuk2024",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-3": "Operate fine-tuning restrictions with real-time alerting"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1026_Uuk2024",
			"name": "Harmlessness training",
			"description": "State-of-the-art reinforcement learning and fine-tuning techniques, such as Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization (DPO), to ensure models do not engage in unsafe behavior.",
			"source": "Uuk2024",
			"subcategoryId": "model-alignment-2.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"reinforcement-learning",
				"foundation"
			],
			"implementationNotes": {
				"phase-2": "Conduct harmlessness training for validation team members",
				"phase-3": "Provide ongoing harmlessness training for end users"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A1027_Uuk2024",
			"name": "Input and output filtering",
			"description": "Monitoring for dangerous outputs (e.g., code that appears to be malware or viral genome sequences) and inputs that violate acceptable use policies to ensure models do not engage in harmful behavior.",
			"source": "Uuk2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate input and output filtering with real-time alerting"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A1028_Uuk2024",
			"name": "Intolerable risk thresholds",
			"description": "Assessing and monitoring AI models with regard to red-line risk or capability thresholds set by a third-party, such as a standardization organization or regulator. Companies would further need to make technical, legal, and organizational preparations to halt development and deployment immediately when a breach occurs.",
			"source": "Uuk2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Operate intolerable risk thresholds with real-time alerting"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1029_Uuk2024",
			"name": "KYC screenings",
			"description": "Know-your-customer (KYC) screenings before granting access to models with very high misuse potential or to users producing large amounts of output.",
			"source": "Uuk2024",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Implement kyc screenings for validation systems",
				"phase-3": "Enforce kyc screenings in production systems"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A1030_Uuk2024",
			"name": "Pre-deployment risk assessments",
			"description": "Comprehensive risk assessments before deployment that would assess reasonably foreseeable misuse and include dangerous capability evaluations that incorporate post-training enhancements and collaborations with domain experts. Risk assessments would inform deployment decisions.",
			"source": "Uuk2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply pre-deployment risk assessments to validation results",
				"phase-3": "Execute continuous pre-deployment risk assessments of deployed system"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A1031_Uuk2024",
			"name": "Pre-development risk assessments",
			"description": "Comprehensive risk assessments based on forecasted capabilities before training new models. Risk assessments would inform impactful development decisions.",
			"source": "Uuk2024",
			"subcategoryId": "risk-management-1.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Conduct initial pre-development risk assessments during project planning"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1032_Uuk2024",
			"name": "Pre-registration of large training runs",
			"description": "Registering upcoming training runs above a certain size with an appropriate state actor. Such reports could include descriptions of architecture, training compute, data collection and curation, training objectives and techniques, and planned risk management procedures.",
			"source": "Uuk2024",
			"subcategoryId": "risk-disclosure-4.2",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Define pre-registration of large training runs requirements and templates"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1033_Uuk2024",
			"name": "Prohibiting high-stakes applications",
			"description": "Enforcing use policies that prohibit high-stakes applications. Requires Know-Your-Customer procedures.",
			"source": "Uuk2024",
			"subcategoryId": "access-management-3.3",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"llm",
				"foundation",
				"multi-modal"
			],
			"implementationNotes": {
				"phase-2": "Test and refine prohibiting high-stakes applications during validation",
				"phase-3": "Follow prohibiting high-stakes applications for all operational activities"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A1034_Uuk2024",
			"name": "Risk-focused governance structures",
			"description": "Companies adopt practices typical of high-reliability organizations (HROs), including board risk committees, chief risk officers, multi-party authorization requirements, ethics boards for reviewing development and deployment decisions, and internal audit teams that report directly to the board, tasked with auditing risk management practices.",
			"source": "Uuk2024",
			"subcategoryId": "board-oversight-1.1",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design risk-focused governance structures framework for training data and model design review",
				"phase-2": "Execute risk-focused governance structures on validation cohorts and model outputs",
				"phase-3": "Perform scheduled risk-focused governance structures on production system and outcomes"
			},
			"effort": 3,
			"impact": 3
		},
		{
			"id": "A1035_Uuk2024",
			"name": "Safety Drills",
			"description": "Regularly practicing the implementation of an emergency response plan to stress test the organization's ability to respond to reasonably foreseeable, fast-moving emergency scenarios.",
			"source": "Uuk2024",
			"subcategoryId": "incident-response-3.6",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Perform safety drills on prospective validation data",
				"phase-3": "Conduct ongoing safety drills on live system performance"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A1036_Uuk2024",
			"name": "Safety incident reports and security information sharing",
			"description": "Disclosing reports about AI incidents, such as concrete harms and near misses as well as cyber threat intelligence and security incident reports with appropriate stakeholders such as select governments.",
			"source": "Uuk2024",
			"subcategoryId": "incident-reporting-4.3",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Produce regular safety incident reports and security information sharing for stakeholders"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A1038_Uuk2024",
			"name": "Safety vs. capabilities investments",
			"description": "A significant fraction of employees and computational resources are dedicated to enhancing model safety rather than advancing its capabilities.",
			"source": "Uuk2024",
			"subcategoryId": "safety-frameworks-1.5",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Apply safety vs. capabilities investments in live clinical deployment"
			},
			"effort": 2,
			"impact": 2
		},
		{
			"id": "A1039_Uuk2024",
			"name": "Sharing safety cases",
			"description": "Disclosing to a regulator how high-stakes decisions regarding model development and deployment are made.",
			"source": "Uuk2024",
			"subcategoryId": "governance-disclosure-4.4",
			"phases": [
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-3": "Enhance sharing safety cases through continuous improvement"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1040_Uuk2024",
			"name": "Third party pre-deployment model audits",
			"description": "External pre-deployment assessment to provide a judgment on the safety of a model. Auditors, which could be governments or independent third parties, would receive access to a fine-tuning API for testing, or further appropriate technical means.",
			"source": "Uuk2024",
			"subcategoryId": "testing-auditing-3.1",
			"phases": [
				"phase-1",
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Design third party pre-deployment model audits framework for training data and model design review",
				"phase-2": "Execute third party pre-deployment model audits on validation cohorts and model outputs",
				"phase-3": "Perform scheduled third party pre-deployment model audits on production system and outcomes"
			},
			"effort": 3,
			"impact": 2
		},
		{
			"id": "A1042_Uuk2024",
			"name": "Unlearning Techniques",
			"description": "Removing specific harmful capabilities (e.g., pathogen design) from models using unlearning techniques.",
			"source": "Uuk2024",
			"subcategoryId": "safety-engineering-2.3",
			"phases": [
				"phase-1"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-1": "Plan unlearning techniques approach during project design phase"
			},
			"effort": 2,
			"impact": 3
		},
		{
			"id": "A1043_Uuk2024",
			"name": "Vetted researcher access",
			"description": "Giving good faith, public interest evaluation researchers access to black-box research APIs that provide technical and legal safe harbours to limit barriers imposed by usage policy enforcement, logging, and stringent terms of service.",
			"source": "Uuk2024",
			"subcategoryId": "third-party-access-4.5",
			"phases": [
				"phase-2"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Apply vetted researcher access to validation results"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A1044_Uuk2024",
			"name": "Whistleblower protections",
			"description": "Refraining from restrictive non-disparagement agreements and instantiating comprehensive whistleblower protection policies that clearly outline relevant reporting processes, protection mechanisms, and non-retaliation assurances.",
			"source": "Uuk2024",
			"subcategoryId": "whistleblower-1.4",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Generate whistleblower protections summarizing validation outcomes",
				"phase-3": "Produce regular whistleblower protections for stakeholders"
			},
			"effort": 1,
			"impact": 2
		},
		{
			"id": "A1045_Uuk2024",
			"name": "Advanced information security",
			"description": "Implementing advanced cybersecurity measures and insider threat safeguards to protect proprietary and unreleased model weights.",
			"source": "Uuk2024",
			"subcategoryId": "infrastructure-security-2.1",
			"phases": [
				"phase-2",
				"phase-3"
			],
			"techTypes": [
				"all"
			],
			"implementationNotes": {
				"phase-2": "Deploy advanced information security in validation environment",
				"phase-3": "Maintain advanced information security in production environment"
			},
			"effort": 2,
			"impact": 3
		}
	]
}